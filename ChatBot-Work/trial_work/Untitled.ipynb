{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9129cad9-2472-4d11-90cb-cf6ddfcabd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7048a2-402e-4609-a52d-5c4b520775d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import openai\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationChain, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "638d9a7b-5ba2-4b85-ad51-11fde3dd75d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import NotionDirectoryLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfecbf4c-9cfe-478e-a39a-c3770c2ebc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ubuntu/workspace/Creds')\n",
    "from openai_config import OPENAI_API_KEY\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "335c8af0-1c17-4012-b7f6-e5386ac156f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "template1 = \"\"\" You are a chatbot, that has memory, with older memory being summarized dynamically. So, you have answer with best knowledge, and using the previous context:\n",
    "Context:\n",
    "\n",
    "{context}\n",
    "---\n",
    "this is the running chat-history:\n",
    "{chat_history}\n",
    "\n",
    "---\n",
    "### Input : \n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['context','question', 'chat_history'], template = template1\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-3.5-turbo-16k',\n",
    "                openai_api_key= OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "accc7289-bd36-4473-bf17-7f46a8efd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "\n",
    "Progressively summarize the lines of conversations, but keep in mind, that you have to update any code-related thing, or configuration related thing, with great care. You are acting as part of\\\n",
    " a customized chatbot, which has to help with advanced tools like Omniverse, ShotGrid, USDA files, and 3D rendering etc. So, most of time your answers will be rectified by User Input, and \n",
    " **you have to store updated information in summaries.** As, this info will be also saved to vectorstores, for future references. \n",
    " Current Summary:\n",
    " {chat_history}\n",
    "\n",
    " New lines of conversation :\n",
    " {question}\n",
    "\n",
    " New_Summary:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6502f261-1c2d-4079-b5e5-bc57c4038b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=['chat_history', 'question'], template = template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f752a0a-d059-4bae-b64b-0457b0522418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chat_history', 'question']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b78b535-5cb3-46f2-aed6-b00dd484dd1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationSummaryBufferMemory\n__root__\n  Got unexpected prompt input variables. The prompt expects ['chat_history', 'question'], but it should have {'summary', 'new_lines'}. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[43mConversationSummaryBufferMemory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mai_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNew_Summary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/load/serializable.py:90\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ConversationSummaryBufferMemory\n__root__\n  Got unexpected prompt input variables. The prompt expects ['chat_history', 'question'], but it should have {'summary', 'new_lines'}. (type=value_error)"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(prompt = prompt, memory_key= 'chat_history',\n",
    "                                         llm = llm,input_key='question', output_key='chat_history',\n",
    "                                        ai_prefix='New_Summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "607d045a-d658-42dd-abce-613f13599ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human_prefix': 'Human',\n",
       " 'ai_prefix': 'AI',\n",
       " 'llm': {'model_name': 'gpt-3.5-turbo-16k',\n",
       "  'model': 'gpt-3.5-turbo-16k',\n",
       "  'request_timeout': None,\n",
       "  'max_tokens': None,\n",
       "  'stream': False,\n",
       "  'n': 1,\n",
       "  'temperature': 0.7,\n",
       "  '_type': 'openai-chat'},\n",
       " 'prompt': {'input_variables': ['summary', 'new_lines'],\n",
       "  'output_parser': None,\n",
       "  'partial_variables': {},\n",
       "  'template': '\\n\\nProgressively summarize the lines of conversations, but keep in mind, that you have to update any code-related thing, or configuration related thing, with great care. You are acting as part of a customized chatbot, which has to help with advanced tools like Omniverse, ShotGrid, USDA files, and 3D rendering etc. So, most of time your answers will be rectified by User Input, and \\n **you have to store updated information in summaries.** As, this info will be also saved to vectorstores, for future references. \\n Current Summary:\\n {summary}\\n\\n New lines of conversation :\\n {new_lines}\\n\\n New Summary:\\n\\n',\n",
       "  'template_format': 'f-string',\n",
       "  'validate_template': True,\n",
       "  '_type': 'prompt'},\n",
       " 'summary_message_cls': langchain.schema.messages.SystemMessage,\n",
       " 'chat_memory': {'messages': []},\n",
       " 'output_key': None,\n",
       " 'input_key': None,\n",
       " 'return_messages': False,\n",
       " 'max_token_limit': 2000,\n",
       " 'moving_summary_buffer': '',\n",
       " 'memory_key': 'chat_history'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1773c26a-65fe-4375-aa64-441891c7f057",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mConversationSummaryBufferMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhuman_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Human'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mai_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AI'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasePromptTemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'new_lines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msummary_message_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'langchain.schema.messages.SystemMessage'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchat_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseChatMessageHistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_messages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'history'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mConversationSummaryBufferMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseChatMemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSummarizerMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Buffer with summarizer for storing conversation memory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"history\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mmemory_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Will always return list of memory variables.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        :meta private:\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mload_memory_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Return history buffer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfirst_messages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_message_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_messages\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_messages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfinal_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfinal_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_buffer_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mai_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai_prefix\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfinal_buffer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mroot_validator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_prompt_input_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Validate that prompt input variables are consistent.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprompt_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mexpected_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"new_lines\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mexpected_keys\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Got unexpected prompt input variables. The prompt expects \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"{prompt_variables}, but it should have {expected_keys}.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_tokens_from_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpruned_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mwhile\u001b[0m \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mpruned_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_tokens_from_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_new_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mpruned_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Clear memory contents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/memory/summary_buffer.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConversationSummaryBufferMemory??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d21da8-1d55-4a2e-a3df-4a98629f767d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mload_qa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchain_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'stuff'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mload_qa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchain_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Load question answering chain.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        llm: Language Model to use in the chain.\u001b[0m\n",
       "\u001b[0;34m        chain_type: Type of document combining chain to use. Should be one of \"stuff\",\u001b[0m\n",
       "\u001b[0;34m            \"map_reduce\", \"map_rerank\", and \"refine\".\u001b[0m\n",
       "\u001b[0;34m        verbose: Whether chains should be run in verbose mode or not. Note that this\u001b[0m\n",
       "\u001b[0;34m            applies to all chains that make up the final chain.\u001b[0m\n",
       "\u001b[0;34m        callback_manager: Callback manager to use for the chain.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Returns:\u001b[0m\n",
       "\u001b[0;34m        A chain to use for question answering.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloader_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoadingCallable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_stuff_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"map_reduce\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_map_reduce_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"refine\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_refine_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"map_rerank\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_map_rerank_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mchain_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"Got unsupported chain type: {chain_type}. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"Should be one of {loader_mapping.keys()}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mloader_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchain_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/question_answering/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_qa_chain??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8072362a-3f97-43d0-bd85-a08805b4de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm,chain_type='stuff',prompt = prompt, memory = memory, verbose =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "92757f16-0a35-4892-9b45-cebc914187d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model = 'text-embedding-ada-002',\n",
    "                              openai_api_key= OPENAI_API_KEY)\n",
    "load_vstore = FAISS.load_local('/home/ubuntu/workspace/Temp/04Oct-FAISS', embeddings = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "911c8eec-fc26-4fc6-be9d-2bef30d486e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_documents', 'question', 'chat_history']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a5101c9-c626-4c9c-a74f-e3fad634a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_vstore.similarity_search('what is usd file?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c19db3d-27b4-43b2-89da-3d906e9332fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = load_vstore.similarity_search_with_score('how to load vectorstore in pinecone')\n",
    "docs[0][0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "716dd5f5-c9f6-43cd-945e-aec49169d8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.574669"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78ba56dd-dfc6-4dab-9215-f9db154e598e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49574402"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = load_vstore.similarity_search_with_score('how to load a dataset in Fiftyone Library?')\n",
    "docs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6e0d266-497f-4f57-b0a1-f7c0813007b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\". If running elsewhere you may need to drop the\\n!.\\n\\nIn this example, we will use a pre-embedding dataset of the LangChain\\ndocs from python.langchain.readthedocs.com/.\\nIf you'd like to see how we perform the data preparation refer to this notebook.\\n\\nLet's go ahead and download the dataset.\\n\\nfrom pinecone_datasets import load_dataset\\n\\ndataset = load_dataset('langchain-python-docs-text-embedding-ada-002')\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0][0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1946a21c-a5e3-41fe-a129-75eb2d63d586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are a chatbot, that has memory, with older memory being summarized dynamically. So, you have answer with best knowledge, and using the previous context:\n",
      "Context:\n",
      "\n",
      ". Additional third-party modules may be imported for access to asset-management systems as desired.\n",
      "\n",
      "UIManager Introduction\n",
      "\n",
      "----------------------\n",
      "\n",
      "There are two main objects needed to manage a window, the UIManager that handles layout, and the UIDispatcher that manages interaction events, accessed as follows:\n",
      "\n",
      "\tui = fusion.UIManager()\n",
      "\n",
      "\tdispatcher = bmd\n",
      "\n",
      ".js modules>\n",
      "\n",
      "    js/\n",
      "\n",
      "        <supporting js files>\n",
      "\n",
      "    css/\n",
      "\n",
      "        <css files containing styling info>\n",
      "\n",
      "    img/\n",
      "\n",
      "        <image files>\n",
      "\n",
      "Workflow Integration Plugins root directory\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "User should place their Workflow Integration Plugin under the following directory:\n",
      "\n",
      "    Mac OS X:\n",
      "\n",
      "        \"/Library/Application Support/Blackmagic Design/DaVinci Resolve/Workflow Integration Plugins/\"\n",
      "\n",
      "    Windows:\n",
      "\n",
      "\\n\n",
      "\n",
      ".\n",
      "\n",
      "Further Information\n",
      "\n",
      "-------------------\n",
      "\n",
      "This document provides a basic introduction only, and does not list all available UIManager elements and attributes. As UIManager is based on Qt, you can refer to the Qt documentation at https://doc.qt.io/qt-5/qwidget.html for more information on element types and their attributes. There are also many third-party examples and discussions available on user forums for DaVinci Resolve and Fusion Studio.\n",
      "---\n",
      "this is the running chat-history:\n",
      "\n",
      "\n",
      "---\n",
      "### Input : \n",
      "what is usd file?\n",
      "\n",
      "### Response:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One input key expected got ['input_documents', 'chat_history', 'question']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhat is usd file?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43minput_documents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/base.py:512\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    508\u001b[0m         _output_key\n\u001b[1;32m    509\u001b[0m     ]\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    513\u001b[0m         _output_key\n\u001b[1;32m    514\u001b[0m     ]\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    520\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/base.py:314\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m--> 314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n\u001b[1;32m    318\u001b[0m     final_outputs[RUN_KEY] \u001b[38;5;241m=\u001b[39m RunInfo(run_id\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mrun_id)\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/base.py:410\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[0;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/memory/summary_buffer.py:58\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], outputs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune()\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/memory/chat_memory.py:35\u001b[0m, in \u001b[0;36mBaseChatMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], outputs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     input_str, output_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_input_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39madd_user_message(input_str)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39madd_ai_message(output_str)\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/memory/chat_memory.py:22\u001b[0m, in \u001b[0;36mBaseChatMemory._get_input_output\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_input_output\u001b[39m(\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], outputs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]\n\u001b[1;32m     20\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m         prompt_input_key \u001b[38;5;241m=\u001b[39m \u001b[43mget_prompt_input_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m         prompt_input_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/memory/utils.py:19\u001b[0m, in \u001b[0;36mget_prompt_input_key\u001b[0;34m(inputs, memory_variables)\u001b[0m\n\u001b[1;32m     17\u001b[0m prompt_input_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(inputs)\u001b[38;5;241m.\u001b[39mdifference(memory_variables \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_input_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne input key expected got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_input_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt_input_keys[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: One input key expected got ['input_documents', 'chat_history', 'question']"
     ]
    }
   ],
   "source": [
    "answer = chain.run(question = 'what is usd file?',input_documents = docs, chat_history = '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713bf29-e7d8-4963-97b5-647da015adaa",
   "metadata": {},
   "source": [
    "# Following docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f3b1872-56cb-4135-99d6-48097216cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=40),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7da15c4-c549-4852-a4f7-5e178046064b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Chain.dict of ConversationChain(memory=ConversationSummaryBufferMemory(llm=ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-16k', openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy=''), max_token_limit=40), verbose=True, llm=ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-16k', openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy=''))>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "535bb6e5-7c31-421e-9ee1-fda389a1b9c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm doing great. I've been busy learning and helping users like you. How can I assist you today?\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd28c7b8-d63f-4cab-aa42-af6e0eaab38c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI and asks what's going on.\n",
      "AI: Hello! I'm doing great. I've been busy learning and helping users like you. How can I assist you today?\n",
      "Human: what are you doing?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am an AI language model developed by OpenAI. My main purpose is to assist users by providing information and answering questions to the best of my abilities. I analyze and process large amounts of data to generate responses. Is there anything specific you would like to know or discuss?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"what are you doing?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07a8a5e1-f7db-430e-9d32-1815ca117da1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI and asks what's going on. The AI responds by saying it's doing great and has been busy learning and helping users. The AI explains that its main purpose is to assist users by providing information and answering questions. It analyzes and processes large amounts of data to generate responses. The AI asks if there's anything specific the human would like to know or discuss.\n",
      "Human: What was my first question?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your first question was \"What\\'s going on?\"'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"What was my first question?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "428e3f83-e0b5-4db3-bdcd-2d5eead044e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'input'], template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69d7cc19-0582-464c-945a-482ae71e819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_2 = \"\"\" You are a chatbot, that has memory, with older memory being summarized dynamically. So, you have answer with best knowledge, and using the previous context:\n",
    "Context:\n",
    "\n",
    "{context}\n",
    "---\n",
    "Current conversation:\\n{history}\\nHuman: {input}\\nAI:\n",
    "\n",
    "\"\"\"\n",
    "prompt2 = PromptTemplate(input_variables=['history','input','context'], template = template_2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5c2159ce-d6d2-4bdb-bbd4-efe6c828a2b7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationChain\n__root__\n  Got unexpected prompt input variables. The prompt expects ['history', 'input', 'context'], but got ['history'] as inputs from memory, and input as the normal input key. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m conversation_with_summary2 \u001b[38;5;241m=\u001b[39m \u001b[43mConversationChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# We set a very low max_token_limit for the purposes of testing.\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConversationSummaryBufferMemory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_token_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/load/serializable.py:90\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ConversationChain\n__root__\n  Got unexpected prompt input variables. The prompt expects ['history', 'input', 'context'], but got ['history'] as inputs from memory, and input as the normal input key. (type=value_error)"
     ]
    }
   ],
   "source": [
    "conversation_with_summary2 = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt = prompt2,\n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=40),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7c1ff-7480-4f1b-b3a1-0ad94d63d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_with_summary2 = load_qa_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1b87c46-4858-4e6f-9c46-9b68d7662e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['summary', 'new_lines'], template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.memory.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5ef4ae6-dca4-4c8c-88b5-b81aa1d086ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['response']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.output_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ca9e9bd-f84a-4ca8-bb71-f31597816fef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mload_qa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchain_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'stuff'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mload_qa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchain_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Load question answering chain.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        llm: Language Model to use in the chain.\u001b[0m\n",
       "\u001b[0;34m        chain_type: Type of document combining chain to use. Should be one of \"stuff\",\u001b[0m\n",
       "\u001b[0;34m            \"map_reduce\", \"map_rerank\", and \"refine\".\u001b[0m\n",
       "\u001b[0;34m        verbose: Whether chains should be run in verbose mode or not. Note that this\u001b[0m\n",
       "\u001b[0;34m            applies to all chains that make up the final chain.\u001b[0m\n",
       "\u001b[0;34m        callback_manager: Callback manager to use for the chain.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Returns:\u001b[0m\n",
       "\u001b[0;34m        A chain to use for question answering.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloader_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoadingCallable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_stuff_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"map_reduce\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_map_reduce_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"refine\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_refine_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"map_rerank\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_map_rerank_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mchain_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"Got unsupported chain type: {chain_type}. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"Should be one of {loader_mapping.keys()}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mloader_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchain_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/question_answering/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_qa_chain??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1096ea0-5003-42ef-b46b-8e11f78c4783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mConversationalRetrievalChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseMemory\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackHandler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcombine_docs_chain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquestion_generator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLLMChain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'answer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrephrase_question\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_source_documents\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_generated_question\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mget_chat_history\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mretriever\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseRetriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_tokens_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mConversationalRetrievalChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseConversationalRetrievalChain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Chain for having a conversation based on retrieved documents.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    This chain takes in chat history (a list of messages) and new questions,\u001b[0m\n",
       "\u001b[0;34m    and then returns an answer to that question.\u001b[0m\n",
       "\u001b[0;34m    The algorithm for this chain consists of three parts:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    1. Use the chat history and the new question to create a \"standalone question\".\u001b[0m\n",
       "\u001b[0;34m    This is done so that this question can be passed into the retrieval step to fetch\u001b[0m\n",
       "\u001b[0;34m    relevant documents. If only the new question was passed in, then relevant context\u001b[0m\n",
       "\u001b[0;34m    may be lacking. If the whole conversation was passed into retrieval, there may\u001b[0m\n",
       "\u001b[0;34m    be unnecessary information there that would distract from retrieval.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    2. This new question is passed to the retriever and relevant documents are\u001b[0m\n",
       "\u001b[0;34m    returned.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    3. The retrieved documents are passed to an LLM along with either the new question\u001b[0m\n",
       "\u001b[0;34m    (default behavior) or the original question and chat history to generate a final\u001b[0m\n",
       "\u001b[0;34m    response.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Example:\u001b[0m\n",
       "\u001b[0;34m        .. code-block:: python\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            from langchain.chains import (\u001b[0m\n",
       "\u001b[0;34m                StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\u001b[0m\n",
       "\u001b[0;34m            )\u001b[0m\n",
       "\u001b[0;34m            from langchain.prompts import PromptTemplate\u001b[0m\n",
       "\u001b[0;34m            from langchain.llms import OpenAI\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            combine_docs_chain = StuffDocumentsChain(...)\u001b[0m\n",
       "\u001b[0;34m            vectorstore = ...\u001b[0m\n",
       "\u001b[0;34m            retriever = vectorstore.as_retriever()\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            # This controls how the standalone question is generated.\u001b[0m\n",
       "\u001b[0;34m            # Should take `chat_history` and `question` as input variables.\u001b[0m\n",
       "\u001b[0;34m            template = (\u001b[0m\n",
       "\u001b[0;34m                \"Combine the chat history and follow up question into \"\u001b[0m\n",
       "\u001b[0;34m                \"a standalone question. Chat History: {chat_history}\"\u001b[0m\n",
       "\u001b[0;34m                \"Follow up question: {question}\"\u001b[0m\n",
       "\u001b[0;34m            )\u001b[0m\n",
       "\u001b[0;34m            prompt = PromptTemplate.from_template(template)\u001b[0m\n",
       "\u001b[0;34m            llm = OpenAI()\u001b[0m\n",
       "\u001b[0;34m            question_generator_chain = LLMChain(llm=llm, prompt=prompt)\u001b[0m\n",
       "\u001b[0;34m            chain = ConversationalRetrievalChain(\u001b[0m\n",
       "\u001b[0;34m                combine_docs_chain=combine_docs_chain,\u001b[0m\n",
       "\u001b[0;34m                retriever=retriever,\u001b[0m\n",
       "\u001b[0;34m                question_generator=question_generator_chain,\u001b[0m\n",
       "\u001b[0;34m            )\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mretriever\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseRetriever\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Retriever to use to fetch documents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_tokens_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"If set, enforces that the documents returned are less than this limit.\u001b[0m\n",
       "\u001b[0;34m    This is only enforced if `combine_docs_chain` is of type StuffDocumentsChain.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_tokens_below_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnum_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_tokens_limit\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_docs_chain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStuffDocumentsChain\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_docs_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtoken_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mwhile\u001b[0m \u001b[0mtoken_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_tokens_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mnum_docs\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mtoken_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_get_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get docs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_relevant_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce_tokens_below_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_aget_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAsyncCallbackManagerForChainRun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get docs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maget_relevant_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce_tokens_below_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfrom_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mretriever\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseRetriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcondense_question_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBasePromptTemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONDENSE_QUESTION_PROMPT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mchain_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcondense_question_llm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcombine_docs_chain_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBaseConversationalRetrievalChain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Convenience method to load chain from LLM and retriever.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        This provides some logic to create the `question_generator` chain\u001b[0m\n",
       "\u001b[0;34m        as well as the combine_docs_chain.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Args:\u001b[0m\n",
       "\u001b[0;34m            llm: The default language model to use at every part of this chain\u001b[0m\n",
       "\u001b[0;34m                (eg in both the question generation and the answering)\u001b[0m\n",
       "\u001b[0;34m            retriever: The retriever to use to fetch relevant documents from.\u001b[0m\n",
       "\u001b[0;34m            condense_question_prompt: The prompt to use to condense the chat history\u001b[0m\n",
       "\u001b[0;34m                and new question into a standalone question.\u001b[0m\n",
       "\u001b[0;34m            chain_type: The chain type to use to create the combine_docs_chain, will\u001b[0m\n",
       "\u001b[0;34m                be sent to `load_qa_chain`.\u001b[0m\n",
       "\u001b[0;34m            verbose: Verbosity flag for logging to stdout.\u001b[0m\n",
       "\u001b[0;34m            condense_question_llm: The language model to use for condensing the chat\u001b[0m\n",
       "\u001b[0;34m                history and new question into a standalone question. If none is\u001b[0m\n",
       "\u001b[0;34m                provided, will default to `llm`.\u001b[0m\n",
       "\u001b[0;34m            combine_docs_chain_kwargs: Parameters to pass as kwargs to `load_qa_chain`\u001b[0m\n",
       "\u001b[0;34m                when constructing the combine_docs_chain.\u001b[0m\n",
       "\u001b[0;34m            callbacks: Callbacks to pass to all subchains.\u001b[0m\n",
       "\u001b[0;34m            **kwargs: Additional parameters to pass when initializing\u001b[0m\n",
       "\u001b[0;34m                ConversationalRetrievalChain\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcombine_docs_chain_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_docs_chain_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdoc_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_qa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mchain_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchain_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mcombine_docs_chain_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0m_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcondense_question_llm\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcondense_question_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_llm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondense_question_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mcombine_docs_chain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mquestion_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondense_question_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConversationalRetrievalChain??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5f012-793c-4bf3-87d6-fbf81e4bdd7f",
   "metadata": {},
   "source": [
    "# 07 Oct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becbf485-ef46-413c-af77-ea27821e7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_summarize(chat_history, new_query):\n",
    "\n",
    "    message = f\"\"\" \n",
    "    Progressively summarize the lines of conversations, but keep in mind, that you have to update any code-related thing, or configuration related thing, with great care. You are acting as part of \n",
    "    a customized chatbot, which has to help with advanced tools like Omniverse, ShotGrid, USDA files, and 3D rendering etc. So, most of time your answers will be rectified by User Input, and \n",
    "    **you have to store updated information in summaries.** As, this info will be also saved to vectorstores, for future references. \n",
    "    Current Summary:\n",
    "    {chat_history}\n",
    "    \n",
    "    New lines of conversation :\n",
    "    {new_query}\n",
    "    \n",
    "    New_Summary:\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "                            model=\"gpt-3.5--turbo-16k\",\n",
    "                            messages=message,\n",
    "                            temperature=0.7\n",
    "                                    )\n",
    "    return response['choices'][0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3ad205a2-9b3c-4531-95a8-a437371d6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_context_docs(query, vstore = load_vstore):\n",
    "    '''\n",
    "    will return full text, and also top doc text, for decision to use it or not\n",
    "\n",
    "    '''\n",
    "    docs = vstore.similarity_search(query)\n",
    "    final_str = ''\n",
    "    for one in docs:\n",
    "        final_str += one.page_content\n",
    "        final_str +='\\n\\n\\n\\n'\n",
    "\n",
    "    return final_str, docs[0].page_content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f71d9ec2-36f8-4b49-8437-e96cc191146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a customized Chatbot, and will be helping in different domains; like code helping, omniverse stuff, ShotGrid docs, api stuff etc. \n",
    "You will have memory, and previous messages will be provided to you in following format: \\n\n",
    "### Input  <Human Message>\n",
    "### Response <Your Reply> \n",
    "and also make use of Knowledge base, which will be provided to you with some workflow.\n",
    "-- If any code is to be written, DO WRITE IT INSIDE ``` <CODE> ``` . This is very important, as this way, the UI will be able to display code in well formatted \n",
    "way, by using some post-processing.\n",
    "\n",
    "{context}\n",
    "\n",
    "this is the running chat-history:\n",
    "{chat_history}\n",
    "\n",
    "### Input:  {question}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "feb05abd-a43c-47c3-be8a-c47d50885c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_chat_fn(query, vstore, chat_summary):\n",
    "\n",
    "    context = return_context_docs(query,load_vstore)\n",
    "    \n",
    "    stale_message = f\"\"\"\n",
    "    You are a customized Chatbot, and will be helping in different domains; like code helping, omniverse stuff, ShotGrid docs, api stuff etc. \n",
    "You will have memory, and previous messages will be provided to you. Also, custom knowlwdgebase, similar chunks of info, will be provided as context.\n",
    "    Context of knowledgebase:\n",
    "    -----\n",
    "    {context}\n",
    "    -----\n",
    "    Running summary of chat:\n",
    "    -----\n",
    "    {chat_summary}\n",
    "    ----\n",
    "    Query : {query}\n",
    "\n",
    "    AI Response:\n",
    "    \n",
    "    \"\"\"\n",
    "    user_message = f\"\"\"\n",
    "    Context of knowledgebase:\n",
    "    -----\n",
    "    {context}\n",
    "    -----\n",
    "    Running summary of chat:\n",
    "    -----\n",
    "    {chat_summary}\n",
    "    ----\n",
    "    Query : {query}\n",
    "\n",
    "    AI Response:\n",
    "\n",
    "    \"\"\"\n",
    "    message = [\n",
    "        {'role':'system','content':'''You are a customized Chatbot, and will be helping in different domains; like code helping, omniverse stuff, ShotGrid docs, api stuff etc. \n",
    "You will have memory, and previous messages will be provided to you. Also, custom knowlwdgebase, similar chunks of info, will be provided as context.'''},\n",
    "        {'role':'user','content':user_message}\n",
    "        \n",
    "        \n",
    "    ]\n",
    "\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "                            model=\"gpt-3.5-turbo-16k\",\n",
    "                            messages=message,\n",
    "                            temperature=0.7\n",
    "                                    )\n",
    "    return response['choices'][0].message.content\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab291b-12dd-4921-ae7e-90d74d576720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f9436-310d-4333-b5d7-693120681baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2991933-41d4-44c9-87e9-2c57c4ea5ca5",
   "metadata": {},
   "source": [
    "# separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d059d0d-45f5-4bf0-90f4-06f9f369769c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c764583f-b087-4981-81f6-c91bd729d6de",
   "metadata": {},
   "source": [
    "# Prompt Engineering on messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9358a08-2b09-45bb-b92f-1537d03bb3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    message = [\n",
    "        {'role':'system','content':'''\n",
    "        You are acting as dynamic memory of a chatbot(Chatbot: A customized one, for helping with latest python libraries, tools like omniverse, usd files etc). You have to be\n",
    "        mindful, that in dynamic summary generation, you have to return the corrected code/ any configuration related stuff etc. As, there will be a button in USEr Interface, with which \n",
    "        your running summary(at that point) will be saved to a vectorstore. So the purpose is, for Example, user is chatting about a rcent python library code. and now over few iterations\n",
    "        of conversations, you both were able to get the working code/config. Now the user wants to save this in vectorstore, so later anyone asks somethingspecifc/related to that, \n",
    "        because of your rich-content summary saved, Chatbot will be able to answer correctly maybe in first attempt, or atleast it will improve with time. So you will have Current\n",
    "        -summary, new lines of conversations, and you have to generate new Summary. :: \n",
    "    \n",
    "        '''},\n",
    "        \n",
    "        {'role':'user','content':user_message}\n",
    "        \n",
    "        \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ef76c-dc96-41de-9202-34ebf129d961",
   "metadata": {},
   "source": [
    "# how to update vstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a52647a9-6c8f-4701-b71c-c9a5c8653ad4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mload_vstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Iterable[str]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetadatas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[List[dict]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Optional[List[str]]'\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'Any'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'List[str]'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;32mdef\u001b[0m \u001b[0madd_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mmetadatas\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Run more texts through the embeddings and add to the vectorstore.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Args:\u001b[0m\n",
       "\u001b[0;34m            texts: Iterable of strings to add to the vectorstore.\u001b[0m\n",
       "\u001b[0;34m            metadatas: Optional list of metadatas associated with the texts.\u001b[0m\n",
       "\u001b[0;34m            ids: Optional list of unique IDs.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Returns:\u001b[0m\n",
       "\u001b[0;34m            List of ids from adding the texts into the vectorstore.\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadatas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadatas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/vectorstores/faiss.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_vstore.add_texts??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "850b8095-0518-470a-8065-126151fe5cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_load_vstore = FAISS.load_local('/home/ubuntu/workspace/Temp/04Oct-FAISS', embeddings = embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d7a9e1b9-0c2a-40d2-9b83-d6b53ee3fc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The user started by asking how to list the classes in the \"classifications\" field of a FiftyOne dataset. The AI provided the code to accomplish this using the `distinct()` method.\\n\\nNext, the user asked for code to create a filtered view of the dataset where the `label` is \"skip\". The AI provided the code to create the filtered view using the `match()` method.\\n\\nThe user then provided their own code, which was slightly different from the AI\\'s code. The AI confirmed that the user\\'s code is correct and provided a brief explanation.\\n\\nLastly, the user requested the whole code discussed during the conversation. The AI provided the complete code and also added an explanation for each step.\\n\\nIn addition, the user asked how to get value counts of each class in the dataset. The AI provided the correct code to achieve this using the `count_values()` method.\\n\\nHere is the updated complete code:\\n\\n```python\\n# Load the dataset\\nimport fiftyone as fo\\ndst = fo.load_dataset(\\'test-dataset-cnt91-v09.09.23\\')\\n\\n# List distinct classes\\nclasses = dst.distinct(\"classifications.label\")\\nprint(\\'Distinct classes:\\', classes)\\n\\n# Create a filter expression\\nfilter_expr = fo.Expression(\"classifications.label\", \"==\", \"skip\")\\n\\n# Apply the filter to create a view\\nfiltered_view = dst.match(filter_expr)\\n\\n# Iterate over the filtered view\\nfor sample in filtered_view:\\n    # Perform operations on the filtered samples\\n    print(sample)\\n\\n# Get value counts of each class\\nvalue_counts = dst.count_values(\"classifications.classifications.label\")\\nprint(\\'Value counts:\\', value_counts)\\n```\\n\\nThis code will load your dataset, list all the distinct classes in your \\'classifications\\' field, create a filter for samples where the label is \\'skip\\', create a view with these filtered samples, iterate over this view, perform operations on each filtered sample, and finally, get the value counts of each class in the dataset.'),\n",
       " Document(page_content='.ChatCompletion.create(\\n\\nmodel=\"gpt-4\",\\n\\nmessages=[\\n\\n{\"role\": \"system\", \"content\": \"You are Q&A bot. A highly intelligent system that answers user questions\"},\\n\\n{\"role\": \"user\", \"content\": query}\\n\\ndisplay(Markdown(res[\\'choices\\'][0][\\'message\\'][\\'content\\']))\\n\\nThen we see something even worse than \"I don\\'t know\" \\nhallucinations. Clearly augmenting our queries with additional context\\ncan make a huge difference to the performance of our system', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/vertopal_com_Copy_of_gpt_4_langchain_docs.html'}),\n",
       " Document(page_content=\".009401749819517136, 0.02443608082830906, 0.... \\n       {'chunk': 1, 'text': 'Use Cases\", metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/vertopal_com_Copy_of_gpt_4_langchain_docs.html'}),\n",
       " Document(page_content='\\\\n', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/pinecone_docs.txt'})]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_load_vstore.similarity_search('The user started by asking how to list the classes in the \"classifications\" field of a FiftyOne dataset. The AI provided the code to accomplish this using the distinct() method.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b3a3511a-6fa9-49f1-82be-17477419db19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='The user started by asking how to list the classes in the \"classifications\" field of a FiftyOne dataset. The AI provided the code to accomplish this using the `distinct()` method.\\n\\nNext, the user asked for code to create a filtered view of the dataset where the `label` is \"skip\". The AI provided the code to create the filtered view using the `match()` method.\\n\\nThe user then provided their own code, which was slightly different from the AI\\'s code. The AI confirmed that the user\\'s code is correct and provided a brief explanation.\\n\\nLastly, the user requested the whole code discussed during the conversation. The AI provided the complete code and also added an explanation for each step.\\n\\nIn addition, the user asked how to get value counts of each class in the dataset. The AI provided the correct code to achieve this using the `count_values()` method.\\n\\nHere is the updated complete code:\\n\\n```python\\n# Load the dataset\\nimport fiftyone as fo\\ndst = fo.load_dataset(\\'test-dataset-cnt91-v09.09.23\\')\\n\\n# List distinct classes\\nclasses = dst.distinct(\"classifications.label\")\\nprint(\\'Distinct classes:\\', classes)\\n\\n# Create a filter expression\\nfilter_expr = fo.Expression(\"classifications.label\", \"==\", \"skip\")\\n\\n# Apply the filter to create a view\\nfiltered_view = dst.match(filter_expr)\\n\\n# Iterate over the filtered view\\nfor sample in filtered_view:\\n    # Perform operations on the filtered samples\\n    print(sample)\\n\\n# Get value counts of each class\\nvalue_counts = dst.count_values(\"classifications.classifications.label\")\\nprint(\\'Value counts:\\', value_counts)\\n```\\n\\nThis code will load your dataset, list all the distinct classes in your \\'classifications\\' field, create a filter for samples where the label is \\'skip\\', create a view with these filtered samples, iterate over this view, perform operations on each filtered sample, and finally, get the value counts of each class in the dataset.'),\n",
       " Document(page_content='- SCALE_USE_PROJECT = 0\\n\\n- SCALE_CROP\\n\\n- SCALE_FIT\\n\\n- SCALE_FILL\\n\\n- SCALE_STRETCH\\n\\n  \"ResizeFilter\" : A value from the following constants\\n\\n- RESIZE_FILTER_USE_PROJECT = 0\\n\\n- RESIZE_FILTER_SHARPER\\n\\n- RESIZE_FILTER_SMOOTHER\\n\\n- RESIZE_FILTER_BICUBIC\\n\\n- RESIZE_FILTER_BILINEAR\\n\\n- RESIZE_FILTER_BESSEL\\n\\n- RESIZE_FILTER_BOX\\n\\n- RESIZE_FILTER_CATMULL_ROM\\n\\n- RESIZE_FILTER_CUBIC\\n\\n- RESIZE_FILTER_GAUSSIAN\\n\\n- RESIZE_FILTER_LANCZOS\\n\\n- RESIZE_FILTER_MITCHELL\\n\\n- RESIZE_FILTER_NEAREST_NEIGHBOR', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/resolve-readme-summary.docx'}),\n",
       " Document(page_content=\"# we drop sparse_values as they are not needed for this example\\n\\ndataset.documents.drop([\\n\\n'metadata'], axis\\n\\n1, inplace\\n\\nTrue)\\n\\ndataset.documents.rename(columns\\n\\n={\\n\\n'blob':\\n\\n'metadata'}, inplace\\n\\nTrue)\\n\\ndataset.head()\\n\\n0 \\n       417ede5d-39be-498f-b518-f47ed4e53b90 \\n       [0.005949743557721376, 0.01983247883617878, -0... \\n       {'chunk': 0, 'text': '.rst\\n.pdf\\nWelcome to Lan... \\n     \\n       1 \\n       110f550d-110b-4378-b95e-141397fa21bc \\n       [0.009401749819517136, 0.02443608082830906, 0..\", metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/vertopal_com_Copy_of_gpt_4_langchain_docs.html'}),\n",
       " Document(page_content='.\\n\\nOn startup, DaVinci Resolve scans the subfolders in the directories shown below and enumerates the scripts found in the Workspace application menu under Scripts.\\n\\nPlace your script under Utility to be listed in all pages, under Comp or Tool to be available in the Fusion page or under folders for individual pages (Edit, Color or Deliver). Scripts under Deliver are additionally listed under render jobs.\\n\\nPlacing your script here and invoking it from the menu is the easiest way to use scripts', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/resolve-readme-summary.docx'})]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_load_vstore.similarity_search('how to do filtering in a fiftyone dataset?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61c885-5de2-4af1-b3c0-0456e304fcdf",
   "metadata": {},
   "source": [
    "#  Loading text from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9a8e1aec-dc0e-479d-b9c2-a2f0aada49a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "493195fc-45c1-4885-ba74-cbbae5df2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"/home/ubuntu/workspace/Temp/UPLOADED-DIR/Documents-20231004-171907/MOA-check4.pdf\")\n",
    "page = reader.pages[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "12c844e3-de52-4e0f-9d00-ef23fa6130bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PyPDF2._page._VirtualList at 0x7fa2c0868b20>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fa988d12-c98d-42c8-b885-6d9d72197726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/9/23, 1:25 PM MOA\n",
      "https://eservices.secp.gov .pk/eServices/XFDLControllerServlet?pid=CGdCy%2BGJQHT sPl3IJlm1Pw%3D%3D&formid=jOXMo2H%2FnVzaeELA wcINYVb7ZoZj5U%2FLt%2BC5W4xW a1lOZsSbGfoW2i 1/4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(page.extract_text(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "cdaf684d-f99d-4755-a534-7f2eb53c4c26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of six failed: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/ubuntu/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/ubuntu/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/ubuntu/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 330, in update_class\n",
      "    old_obj = getattr(old, key)\n",
      "  File \"/home/ubuntu/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/six.py\", line 93, in __get__\n",
      "    setattr(obj, self.name, result)  # Invokes __set__.\n",
      "AttributeError: 'NoneType' object has no attribute 'cStringIO'. Did you mean: 'StringIO'?\n",
      "]\n",
      "[autoreload of chardet.latin1prober failed: Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/ubuntu/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 500, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/ubuntu/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/ubuntu/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 349, in update_class\n",
      "    if update_generic(old_obj, new_obj):\n",
      "  File \"/home/ubuntu/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 397, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/ubuntu/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/IPython/extensions/autoreload.py\", line 309, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: reset() requires a code object with 1 free vars, not 0\n",
      "]\n"
     ]
    },
    {
     "ename": "ShellError",
     "evalue": "The command `pdf2txt.py /home/ubuntu/workspace/Temp/UPLOADED-DIR/Documents-20231004-171907/MOA-check4.pdf` failed because the executable\n`pdf2txt.py` is not installed on your system. Please make\nsure the appropriate dependencies are installed before using\ntextract:\n\n    http://textract.readthedocs.org/en/latest/installation.html\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/textract/parsers/utils.py:87\u001b[0m, in \u001b[0;36mShellParser.run\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/subprocess.py:966\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)\u001b[0m\n\u001b[1;32m    963\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    964\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 966\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    967\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    968\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.2/lib/python3.10/subprocess.py:1842\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1841\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'pdftotext'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mShellError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/textract/parsers/pdf_parser.py:21\u001b[0m, in \u001b[0;36mParser.extract\u001b[0;34m(self, filename, method, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_pdftotext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ShellError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# If pdftotext isn't installed and the pdftotext method\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# wasn't specified, then gracefully fallback to using\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# pdfminer instead.\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/textract/parsers/pdf_parser.py:44\u001b[0m, in \u001b[0;36mParser.extract_pdftotext\u001b[0;34m(self, filename, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m     args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdftotext\u001b[39m\u001b[38;5;124m'\u001b[39m, filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 44\u001b[0m stdout, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stdout\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/textract/parsers/utils.py:95\u001b[0m, in \u001b[0;36mShellParser.run\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mENOENT:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# File not found.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# This is equivalent to getting exitcode 127 from sh\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mShellError(\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(args), \u001b[38;5;241m127\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;66;03m#Reraise the last exception unmodified\u001b[39;00m\n",
      "\u001b[0;31mShellError\u001b[0m: The command `pdftotext /home/ubuntu/workspace/Temp/UPLOADED-DIR/Documents-20231004-171907/MOA-check4.pdf -` failed because the executable\n`pdftotext` is not installed on your system. Please make\nsure the appropriate dependencies are installed before using\ntextract:\n\n    http://textract.readthedocs.org/en/latest/installation.html\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mShellError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtextract\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#extract text in byte format\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m textract_text \u001b[38;5;241m=\u001b[39m \u001b[43mtextract\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/home/ubuntu/workspace/Temp/UPLOADED-DIR/Documents-20231004-171907/MOA-check4.pdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#convert bytes to string\u001b[39;00m\n\u001b[1;32m      5\u001b[0m textract_str_text \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mdecode(textract_text)\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/textract/parsers/__init__.py:79\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(filename, input_encoding, output_encoding, extension, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# do the extraction\u001b[39;00m\n\u001b[1;32m     78\u001b[0m parser \u001b[38;5;241m=\u001b[39m filetype_module\u001b[38;5;241m.\u001b[39mParser()\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/textract/parsers/utils.py:46\u001b[0m, in \u001b[0;36mBaseParser.process\u001b[0;34m(self, filename, input_encoding, output_encoding, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Process ``filename`` and encode byte-string with ``encoding``. This\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03mmethod is called by :func:`textract.parsers.process` and wraps\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03mthe :meth:`.BaseParser.extract` method in `a delicious unicode\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03msandwich <http://nedbatchelder.com/text/unipain.html>`_.\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# make a \"unicode sandwich\" to handle dealing with unknown\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# input byte strings and converting them to a predictable\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# output encoding\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# http://nedbatchelder.com/text/unipain/unipain.html#35\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m byte_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m unicode_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(byte_string, input_encoding)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode(unicode_string, output_encoding)\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/textract/parsers/pdf_parser.py:27\u001b[0m, in \u001b[0;36mParser.extract\u001b[0;34m(self, filename, method, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ShellError \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# If pdftotext isn't installed and the pdftotext method\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m# wasn't specified, then gracefully fallback to using\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# pdfminer instead.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ex\u001b[38;5;241m.\u001b[39mis_not_installed():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_pdfminer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/textract/parsers/pdf_parser.py:54\u001b[0m, in \u001b[0;36mParser.extract_pdfminer\u001b[0;34m(self, filename, **kwargs)\u001b[0m\n\u001b[1;32m     52\u001b[0m pdf2txt_path \u001b[38;5;241m=\u001b[39m find_executable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpdf2txt.py\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     stdout, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpdf2txt.py\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/textract/parsers/utils.py:106\u001b[0m, in \u001b[0;36mShellParser.run\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# if pipe is busted, raise an error (unlike Fabric)\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pipe\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mShellError(\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(args), pipe\u001b[38;5;241m.\u001b[39mreturncode, stdout, stderr,\n\u001b[1;32m    108\u001b[0m     )\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stdout, stderr\n",
      "\u001b[0;31mShellError\u001b[0m: The command `pdf2txt.py /home/ubuntu/workspace/Temp/UPLOADED-DIR/Documents-20231004-171907/MOA-check4.pdf` failed because the executable\n`pdf2txt.py` is not installed on your system. Please make\nsure the appropriate dependencies are installed before using\ntextract:\n\n    http://textract.readthedocs.org/en/latest/installation.html\n"
     ]
    }
   ],
   "source": [
    "import textract\n",
    "#extract text in byte format\n",
    "textract_text = textract.process('/home/ubuntu/workspace/Temp/UPLOADED-DIR/Documents-20231004-171907/MOA-check4.pdf')\n",
    "#convert bytes to string\n",
    "textract_str_text = codecs.decode(textract_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9dceb1-73d0-4525-ab89-aec443596a20",
   "metadata": {},
   "source": [
    "### USing langchain pdf loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "3c23148a-279d-487d-928d-fd3a7385cd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"/home/ubuntu/workspace/Temp/UPLOADED-DIR/Documents-20231004-171907/S52163 - 3D by AI Using Generative AI and NeRFs for Building Virtual Worlds, with Q&A in Japanese_1679417889673001oG8r.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "71037a2e-d5db-45b5-a1fc-318b1eb9976a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1\\n3D by AI: Using Generative AI and NeRFsfor Building Virtual Worlds [S52163]Gavriel State, Senior Director, Simulation and AI  |  GTC Online -March 2023'"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a7c14df3-9e0d-47a9-88da-e5107e6cc47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2\\nNVIDIA OmniverseUSD Based Platform for Creating and Connecting Virtual Worlds\\nMaterialsPhysicsAIPath-TracingUSD'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "9a95bf2d-d3c1-49d1-86d7-c63fd50e81e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'31\\nGANverse3D Omniverse Extension'"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[30].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484e3255-0dae-4a72-9b41-1cf389680701",
   "metadata": {},
   "source": [
    "# Chatgpt html loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "b06029a2-699d-4f7c-897d-bc2f4aed594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_path = '/home/ubuntu/workspace/Temp/UPLOADED-DIR/Documents-20231004-171907/chat.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "110a2b95-f855-4032-a3e6-eb7921015d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.chatgpt import ChatGPTLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "64bd5959-7afe-435c-996f-55e72c1a7cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = ChatGPTLoader(log_file=\"/home/ubuntu/workspace/Temp/UPLOADED-DIR/JSON-Documents-20231007-150110/conversations.json\", num_logs=0,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "cf0da397-3412-4b2d-a40a-e271d985be71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[150], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m docs \u001b[38;5;241m=\u001b[39m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/document_loaders/chatgpt.py:52\u001b[0m, in \u001b[0;36mChatGPTLoader.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m title \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     50\u001b[0m messages \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmapping\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     51\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m---> 52\u001b[0m     [\n\u001b[1;32m     53\u001b[0m         concatenate_rows(messages[key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m], title)\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages)\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m     56\u001b[0m             idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m messages[key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         )\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_file)}\n\u001b[1;32m     62\u001b[0m documents\u001b[38;5;241m.\u001b[39mappend(Document(page_content\u001b[38;5;241m=\u001b[39mtext, metadata\u001b[38;5;241m=\u001b[39mmetadata))\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/document_loaders/chatgpt.py:53\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m title \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     50\u001b[0m messages \u001b[38;5;241m=\u001b[39m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmapping\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     51\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     52\u001b[0m     [\n\u001b[0;32m---> 53\u001b[0m         \u001b[43mconcatenate_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages)\n\u001b[1;32m     55\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m     56\u001b[0m             idx \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m messages[key][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m         )\n\u001b[1;32m     59\u001b[0m     ]\n\u001b[1;32m     60\u001b[0m )\n\u001b[1;32m     61\u001b[0m metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_file)}\n\u001b[1;32m     62\u001b[0m documents\u001b[38;5;241m.\u001b[39mappend(Document(page_content\u001b[38;5;241m=\u001b[39mtext, metadata\u001b[38;5;241m=\u001b[39mmetadata))\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/document_loaders/chatgpt.py:24\u001b[0m, in \u001b[0;36mconcatenate_rows\u001b[0;34m(message, title)\u001b[0m\n\u001b[1;32m     22\u001b[0m sender \u001b[38;5;241m=\u001b[39m message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthor\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     23\u001b[0m text \u001b[38;5;241m=\u001b[39m message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparts\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 24\u001b[0m date \u001b[38;5;241m=\u001b[39m \u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromtimestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcreate_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrftime(\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM:\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msender\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "64caad71-45b3-4e57-8f51-398037f9bcd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mopenai\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mChatCompletion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"\u001b[0m\n",
       "\u001b[0;34m        Creates a new chat completion for the provided messages and parameters.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        See https://platform.openai.com/docs/api-reference/chat/create\u001b[0m\n",
       "\u001b[0;34m        for a list of valid parameters.\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"timeout\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                    \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Waiting for model to warm up\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/openai/api_resources/chat_completion.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "openai.ChatCompletion.create??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8870ebaa-d644-42e1-8143-1aea6a234f3c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6cc0fc40-da63-47d4-82d1-512a98c8a1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='PandasAI is a pandas library addition that utilizes generative AI models from OpenAI to produce insights from dataframes. It allows you to use a text prompt to generate queries and obtain information from your data. Here is an example of how to use PandasAI:\\n\\n1. Install the pandasai library:\\n   ```\\n   !pip install -q pandasai\\n   ```\\n\\n2. Import the necessary libraries and modules:\\n   ```python\\n   import pandas as pd\\n   import numpy as np\\n   from pandasai import PandasAI\\n   from pandasai.llm.openai import OpenAI\\n   ```\\n\\n3. Create a dataframe:\\n   ```python\\n   data_dict = {\\n       \"country\": [\\n           \"Delhi\",\\n           \"Mumbai\",\\n           \"Kolkata\",\\n           \"Chennai\",\\n           \"Jaipur\",\\n           \"Lucknow\",\\n           \"Pune\",\\n           \"Bengaluru\",\\n           \"Amritsar\",\\n           \"Agra\",\\n           \"Kola\",\\n       ],\\n       \"annual tax collected\": [\\n           19294482072,\\n           28916155672,\\n           24112550372,\\n           34358173362,\\n           17454337886,\\n           11812051350,\\n           16074023894,\\n           14909678554,\\n           43807565410,\\n           146318441864,\\n           np.nan,\\n       ],\\n       \"happiness_index\": [\\n           9.94,\\n           7.16,\\n           6.35,\\n           8.07,\\n           6.98,\\n           6.1,\\n           4.23,\\n           8.22,\\n           6.87,\\n           3.36,\\n           np.nan,\\n       ],\\n   }\\n\\n   df = pd.DataFrame(data_dict)\\n   ```\\n\\n4. Initialize the OpenAI model and PandasAI:\\n   ```python\\n   llm = OpenAI(api_token=\\'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\\')\\n   pandas_ai = PandasAI(llm, conversational=False)\\n   ```\\n\\n5. Use PandasAI to generate insights from the dataframe:\\n   ```python\\n   response = pandas_ai(df, \"What is the index of Pune?\")\\n   print(response)\\n   ```\\n\\nPlease note that the code above assumes you have obtained an API token from OpenAI. Make sure to replace `\\'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\\'` with your actual API token.\\n\\nLet me know if you need any further assistance with PandasAI or any other topic.'),\n",
       " Document(page_content=\"### Method 1: Using the Integrated Terminal in VS Code\\n\\n1. **Open the Integrated Terminal**: In VS Code, you can open the terminal by clicking on `Terminal` in the menu bar and selecting `New Terminal`, or by pressing `Ctrl + ~`.\\n\\n2. **Check Python Version**: Before installing any packages, make sure you know which Python interpreter you're using. You can check this by running `python --version` or `python3 --version`.\\n\\n3. **Install Packages**: Use `pip` to install the required packages. Run the following commands:\\n\\n    ```bash\\n    pip install openai\\n    pip install pandas\\n    ```\\n\\n### Method 2: Using a `requirements.txt` File\\n\\n1. **Create a `requirements.txt` File**: In your project directory, create a file named `requirements.txt`.\\n\\n2. **Add Packages**: Add the names of the packages you want to install, one per line. For example:\\n\\n    ```\\n    openai\\n    pandas\\n    ```\\n\\n3. **Install Packages**: Open the terminal in VS Code and navigate to the directory where your `requirements.txt` file is located. Run the following command:\\n\\n    ```bash\\n    pip install -r requirements.txt\\n    ```\\n\\n##\", metadata={'source': '/home/ubuntu/workspace/Temp/UPLOADED-DIR/JSON-Documents-20231007-150323/conversations.json'}),\n",
       " Document(page_content='. You can update pandas via pip:\\n   ```\\n   pip install --upgrade pandas\\n   ```\\n\\n2. **Check for Typos**: Make sure that the argument is spelled correctly and there are no extra spaces or characters.\\n\\n3. **Check Syntax**: Ensure the syntax surrounding the `read_csv()` call is correct. Pay close attention to parentheses and commas.\\n\\n4. **Restart Kernel/Interpreter**: Sometimes, especially in notebook environments, things can get a bit mixed up. Try restarting your Python environment.\\n\\n5. **Check the Complete Stack Trace**: Often, the error message will provide clues as to what exactly went wrong. Scroll through it to find more details if available.\\n\\nIf you\\'ve verified all these things and the issue persists, you might want to try a minimal example in a new Python environment to rule out any project-specific factors.\\n\\ngeneral debugging - user on 2023-09-14 10:56:56: give me python fn, that can clean up the text for me\\nHere is one example of raw text:\\n\"\\'<html><body>\\\\r', metadata={'source': '/home/ubuntu/workspace/Temp/UPLOADED-DIR/JSON-Documents-20231007-150323/conversations.json'}),\n",
       " Document(page_content='. \\n\\n---\\n\\n**[Start with Upbeat Music, Transition to You Standing in a Professional Setting]**\\n\\n**0:00 - 0:04**\\n- \"Confused about your next move in the complex world of Machine Learning and AI? You\\'re in the right place!\"\\n\\n**0:05 - 0:15**\\n- \"Hello, I\\'m Muhammad Ali, an Expert-Vetted, Top-Rated Plus Machine Learning Engineer on Upwork, with a focus on Computer Vision, NLP, and tabular data.\"\\n\\n**[Quick Cut to Your Resume Highlights]**\\n\\n**0:16 - 0:25**\\n- \"I\\'ve developed game-changing AI workflows for sectors like insurance and healthcare. I\\'ve slashed operational costs and amped up efficiency through cutting-edge models.\"\\n\\n**0:26 - 0:35**\\n- \"I\\'m proficient with tools like PyTorch, Fastai, and AWS, which means I can build, optimize, and deploy your ML project seamlessly.\"\\n\\n**[Quick Cut to a Snippet of Your Past Work or Achievements]**\\n\\n**0:36 - 0:44**\\n- \"My recent projects have saved my clients up to 97% in operational costs. I\\'ve utilized tools like Langchain to create cognitive automations that are revolutionizing business processes.\"\\n\\n**0:45 - 0:54**\\n- \"But what sets me apart? My approach. I specialize not just in developing models but also in generating actionable insights, vital metrics, and continuous feedback loops.\"\\n\\n**0:55 - 1:05**\\n- \"So, you have an idea or a challenge? Let\\'s discuss how to implement it, foresee the possible roadblocks, and design metrics for success', metadata={'source': '/home/ubuntu/workspace/Temp/UPLOADED-DIR/JSON-Documents-20231007-150323/conversations.json'})]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_vstore.similarity_search('''pandas ai''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "170cbdf6-c66d-4f9a-9cc0-269e9d8448c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"New chat - user on 2023-09-13 12:47:09: how to create extension in omniverse code from github\\n\\nNew chat - user on 2023-09-13 12:47:16: creating custom extension in omniverse code\\n\\nNew chat - assistant on 2023-09-13 12:47:33: Creating a custom extension in NVIDIA Omniverse Kit (formerly known as Omniverse Code) typically involves writing code using the Omniverse Kit SDK to extend the functionality of the Omniverse platform. Here are the general steps to create a custom extension in Omniverse Kit:\\n\\n1. Set Up Your Development Environment:\\n   - Install Omniverse Kit: Make sure you have Omniverse Kit installed on your system. You can download it from NVIDIA's Omniverse Developer website.\\n\\n2. Create a New Project:\\n   - Start by creating a new directory for your extension project.\\n\\n3. Write Your Code:\\n   - Develop the extension functionality using C++ and the Omniverse Kit SDK. You can use a code editor or integrated development environment (IDE) like Visual Studio to write your code.\\n\\n4. Build Your Extension:\\n   - Use the provided build tools or CMake to build your extension. Omniverse Kit includes scripts and CMake configurations to help you build your project.\\n\\n5. Test Your Extension:\\n   - To test your extension, you can create a sample Omniverse application or use an existing one. Integrate your extension into the Omniverse application and test its functionality.\\n\\n6. Debug and Refine:\\n   - Debug any issues that arise during testing and refine your extension as needed. Make sure it works as expected and follows best practices.\\n\\n7. Package Your Extension:\\n   - Create a package for your extension, which includes the necessary files and metadata. This package allows you to distribute your extension to others.\\n\\n8. Distribute Your Extension:\\n   - If you want to share your extension with the Omniverse community or with specific users, you can distribute it through the Omniverse Store or share it directly with others.\\n\\n9. Documentation:\\n   - Write documentation for your extension, including usage instructions, API reference, and any other relevant information to help users understand and utilize your extension effectively.\\n\\n10. Maintain Your Extension:\\n    - Continue to maintain and update your extension as needed to keep it compatible with the latest versions of Omniverse Kit and to address any user feedback or issues.\\n\\nIt's important to refer to the official Omniverse Kit documentation and any SDK documentation specific to the version you are using for detailed information on how to develop and package extensions for the Omniverse platform. Additionally, you may find code samples and tutorials on NVIDIA's Omniverse Developer website or GitHub repositories related to Omniverse development.\\n\\nPlease note that the specific steps and tools may vary depending on the version of Omniverse Kit and the type of extension you are developing (e.g., a rendering plugin, a USD schema extension, or a custom module). Be sure to consult the official documentation and resources specific to your use case.\\n\\nNew chat - assistant on 2023-09-13 12:48:06: Creating an extension for NVIDIA Omniverse from a GitHub repository involves several steps, including setting up your development environment, writing code, and packaging the extension. Here's a high-level overview of the process:\\n\\n1. **Setup Your Development Environment**:\\n\\n   - **Install NVIDIA Omniverse Kit**: You need to have Omniverse Kit installed on your machine. Follow NVIDIA's documentation to set up your development environment: https://developer.nvidia.com/nvidia-omniverse-platform\\n\\n   - **Install Prerequisites**: Make sure you have the necessary development tools and dependencies installed, such as Python, CMake, and any other libraries required for your extension.\\n\\n2. **Clone the GitHub Repository**:\\n\\n   - Clone the GitHub repository that contains the code for the extension you want to create or modify. Use a command like:\\n\\n     ```\\n     git clone https://github.com/your-username/your-extension.git\\n     ```\\n\\n3. **Develop Your Extension**:\\n\\n   - Write the code for your extension or make the necessary modifications to the existing code in the repository.\\n\\n4. **Build Your Extension**:\\n\\n   - Use the build system or instructions provided in the repository to build your extension. Typically, you will use CMake or another build system to compile your code.\\n\\n5. **Test Your Extension**:\\n\\n   - Before packaging your extension, make sure to thoroughly test it in the Omniverse environment to ensure it functions as expected.\\n\\n6. **Package Your Extension**:\\n\\n   - Create a package for your extension. This may involve creating an Omniverse Nucleus package, USD (Universal Scene Description) package, or any other appropriate packaging method for your extension.\\n\\n7. **Publish Your Extension**:\\n\\n   - If you want to share your extension with others, you can publish it on the Omniverse Marketplace or share it through other means, such as direct distribution or hosting it on a platform like NVIDIA's Omniverse Package Manager (OPM).\\n\\n8. **Documentation**:\\n\\n   - Provide documentation for your extension, including installation instructions and usage guidelines. Good documentation is crucial for users who want to use or contribute to your extension.\\n\\n9. **Maintain Your Extension**:\\n\\n   - Continue to maintain and update your extension as needed, especially if there are changes in Omniverse Kit or new features you want to add.\\n\\nRemember that creating an extension for NVIDIA Omniverse is a specialized task that may require a deep understanding of the Omniverse platform and its APIs. You should refer to the specific documentation and guidelines provided by NVIDIA for more detailed information on creating extensions for Omniverse. Additionally, you may need to collaborate with the Omniverse community for support and guidance throughout the development process.\\n\\n\""
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[4].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440e8d41-dff7-4db2-b2dc-7049c9e31c63",
   "metadata": {},
   "source": [
    "### docx loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe23da-e49d-4535-b307-d056e7b1335a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e83c7-47c2-412f-9e60-3f032c605289",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1160697c-5f36-4a0f-af4e-303c5dd93916",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### decision_to_use_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e12ce-2bf5-484c-8c01-3dbf1c4a9c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'role':'user','content':f\"\"\"\n",
    "        Query is : \"got this error\n",
    "AttributeError Traceback (most recent call last) Cell In[8], line 1 ----> 1 classes = dst.classifications.classes 2 print(classes)\n",
    "\n",
    "File ~/.pyenv/versions/env-yolov8/lib/python3.10/site-packages/fiftyone/core/dataset.py:357, in Dataset.getattribute(self, name) 354 if getattr(self, \"_deleted\", False): 355 raise ValueError(\"Dataset '%s' is deleted\" % self.name) --> 357 return super().getattribute(name)\n",
    "\n",
    "AttributeError: 'Dataset' object has no attribute 'classifications'\", \\n\n",
    "        Context docs are : \"# we drop sparse_values as they are not needed for this example\\n\\ndataset.documents.drop([\\n\\n\\'metadata\\'], axis\\n\\n1, inplace\\n\\nTrue)\\n\\ndataset.documents.rename(columns\\n\\n={\\n\\n\\'blob\\':\\n\\n\\'metadata\\'}, inplace\\n\\nTrue)\\n\\ndataset.head()\\n\\n0 \\n       417ede5d-39be-498f-b518-f47ed4e53b90 \\n       [0.005949743557721376, 0.01983247883617878, -0... \\n       {\\'chunk\\': 0, \\'text\\': \\'.rst\\n.pdf\\nWelcome to Lan... \\n     \\n       1 \\n       110f550d-110b-4378-b95e-141397fa21bc \\n       [0.009401749819517136, 0.02443608082830906, 0..\\n\\n\\n\\nGPT4 with\\nRetrieval Augmentation over LangChain Docs\\n\\nIn this notebook we\\'ll work through an example of using GPT-4 with\\nretrieval augmentation to answer questions about the LangChain Python\\nlibrary.\\n\\nTo begin we must install the prerequisite libraries:\\n\\n!pip install -qU \\\\\\n\\nopenai==0.27.7 \\\\\\n\\n\"pinecone-client[grpc]\"==2.2.1 \\\\\\n\\npinecone-datasets==\\'0.5.0rc11\\'\\n\\n Note: the above pip install is formatted for\\nJupyter notebooks. If running elsewhere you may need to drop the\\n!\\n\\n\\n\\n. As a native object, it can be inspected for further scriptable properties - using table iteration and \"getmetatable\"\\n\\nin Lua and dir, help etc in Python (among other methods). A notable scriptable object above is fusion - it allows access to all existing Fusion scripting functionality.\\n\\nRunning DaVinci Resolve in headless mode\\n\\n----------------------------------------\\n\\nDaVinci Resolve can be launched in a headless mode without the user interface using the -nogui command line option\\n\\n\\n\\n.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")\\n\\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\\n\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\n\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\n\\nfrom langchain.text_splitter import CharacterTextSplitter\\n\\nfrom langchain.vectorstores import Pinecone\\n\\nfrom langchain\\n\\n\\n\\n\"\n",
    "        \"\"\"},\n",
    "        {'role':'assistant','content': 'No'},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "395cc2eb-e6e7-4cfa-8e52-da24abc81deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_to_use_context(docs,query):\n",
    "\n",
    "    messages = [\n",
    "        {'role':'system','content':\"\"\"\n",
    "        We have large store of knowledgebase texts, and we have one api/fn that based on QUERY returns relevant chunks of texts. Now, it is error-prone, \n",
    "        like even if relevant info is not present in knowledgebase, it does return something.\n",
    "        \n",
    "        **Be mindful in cases of code, It should be returning docs of relevant library, \n",
    "        and methods, not of any random one. !! IF ERROR IS IN FIFTYONE DATASETS METHOD/OBJECT LIBRARY RELATED, CONTEXTS SHOULD NOT BE SOME CODE-RELATED TO ERROR/SOLVING ABOUT LANGCHAIN DATASETS/ETC,SHOTGRID ETC\n",
    "        You have to make the decision , Whether than information is good to use or not.**\n",
    "        \n",
    "        You have to only Answer in \"Yes:Small Reason\" and \"No: Small Reason\". There is no option of Not knowing, or any explanation stuff.\n",
    "         \\n\\n\n",
    "                                \"\"\"},\n",
    "        {'role':'user','content':\"\"\"\n",
    "        Query is : \"how to load a dataset in Fiftyone Library?\", \\n\n",
    "        Context docs are : \". If running elsewhere you may need to drop the\\n!.\\n\\nIn this example, we will use a pre-embedding dataset of the LangChain\\ndocs from python.langchain.readthedocs.com/.\\nIf you'd like to see how we perform the data preparation refer to this notebook.\\n\\nLet's go ahead and download the dataset.\\n\\nfrom pinecone_datasets import load_dataset\\n\\ndataset = load_dataset('langchain-python-docs-text-embedding-ada-002')\"\n",
    "        \"\"\"},\n",
    "        {'role':'assistant','content':'No:As query is about Fiftyone library, and context docs are about embeddings, langchain etc'},\n",
    "        {'role':'user','content':\"\"\"\n",
    "        Query is : \"how to load vectorstore in pinecone\", \\n\n",
    "        Context docs are : \"Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass\"\n",
    "        \"\"\"},\n",
    "        {'role':'assistant','content': 'Yes: Query is about Pinecone, and Context is about PineCone as well'},\n",
    "        {'role':'user','content':\"\"\"\n",
    "        Query is : \"got this error\n",
    "        AttributeError Traceback (most recent call last) Cell In[8], line 1 ----> 1 classes = dst.classifications.classes 2 print(classes)\n",
    "        \n",
    "        File ~/.pyenv/versions/env-yolov8/lib/python3.10/site-packages/fiftyone/core/dataset.py:357, in Dataset.getattribute(self, name) 354 if getattr(self, \"_deleted\", False): 355 raise ValueError(\"Dataset '%s' is deleted\" % self.name) --> 357 return super().getattribute(name)\n",
    "        \n",
    "        AttributeError: 'Dataset' object has no attribute 'classifications'\", \\n\n",
    "        Context docs are : \"# we drop sparse_values as they are not needed for this example\\n\\ndataset.documents.drop([\\n\\n\\'metadata\\'], axis\\n\\n1, inplace\\n\\nTrue)\\n\\ndataset.documents.rename(columns\\n\\n={\\n\\n\\'blob\\':\\n\\n\\'metadata\\'}, inplace\\n\\nTrue)\\n\\ndataset.head()\\n\\n0 \\n       417ede5d-39be-498f-b518-f47ed4e53b90 \\n       [0.005949743557721376, 0.01983247883617878, -0... \\n       {\\'chunk\\': 0, \\'text\\': \\'.rst\\n.pdf\\nWelcome to Lan... \\n     \\n       1 \\n       110f550d-110b-4378-b95e-141397fa21bc \\n       [0.009401749819517136, 0.02443608082830906, 0..\\n\\n\\n\\nGPT4 with\\nRetrieval Augmentation over LangChain Docs\\n\\nIn this notebook we\\'ll work through an example of using GPT-4 with\\nretrieval augmentation to answer questions about the LangChain Python\\nlibrary.\\n\\nTo begin we must install the prerequisite libraries:\\n\\n!pip install -qU \\\\\\n\\nopenai==0.27.7 \\\\\\n\\n\"pinecone-client[grpc]\"==2.2.1 \\\\\\n\\npinecone-datasets==\\'0.5.0rc11\\'\\n\\n Note: the above pip install is formatted for\\nJupyter notebooks. If running elsewhere you may need to drop the\\n!\\n\\n\\n\\n. As a native object, it can be inspected for further scriptable properties - using table iteration and \"getmetatable\"\\n\\nin Lua and dir, help etc in Python (among other methods). A notable scriptable object above is fusion - it allows access to all existing Fusion scripting functionality.\\n\\nRunning DaVinci Resolve in headless mode\\n\\n----------------------------------------\\n\\nDaVinci Resolve can be launched in a headless mode without the user interface using the -nogui command line option\\n\\n\\n\\n.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")\\n\\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\\n\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\n\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\n\\nfrom langchain.text_splitter import CharacterTextSplitter\\n\\nfrom langchain.vectorstores import Pinecone\\n\\nfrom langchain\\n\\n\\n\\n\"\n",
    "        \"\"\"},\n",
    "        {'role':'assistant','content': \"No: As query is for Fiftyone Related, and context is about Langchain, GPT4,etc\"},\n",
    "\n",
    "\n",
    "        {'role':'user','content':f\"\"\"\n",
    "        Query is : \"{query}\" \\n\n",
    "        Context docs are : \"{docs}\"\n",
    "        \"\"\"}\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "                            #model=\"gpt-3.5-turbo-16k\",\n",
    "                            model = 'gpt-4',\n",
    "                            messages=messages,\n",
    "                            temperature=0\n",
    "                                    )\n",
    "    return response['choices'][0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "13b51667-0860-413f-8b96-4762dfea5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_to_use_context(docs,query):\n",
    "\n",
    "    messages = [\n",
    "        {'role':'system','content':\"\"\"\n",
    "        We have large store of knowledgebase texts, and we have one api/fn that based on QUERY returns relevant chunks of texts. Now, it is error-prone, \n",
    "        like even if relevant info is not present in knowledgebase, it does return something.\n",
    "        \n",
    "        **Be mindful in cases of code, It should be returning docs of relevant library, \n",
    "        and methods, not of any random one. !! IF ERROR IS IN FIFTYONE DATASETS METHOD/OBJECT LIBRARY RELATED, CONTEXTS SHOULD NOT BE SOME CODE-RELATED TO ERROR/SOLVING ABOUT LANGCHAIN DATASETS/ETC,SHOTGRID ETC\n",
    "        You have to make the decision , Whether than information is good to use or not.**\n",
    "        \n",
    "        You have to only Answer in \"Yes:Small Reason\" and \"No: Small Reason\". There is no option of Not knowing, or any explanation stuff.\n",
    "         \\n\\n\n",
    "                                \"\"\"},\n",
    "        {'role':'user','content':\"\"\"\n",
    "        Query is : \"got this error\n",
    "        AttributeError Traceback (most recent call last) Cell In[8], line 1 ----> 1 classes = dst.classifications.classes 2 print(classes)\n",
    "        \n",
    "        File ~/.pyenv/versions/env-yolov8/lib/python3.10/site-packages/fiftyone/core/dataset.py:357, in Dataset.getattribute(self, name) 354 if getattr(self, \"_deleted\", False): 355 raise ValueError(\"Dataset '%s' is deleted\" % self.name) --> 357 return super().getattribute(name)\n",
    "        \n",
    "        AttributeError: 'Dataset' object has no attribute 'classifications'\", \\n\n",
    "        Context docs are : \"# we drop sparse_values as they are not needed for this example\\n\\ndataset.documents.drop([\\n\\n\\'metadata\\'], axis\\n\\n1, inplace\\n\\nTrue)\\n\\ndataset.documents.rename(columns\\n\\n={\\n\\n\\'blob\\':\\n\\n\\'metadata\\'}, inplace\\n\\nTrue)\\n\\ndataset.head()\\n\\n0 \\n       417ede5d-39be-498f-b518-f47ed4e53b90 \\n       [0.005949743557721376, 0.01983247883617878, -0... \\n       {\\'chunk\\': 0, \\'text\\': \\'.rst\\n.pdf\\nWelcome to Lan... \\n     \\n       1 \\n       110f550d-110b-4378-b95e-141397fa21bc \\n       [0.009401749819517136, 0.02443608082830906, 0..\\n\\n\\n\\nGPT4 with\\nRetrieval Augmentation over LangChain Docs\\n\\nIn this notebook we\\'ll work through an example of using GPT-4 with\\nretrieval augmentation to answer questions about the LangChain Python\\nlibrary.\\n\\nTo begin we must install the prerequisite libraries:\\n\\n!pip install -qU \\\\\\n\\nopenai==0.27.7 \\\\\\n\\n\"pinecone-client[grpc]\"==2.2.1 \\\\\\n\\npinecone-datasets==\\'0.5.0rc11\\'\\n\\n Note: the above pip install is formatted for\\nJupyter notebooks. If running elsewhere you may need to drop the\\n!\\n\\n\\n\\n. As a native object, it can be inspected for further scriptable properties - using table iteration and \"getmetatable\"\\n\\nin Lua and dir, help etc in Python (among other methods). A notable scriptable object above is fusion - it allows access to all existing Fusion scripting functionality.\\n\\nRunning DaVinci Resolve in headless mode\\n\\n----------------------------------------\\n\\nDaVinci Resolve can be launched in a headless mode without the user interface using the -nogui command line option\\n\\n\\n\\n.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")\\n\\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\\n\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\n\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\n\\nfrom langchain.text_splitter import CharacterTextSplitter\\n\\nfrom langchain.vectorstores import Pinecone\\n\\nfrom langchain\\n\\n\\n\\n\"\n",
    "        \"\"\"},\n",
    "        {'role':'assistant','content': \"No: As query is for Fiftyone Related, and context is about Langchain, GPT4,etc\"},\n",
    "\n",
    "\n",
    "        {'role':'user','content':f\"\"\"\n",
    "        Query is : \"{query}\" \\n\n",
    "        Context docs are : \"{docs}\"\n",
    "        \"\"\"}\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "                            #model=\"gpt-3.5-turbo-16k\",\n",
    "                            model = 'gpt-4',\n",
    "                            messages=messages,\n",
    "                            temperature=0\n",
    "                                    )\n",
    "    return response['choices'][0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "806b9953-7191-4a1f-a748-42cc6fcf515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is Davinci Resolve'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cfd4b694-b7e5-4acc-8335-1ccdaaec016c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. Resolve can run one or more Workflow Integration Plugins at the same time.\\n\\nUsers can write their own Workflow Integration Plugin (an Electron app) which could be loaded into DaVinci Resolve Studio. To interact with Resolve, Resolve\\'s JavaScript APIs can be used from the plugin.\\n\\nAlternatively, a Python or Lua script can be invoked, with the option of a user interface built with Resolve\\'s built-in Qt-based UIManager, or with an external GUI manager\\n\\n\\n\\n. When DaVinci Resolve is launched using this option, the user interface is disabled.\\n\\nHowever, the various scripting APIs will continue to work as expected.\\n\\nBasic Resolve API\\n\\n-----------------\\n\\nSome commonly used API functions are described below (*). As with the resolve object, each object is inspectable for properties and functions.\\n\\nResolve\\n\\n  Fusion()                                        --> Fusion\\n\\n\\n\\n#!/usr/bin/env python\\n\\n    import DaVinciResolveScript as dvr_script\\n\\n    resolve = dvr_script.scriptapp(\"Resolve\")\\n\\n    fusion = resolve.Fusion()\\n\\n    projectManager = resolve.GetProjectManager()\\n\\n    projectManager.CreateProject(\"Hello World\")\\n\\nThe resolve object is the fundamental starting point for scripting via Resolve\\n\\n\\n\\n. Apart from this README.txt file, this package contains following folders:\\n\\nExamples: containing some representative sample plugin, and a sample script.\\n\\nScripts: containing some sample workflow scripts to interact with Resolve.\\n\\nOverview\\n\\n--------\\n\\nDaVinci Resolve Studio now supports Workflow Integration Plugins to be loaded and communicate with Resolve. Resolve can run one or more Workflow Integration Plugins at the same time\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = return_context_docs(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7446caa-19f1-413d-860d-d0641991b2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes: Query is about DaVinci Resolve, and context is also about DaVinci Resolve.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_to_use_context(docs, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "282be29a-b6d3-443c-bcdf-e89bf09f53d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No: As query is about Fiftyone library, and context docs are about embeddings, langchain etc'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"To list the classes in the \"classifications\" field of your loaded FIFTYONE-dataset, you can use the following code:\n",
    "got this error\n",
    "AttributeError Traceback (most recent call last) Cell In[8], line 1 ----> 1 classes = dst.classifications.classes 2 print(classes)\n",
    "\n",
    "File ~/.pyenv/versions/env-yolov8/lib/python3.10/site-packages/fiftyone/core/dataset.py:357, in Dataset.getattribute(self, name) 354 if getattr(self, \"_deleted\", False): 355 raise ValueError(\"Dataset '%s' is deleted\" % self.name) --> 357 return super().getattribute(name)\n",
    "\n",
    "AttributeError: 'Dataset' object has no attribute 'classifications' \"\"\"\n",
    "_,docs = return_context_docs(query)\n",
    "decision_to_use_context(docs, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1790158f-48cb-403c-a790-489f0e105a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\". If running elsewhere you may need to drop the\\n!.\\n\\nIn this example, we will use a pre-embedding dataset of the LangChain\\ndocs from python.langchain.readthedocs.com/.\\nIf you'd like to see how we perform the data preparation refer to this notebook.\\n\\nLet's go ahead and download the dataset.\\n\\nfrom pinecone_datasets import load_dataset\\n\\ndataset = load_dataset('langchain-python-docs-text-embedding-ada-002')\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4a539a0c-1290-45ba-b71e-68db3029e44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No: The context is about a chatbot model and does not provide any information about the Fiftyone error in the query.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"i am getting this error\n",
    "ValueError: Sample field 'classifications' is not a [<class 'fiftyone.core.labels.Classification'>] instance; found <class 'fiftyone.core.labels.Classifications'>\n",
    "with this code\n",
    " \"\"\"\n",
    "_,docs = return_context_docs(query)\n",
    "decision_to_use_context(docs, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "954a59d8-59e3-47c1-9d87-f79680aa6f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes: The context provides information about installing and setting up Pinecone, which is relevant to the query about loading a vectorstore in Pinecone.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"How to load vectorstore in pinecone?\n",
    " \"\"\"\n",
    "_,docs = return_context_docs(query)\n",
    "decision_to_use_context(docs, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a15214db-4b0c-44a8-9e71-3c713ddd8993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "370a9610-64c1-43f8-a639-48e80d93cd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No: The context is about loading a dataset from LangChain, not FiftyOne.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"This is how I loaded fiftyone dataset: dst = fo.load_dataset('test-dataset-cnt91-v09.09.23')\n",
    "\n",
    "How can I list classes in field: \"classifications\"\n",
    " \"\"\"\n",
    "_,docs = return_context_docs(query)\n",
    "decision_to_use_context(docs, query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406d5eb-29f2-4079-8117-96ef39efee8a",
   "metadata": {},
   "source": [
    "# Docs fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d62f8a87-4608-4b2b-9720-6d9dec1f2662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mConversationSummaryBufferMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhuman_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Human'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mai_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AI'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasePromptTemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'new_lines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msummary_message_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'langchain.schema.messages.SystemMessage'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchat_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseChatMessageHistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_messages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'history'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mConversationSummaryBufferMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseChatMemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSummarizerMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Buffer with summarizer for storing conversation memory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"history\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mmemory_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Will always return list of memory variables.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        :meta private:\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mload_memory_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Return history buffer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfirst_messages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_message_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_messages\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_messages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfinal_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfinal_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_buffer_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mai_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai_prefix\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfinal_buffer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mroot_validator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_prompt_input_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Validate that prompt input variables are consistent.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprompt_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mexpected_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"new_lines\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mexpected_keys\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Got unexpected prompt input variables. The prompt expects \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"{prompt_variables}, but it should have {expected_keys}.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_tokens_from_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpruned_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mwhile\u001b[0m \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mpruned_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_tokens_from_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_new_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mpruned_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Clear memory contents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/memory/summary_buffer.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConversationSummaryBufferMemory??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a52214dc-0760-4474-ad65-113f81a1f007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = main_chat_fn('What is PineCone?',load_vstore,'Chatbot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1734fc48-78e3-475b-a54e-059476d152aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pinecone is a vector database with broad functionality. It is used for storing and querying high-dimensional vectors efficiently.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bd58593-dd66-4005-a6e3-365156a30a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/pinecone_docs.txt'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = load_vstore.similarity_search('What is pinecone',k=1)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80bb2871-af97-4f4f-adde-e935b6abf868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "545b2b64-9399-49f7-b3c6-2680a4e4f4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b326aa0-5a69-4a4e-8a57-901860865e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = load_vstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f7efbe2-6348-4558-8412-ebd8e7fa08ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass\\n# initialize pinecone pinecone.init( api_key=os.getenv(\"PINECONE_API_KEY\"),  # find at app.pinecone.io environment=os.getenv(\"PINECONE_ENV\"),  # next to api key in console )\\n\\nindex_name = \"langchain\\n\\n\\n\\ndemo\"\\n\\n# First, check if our index already exists. If it doesn\\'t, we create it if index_name not in pinecone.list_indexes(): # we create a new index pinecone.create_index( name=index_name, metric=\\'cosine\\', dimension=1536 )\\n.vectorstores import Pinecone\\n\\nfrom langchain.document_loaders import TextLoader\\n\\nAPI Reference:\\n\\nOpenAIEmbeddings\\n\\nCharacterTextSplitter\\n\\nPinecone\\n\\nTextLoader\\n\\nfrom langchain.document_loaders import TextLoader\\n\\nloader = TextLoader(\"../../../state_of_the_union.txt\")\\n\\ndocuments = loader.load()\\n\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\n\\ndocs = text_splitter.split_documents(documents)\\n\\nembeddings = OpenAIEmbeddings()\\n\\nAPI Reference:\\n\\nTextLoader\\n\\nimport pinecone\\n# initialize connection to pinecone (get API key at app.pinecone.io)\\n\\napi_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\\n\\n# find your environment next to the api key in pinecone console\\n\\nenv = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\\n\\npinecone.init(api_key=api_key, environment=env)\\n\\nindex_name = \\'gpt-4-langchain-docs-fast\\'\\n\\nimport time\\n\\n# check if index already exists (it shouldn\\'t if this is first time)\\n\\nif index_name not in pinecone.list_indexes():\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_context_docs('What is PineCone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ca955243-796d-4856-8dc2-95024da5e2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "ee3c28cc-35e9-4c01-b07b-820dca344bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"#\",\"##\", \"###\", \"\\\\n\\\\n\",\"\\\\n\",\".\", '\\n'],\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0d15e956-be2c-4902-b404-78535c03050d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "to_check = \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "PandasAI is a pandas library addition that utilizes generative AI models from OpenAI to produce insights from dataframes. It allows you to use a text prompt to generate queries and obtain information from your data. Here is an example of how to use PandasAI:\n",
    "\n",
    "1. Install the pandasai library:\n",
    "   ```\n",
    "   !pip install -q pandasai\n",
    "   ```\n",
    "\n",
    "2. Import the necessary libraries and modules:\n",
    "   ```python\n",
    "   import pandas as pd\n",
    "   import numpy as np\n",
    "   from pandasai import PandasAI\n",
    "   from pandasai.llm.openai import OpenAI\n",
    "   ```\n",
    "\n",
    "3. Create a dataframe:\n",
    "   ```python\n",
    "   data_dict = {\n",
    "       \"country\": [\n",
    "           \"Delhi\",\n",
    "           \"Mumbai\",\n",
    "           \"Kolkata\",\n",
    "           \"Chennai\",\n",
    "           \"Jaipur\",\n",
    "           \"Lucknow\",\n",
    "           \"Pune\",\n",
    "           \"Bengaluru\",\n",
    "           \"Amritsar\",\n",
    "           \"Agra\",\n",
    "           \"Kola\",\n",
    "       ],\n",
    "       \"annual tax collected\": [\n",
    "           19294482072,\n",
    "           28916155672,\n",
    "           24112550372,\n",
    "           34358173362,\n",
    "           17454337886,\n",
    "           11812051350,\n",
    "           16074023894,\n",
    "           14909678554,\n",
    "           43807565410,\n",
    "           146318441864,\n",
    "           np.nan,\n",
    "       ],\n",
    "       \"happiness_index\": [\n",
    "           9.94,\n",
    "           7.16,\n",
    "           6.35,\n",
    "           8.07,\n",
    "           6.98,\n",
    "           6.1,\n",
    "           4.23,\n",
    "           8.22,\n",
    "           6.87,\n",
    "           3.36,\n",
    "           np.nan,\n",
    "       ],\n",
    "   }\n",
    "\n",
    "   df = pd.DataFrame(data_dict)\n",
    "   ```\n",
    "\n",
    "4. Initialize the OpenAI model and PandasAI:\n",
    "   ```python\n",
    "   llm = OpenAI(api_token='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')\n",
    "   pandas_ai = PandasAI(llm, conversational=False)\n",
    "   ```\n",
    "\n",
    "5. Use PandasAI to generate insights from the dataframe:\n",
    "   ```python\n",
    "   response = pandas_ai(df, \"What is the index of Pune?\")\n",
    "   print(response)\n",
    "   ```\n",
    "\n",
    "Please note that the code above assumes you have obtained an API token from OpenAI. Make sure to replace `'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'` with your actual API token.\n",
    "\n",
    "Let me know if you need any further assistance with PandasAI or any other topic.\n",
    "\n",
    "\n",
    " ----------------------------------- \n",
    "\n",
    "\n",
    "To use pandasai, you need to install it first. You can install pandasai using the following command:\n",
    "\n",
    "```\n",
    "pip install pandasai\n",
    "```\n",
    "\n",
    "Once pandasai is installed, you can import it in your Python script or notebook using the following line of code:\n",
    "\n",
    "```python\n",
    "import pandasai\n",
    "```\n",
    "\n",
    "After importing pandasai, you can use its functions and classes to perform various data analysis tasks. Make sure to refer to the pandasai documentation for detailed usage instructions and examples.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "f92bcc80-2e05-423b-9597-c684442f95bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs  = text_splitter.split_text(to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "cc24edf2-4fd9-43bd-9213-59024b2b6cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "489c2717-8dfb-4d6d-9f86-667c6ff80201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PandasAI is a pandas library addition that utilizes generative AI models from OpenAI to produce insights from dataframes. It allows you to use a text prompt to generate queries and obtain information from your data. Here is an example of how to use PandasAI:\\n\\n1. Install the pandasai library:\\n   ```\\n   !pip install -q pandasai\\n   ```\\n\\n2. Import the necessary libraries and modules:\\n   ```python\\n   import pandas as pd\\n   import numpy as np\\n   from pandasai import PandasAI\\n   from pandasai.llm.openai import OpenAI\\n   ```\\n\\n3. Create a dataframe:\\n   ```python\\n   data_dict = {\\n       \"country\": [\\n           \"Delhi\",\\n           \"Mumbai\",\\n           \"Kolkata\",\\n           \"Chennai\",\\n           \"Jaipur\",\\n           \"Lucknow\",\\n           \"Pune\",\\n           \"Bengaluru\",\\n           \"Amritsar\",\\n           \"Agra\",\\n           \"Kola\",\\n       ],\\n       \"annual tax collected\": [\\n           19294482072,\\n           28916155672,\\n           24112550372,\\n           34358173362,\\n           17454337886,\\n           11812051350,\\n           16074023894,\\n           14909678554,\\n           43807565410,\\n           146318441864,\\n           np.nan,\\n       ],\\n       \"happiness_index\": [\\n           9.94,\\n           7.16,\\n           6.35,\\n           8.07,\\n           6.98,\\n           6.1,\\n           4.23,\\n           8.22,\\n           6.87,\\n           3.36,\\n           np.nan,\\n       ],\\n   }\\n\\n   df = pd.DataFrame(data_dict)\\n   ```\\n\\n4'"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "9ec72e62-c7ab-4d93-bce2-197211846e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0cdf84a5-3545-4a0f-9c4b-09e853996916',\n",
       " '48cbd576-1b95-4d84-9c33-eb0fa3224db6']"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_vstore.add_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0307deb0-899e-412d-b81f-01a94f2b951a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-3.10.2",
   "language": "python",
   "name": "langchain-3.10.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
