{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9129cad9-2472-4d11-90cb-cf6ddfcabd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e7048a2-402e-4609-a52d-5c4b520775d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import openai\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.chains import ConversationChain, ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfecbf4c-9cfe-478e-a39a-c3770c2ebc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ubuntu/workspace/Creds')\n",
    "from openai_config import OPENAI_API_KEY\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "335c8af0-1c17-4012-b7f6-e5386ac156f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "template1 = \"\"\" You are a chatbot, that has memory, with older memory being summarized dynamically. So, you have answer with best knowledge, and using the previous context:\n",
    "Context:\n",
    "\n",
    "{context}\n",
    "---\n",
    "this is the running chat-history:\n",
    "{chat_history}\n",
    "\n",
    "---\n",
    "### Input : \n",
    "{question}\n",
    "\n",
    "### Response:\n",
    "\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['context','question', 'chat_history'], template = template1\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-3.5-turbo-16k',\n",
    "                openai_api_key= OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "accc7289-bd36-4473-bf17-7f46a8efd854",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "\n",
    "Progressively summarize the lines of conversations, but keep in mind, that you have to update any code-related thing, or configuration related thing, with great care. You are acting as part of\\\n",
    " a customized chatbot, which has to help with advanced tools like Omniverse, ShotGrid, USDA files, and 3D rendering etc. So, most of time your answers will be rectified by User Input, and \n",
    " **you have to store updated information in summaries.** As, this info will be also saved to vectorstores, for future references. \n",
    " Current Summary:\n",
    " {chat_history}\n",
    "\n",
    " New lines of conversation :\n",
    " {question}\n",
    "\n",
    " New_Summary:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6502f261-1c2d-4079-b5e5-bc57c4038b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(input_variables=['chat_history', 'question'], template = template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f752a0a-d059-4bae-b64b-0457b0522418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chat_history', 'question']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.input_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b78b535-5cb3-46f2-aed6-b00dd484dd1b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationSummaryBufferMemory\n__root__\n  Got unexpected prompt input variables. The prompt expects ['chat_history', 'question'], but it should have {'summary', 'new_lines'}. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m memory \u001b[38;5;241m=\u001b[39m \u001b[43mConversationSummaryBufferMemory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43minput_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43mai_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNew_Summary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/load/serializable.py:90\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ConversationSummaryBufferMemory\n__root__\n  Got unexpected prompt input variables. The prompt expects ['chat_history', 'question'], but it should have {'summary', 'new_lines'}. (type=value_error)"
     ]
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(prompt = prompt, memory_key= 'chat_history',\n",
    "                                         llm = llm,input_key='question', output_key='chat_history',\n",
    "                                        ai_prefix='New_Summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "607d045a-d658-42dd-abce-613f13599ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'human_prefix': 'Human',\n",
       " 'ai_prefix': 'AI',\n",
       " 'llm': {'model_name': 'gpt-3.5-turbo-16k',\n",
       "  'model': 'gpt-3.5-turbo-16k',\n",
       "  'request_timeout': None,\n",
       "  'max_tokens': None,\n",
       "  'stream': False,\n",
       "  'n': 1,\n",
       "  'temperature': 0.7,\n",
       "  '_type': 'openai-chat'},\n",
       " 'prompt': {'input_variables': ['summary', 'new_lines'],\n",
       "  'output_parser': None,\n",
       "  'partial_variables': {},\n",
       "  'template': '\\n\\nProgressively summarize the lines of conversations, but keep in mind, that you have to update any code-related thing, or configuration related thing, with great care. You are acting as part of a customized chatbot, which has to help with advanced tools like Omniverse, ShotGrid, USDA files, and 3D rendering etc. So, most of time your answers will be rectified by User Input, and \\n **you have to store updated information in summaries.** As, this info will be also saved to vectorstores, for future references. \\n Current Summary:\\n {summary}\\n\\n New lines of conversation :\\n {new_lines}\\n\\n New Summary:\\n\\n',\n",
       "  'template_format': 'f-string',\n",
       "  'validate_template': True,\n",
       "  '_type': 'prompt'},\n",
       " 'summary_message_cls': langchain.schema.messages.SystemMessage,\n",
       " 'chat_memory': {'messages': []},\n",
       " 'output_key': None,\n",
       " 'input_key': None,\n",
       " 'return_messages': False,\n",
       " 'max_token_limit': 2000,\n",
       " 'moving_summary_buffer': '',\n",
       " 'memory_key': 'chat_history'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.dict()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1773c26a-65fe-4375-aa64-441891c7f057",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mConversationSummaryBufferMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhuman_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Human'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mai_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AI'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasePromptTemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'new_lines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msummary_message_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'langchain.schema.messages.SystemMessage'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchat_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseChatMessageHistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_messages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'history'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mConversationSummaryBufferMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseChatMemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSummarizerMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Buffer with summarizer for storing conversation memory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"history\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mmemory_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Will always return list of memory variables.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        :meta private:\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mload_memory_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Return history buffer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfirst_messages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_message_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_messages\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_messages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfinal_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfinal_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_buffer_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mai_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai_prefix\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfinal_buffer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mroot_validator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_prompt_input_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Validate that prompt input variables are consistent.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprompt_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mexpected_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"new_lines\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mexpected_keys\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Got unexpected prompt input variables. The prompt expects \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"{prompt_variables}, but it should have {expected_keys}.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_tokens_from_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpruned_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mwhile\u001b[0m \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mpruned_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_tokens_from_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_new_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mpruned_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Clear memory contents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/memory/summary_buffer.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConversationSummaryBufferMemory??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d21da8-1d55-4a2e-a3df-4a98629f767d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mload_qa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchain_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'stuff'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mload_qa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchain_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Load question answering chain.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        llm: Language Model to use in the chain.\u001b[0m\n",
       "\u001b[0;34m        chain_type: Type of document combining chain to use. Should be one of \"stuff\",\u001b[0m\n",
       "\u001b[0;34m            \"map_reduce\", \"map_rerank\", and \"refine\".\u001b[0m\n",
       "\u001b[0;34m        verbose: Whether chains should be run in verbose mode or not. Note that this\u001b[0m\n",
       "\u001b[0;34m            applies to all chains that make up the final chain.\u001b[0m\n",
       "\u001b[0;34m        callback_manager: Callback manager to use for the chain.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Returns:\u001b[0m\n",
       "\u001b[0;34m        A chain to use for question answering.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloader_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoadingCallable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_stuff_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"map_reduce\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_map_reduce_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"refine\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_refine_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"map_rerank\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_map_rerank_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mchain_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"Got unsupported chain type: {chain_type}. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"Should be one of {loader_mapping.keys()}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mloader_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchain_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/question_answering/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_qa_chain??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8072362a-3f97-43d0-bd85-a08805b4de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm,chain_type='stuff',prompt = prompt, memory = memory, verbose =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92757f16-0a35-4892-9b45-cebc914187d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model = 'text-embedding-ada-002',\n",
    "                              openai_api_key= OPENAI_API_KEY)\n",
    "load_vstore = FAISS.load_local('/home/ubuntu/workspace/Temp/04Oct-FAISS', embeddings = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "911c8eec-fc26-4fc6-be9d-2bef30d486e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_documents', 'question', 'chat_history']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.input_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1a5101c9-c626-4c9c-a74f-e3fad634a4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = load_vstore.similarity_search('what is usd file?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0c19db3d-27b4-43b2-89da-3d906e9332fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = load_vstore.similarity_search_with_score('how to load vectorstore in pinecone')\n",
    "docs[0][0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "716dd5f5-c9f6-43cd-945e-aec49169d8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.574669"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "78ba56dd-dfc6-4dab-9215-f9db154e598e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49574402"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = load_vstore.similarity_search_with_score('how to load a dataset in Fiftyone Library?')\n",
    "docs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b6e0d266-497f-4f57-b0a1-f7c0813007b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\". If running elsewhere you may need to drop the\\n!.\\n\\nIn this example, we will use a pre-embedding dataset of the LangChain\\ndocs from python.langchain.readthedocs.com/.\\nIf you'd like to see how we perform the data preparation refer to this notebook.\\n\\nLet's go ahead and download the dataset.\\n\\nfrom pinecone_datasets import load_dataset\\n\\ndataset = load_dataset('langchain-python-docs-text-embedding-ada-002')\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0][0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1946a21c-a5e3-41fe-a129-75eb2d63d586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m You are a chatbot, that has memory, with older memory being summarized dynamically. So, you have answer with best knowledge, and using the previous context:\n",
      "Context:\n",
      "\n",
      ". Additional third-party modules may be imported for access to asset-management systems as desired.\n",
      "\n",
      "UIManager Introduction\n",
      "\n",
      "----------------------\n",
      "\n",
      "There are two main objects needed to manage a window, the UIManager that handles layout, and the UIDispatcher that manages interaction events, accessed as follows:\n",
      "\n",
      "\tui = fusion.UIManager()\n",
      "\n",
      "\tdispatcher = bmd\n",
      "\n",
      ".js modules>\n",
      "\n",
      "    js/\n",
      "\n",
      "        <supporting js files>\n",
      "\n",
      "    css/\n",
      "\n",
      "        <css files containing styling info>\n",
      "\n",
      "    img/\n",
      "\n",
      "        <image files>\n",
      "\n",
      "Workflow Integration Plugins root directory\n",
      "\n",
      "-------------------------------------------\n",
      "\n",
      "User should place their Workflow Integration Plugin under the following directory:\n",
      "\n",
      "    Mac OS X:\n",
      "\n",
      "        \"/Library/Application Support/Blackmagic Design/DaVinci Resolve/Workflow Integration Plugins/\"\n",
      "\n",
      "    Windows:\n",
      "\n",
      "\\n\n",
      "\n",
      ".\n",
      "\n",
      "Further Information\n",
      "\n",
      "-------------------\n",
      "\n",
      "This document provides a basic introduction only, and does not list all available UIManager elements and attributes. As UIManager is based on Qt, you can refer to the Qt documentation at https://doc.qt.io/qt-5/qwidget.html for more information on element types and their attributes. There are also many third-party examples and discussions available on user forums for DaVinci Resolve and Fusion Studio.\n",
      "---\n",
      "this is the running chat-history:\n",
      "\n",
      "\n",
      "---\n",
      "### Input : \n",
      "what is usd file?\n",
      "\n",
      "### Response:\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "One input key expected got ['input_documents', 'chat_history', 'question']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwhat is usd file?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43minput_documents\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_history\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/base.py:512\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    508\u001b[0m         _output_key\n\u001b[1;32m    509\u001b[0m     ]\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    513\u001b[0m         _output_key\n\u001b[1;32m    514\u001b[0m     ]\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    517\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    518\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    519\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    520\u001b[0m     )\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/base.py:314\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m--> 314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n\u001b[1;32m    318\u001b[0m     final_outputs[RUN_KEY] \u001b[38;5;241m=\u001b[39m RunInfo(run_id\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mrun_id)\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/base.py:410\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[0;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/memory/summary_buffer.py:58\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], outputs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprune()\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/memory/chat_memory.py:35\u001b[0m, in \u001b[0;36mBaseChatMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], outputs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     input_str, output_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_input_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39madd_user_message(input_str)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39madd_ai_message(output_str)\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/memory/chat_memory.py:22\u001b[0m, in \u001b[0;36mBaseChatMemory._get_input_output\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_input_output\u001b[39m(\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m, inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any], outputs: Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]\n\u001b[1;32m     20\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m         prompt_input_key \u001b[38;5;241m=\u001b[39m \u001b[43mget_prompt_input_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory_variables\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m         prompt_input_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/memory/utils.py:19\u001b[0m, in \u001b[0;36mget_prompt_input_key\u001b[0;34m(inputs, memory_variables)\u001b[0m\n\u001b[1;32m     17\u001b[0m prompt_input_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(inputs)\u001b[38;5;241m.\u001b[39mdifference(memory_variables \u001b[38;5;241m+\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompt_input_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne input key expected got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_input_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prompt_input_keys[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: One input key expected got ['input_documents', 'chat_history', 'question']"
     ]
    }
   ],
   "source": [
    "answer = chain.run(question = 'what is usd file?',input_documents = docs, chat_history = '')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713bf29-e7d8-4963-97b5-647da015adaa",
   "metadata": {},
   "source": [
    "# Following docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5f3b1872-56cb-4135-99d6-48097216cabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm,\n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=40),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7da15c4-c549-4852-a4f7-5e178046064b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Chain.dict of ConversationChain(memory=ConversationSummaryBufferMemory(llm=ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-16k', openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy=''), max_token_limit=40), verbose=True, llm=ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo-16k', openai_api_key='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', openai_api_base='', openai_organization='', openai_proxy=''))>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "535bb6e5-7c31-421e-9ee1-fda389a1b9c2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello! I'm doing great. I've been busy learning and helping users like you. How can I assist you today?\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Hi, what's up?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dd28c7b8-d63f-4cab-aa42-af6e0eaab38c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI and asks what's going on.\n",
      "AI: Hello! I'm doing great. I've been busy learning and helping users like you. How can I assist you today?\n",
      "Human: what are you doing?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'I am an AI language model developed by OpenAI. My main purpose is to assist users by providing information and answering questions to the best of my abilities. I analyze and process large amounts of data to generate responses. Is there anything specific you would like to know or discuss?'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"what are you doing?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "07a8a5e1-f7db-430e-9d32-1815ca117da1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: The human greets the AI and asks what's going on. The AI responds by saying it's doing great and has been busy learning and helping users. The AI explains that its main purpose is to assist users by providing information and answering questions. It analyzes and processes large amounts of data to generate responses. The AI asks if there's anything specific the human would like to know or discuss.\n",
      "Human: What was my first question?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your first question was \"What\\'s going on?\"'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"What was my first question?\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "428e3f83-e0b5-4db3-bdcd-2d5eead044e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['history', 'input'], template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\\n\\nCurrent conversation:\\n{history}\\nHuman: {input}\\nAI:')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "69d7cc19-0582-464c-945a-482ae71e819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_2 = \"\"\" You are a chatbot, that has memory, with older memory being summarized dynamically. So, you have answer with best knowledge, and using the previous context:\n",
    "Context:\n",
    "\n",
    "{context}\n",
    "---\n",
    "Current conversation:\\n{history}\\nHuman: {input}\\nAI:\n",
    "\n",
    "\"\"\"\n",
    "prompt2 = PromptTemplate(input_variables=['history','input','context'], template = template_2,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5c2159ce-d6d2-4bdb-bbd4-efe6c828a2b7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for ConversationChain\n__root__\n  Got unexpected prompt input variables. The prompt expects ['history', 'input', 'context'], but got ['history'] as inputs from memory, and input as the normal input key. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m conversation_with_summary2 \u001b[38;5;241m=\u001b[39m \u001b[43mConversationChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# We set a very low max_token_limit for the purposes of testing.\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mConversationSummaryBufferMemory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_token_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/load/serializable.py:90\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ConversationChain\n__root__\n  Got unexpected prompt input variables. The prompt expects ['history', 'input', 'context'], but got ['history'] as inputs from memory, and input as the normal input key. (type=value_error)"
     ]
    }
   ],
   "source": [
    "conversation_with_summary2 = ConversationChain(\n",
    "    llm=llm,\n",
    "    prompt = prompt2,\n",
    "    # We set a very low max_token_limit for the purposes of testing.\n",
    "    memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=40),\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c7c1ff-7480-4f1b-b3a1-0ad94d63d14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_with_summary2 = load_qa_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a1b87c46-4858-4e6f-9c46-9b68d7662e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['summary', 'new_lines'], template='Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.memory.prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f5ef4ae6-dca4-4c8c-88b5-b81aa1d086ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['response']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.output_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4ca9e9bd-f84a-4ca8-bb71-f31597816fef",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mload_qa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchain_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'stuff'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mload_qa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchain_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Load question answering chain.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m        llm: Language Model to use in the chain.\u001b[0m\n",
       "\u001b[0;34m        chain_type: Type of document combining chain to use. Should be one of \"stuff\",\u001b[0m\n",
       "\u001b[0;34m            \"map_reduce\", \"map_rerank\", and \"refine\".\u001b[0m\n",
       "\u001b[0;34m        verbose: Whether chains should be run in verbose mode or not. Note that this\u001b[0m\n",
       "\u001b[0;34m            applies to all chains that make up the final chain.\u001b[0m\n",
       "\u001b[0;34m        callback_manager: Callback manager to use for the chain.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Returns:\u001b[0m\n",
       "\u001b[0;34m        A chain to use for question answering.\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloader_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoadingCallable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_stuff_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"map_reduce\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_map_reduce_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"refine\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_refine_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"map_rerank\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_load_map_rerank_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mchain_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"Got unsupported chain type: {chain_type}. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34mf\"Should be one of {loader_mapping.keys()}\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mloader_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchain_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/question_answering/__init__.py\n",
       "\u001b[0;31mType:\u001b[0m      function"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_qa_chain??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1096ea0-5003-42ef-b46b-8e11f78c4783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mConversationalRetrievalChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseMemory\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackHandler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcallback_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCallbackManager\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcombine_docs_chain\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_documents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseCombineDocumentsChain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mquestion_generator\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLLMChain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'answer'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrephrase_question\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_source_documents\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_generated_question\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mget_chat_history\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mretriever\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseRetriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_tokens_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mConversationalRetrievalChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseConversationalRetrievalChain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Chain for having a conversation based on retrieved documents.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    This chain takes in chat history (a list of messages) and new questions,\u001b[0m\n",
       "\u001b[0;34m    and then returns an answer to that question.\u001b[0m\n",
       "\u001b[0;34m    The algorithm for this chain consists of three parts:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    1. Use the chat history and the new question to create a \"standalone question\".\u001b[0m\n",
       "\u001b[0;34m    This is done so that this question can be passed into the retrieval step to fetch\u001b[0m\n",
       "\u001b[0;34m    relevant documents. If only the new question was passed in, then relevant context\u001b[0m\n",
       "\u001b[0;34m    may be lacking. If the whole conversation was passed into retrieval, there may\u001b[0m\n",
       "\u001b[0;34m    be unnecessary information there that would distract from retrieval.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    2. This new question is passed to the retriever and relevant documents are\u001b[0m\n",
       "\u001b[0;34m    returned.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    3. The retrieved documents are passed to an LLM along with either the new question\u001b[0m\n",
       "\u001b[0;34m    (default behavior) or the original question and chat history to generate a final\u001b[0m\n",
       "\u001b[0;34m    response.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Example:\u001b[0m\n",
       "\u001b[0;34m        .. code-block:: python\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            from langchain.chains import (\u001b[0m\n",
       "\u001b[0;34m                StuffDocumentsChain, LLMChain, ConversationalRetrievalChain\u001b[0m\n",
       "\u001b[0;34m            )\u001b[0m\n",
       "\u001b[0;34m            from langchain.prompts import PromptTemplate\u001b[0m\n",
       "\u001b[0;34m            from langchain.llms import OpenAI\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            combine_docs_chain = StuffDocumentsChain(...)\u001b[0m\n",
       "\u001b[0;34m            vectorstore = ...\u001b[0m\n",
       "\u001b[0;34m            retriever = vectorstore.as_retriever()\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            # This controls how the standalone question is generated.\u001b[0m\n",
       "\u001b[0;34m            # Should take `chat_history` and `question` as input variables.\u001b[0m\n",
       "\u001b[0;34m            template = (\u001b[0m\n",
       "\u001b[0;34m                \"Combine the chat history and follow up question into \"\u001b[0m\n",
       "\u001b[0;34m                \"a standalone question. Chat History: {chat_history}\"\u001b[0m\n",
       "\u001b[0;34m                \"Follow up question: {question}\"\u001b[0m\n",
       "\u001b[0;34m            )\u001b[0m\n",
       "\u001b[0;34m            prompt = PromptTemplate.from_template(template)\u001b[0m\n",
       "\u001b[0;34m            llm = OpenAI()\u001b[0m\n",
       "\u001b[0;34m            question_generator_chain = LLMChain(llm=llm, prompt=prompt)\u001b[0m\n",
       "\u001b[0;34m            chain = ConversationalRetrievalChain(\u001b[0m\n",
       "\u001b[0;34m                combine_docs_chain=combine_docs_chain,\u001b[0m\n",
       "\u001b[0;34m                retriever=retriever,\u001b[0m\n",
       "\u001b[0;34m                question_generator=question_generator_chain,\u001b[0m\n",
       "\u001b[0;34m            )\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mretriever\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseRetriever\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Retriever to use to fetch documents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_tokens_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"If set, enforces that the documents returned are less than this limit.\u001b[0m\n",
       "\u001b[0;34m    This is only enforced if `combine_docs_chain` is of type StuffDocumentsChain.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_tokens_below_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mnum_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_tokens_limit\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_docs_chain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStuffDocumentsChain\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombine_docs_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mtoken_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mwhile\u001b[0m \u001b[0mtoken_count\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_tokens_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mnum_docs\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mtoken_count\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_docs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_get_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get docs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_relevant_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce_tokens_below_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32masync\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_aget_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mquestion\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAsyncCallbackManagerForChainRun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get docs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maget_relevant_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reduce_tokens_below_limit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mfrom_llm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mretriever\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseRetriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcondense_question_prompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBasePromptTemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONDENSE_QUESTION_PROMPT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mchain_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"stuff\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcondense_question_llm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcombine_docs_chain_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBaseConversationalRetrievalChain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Convenience method to load chain from LLM and retriever.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        This provides some logic to create the `question_generator` chain\u001b[0m\n",
       "\u001b[0;34m        as well as the combine_docs_chain.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        Args:\u001b[0m\n",
       "\u001b[0;34m            llm: The default language model to use at every part of this chain\u001b[0m\n",
       "\u001b[0;34m                (eg in both the question generation and the answering)\u001b[0m\n",
       "\u001b[0;34m            retriever: The retriever to use to fetch relevant documents from.\u001b[0m\n",
       "\u001b[0;34m            condense_question_prompt: The prompt to use to condense the chat history\u001b[0m\n",
       "\u001b[0;34m                and new question into a standalone question.\u001b[0m\n",
       "\u001b[0;34m            chain_type: The chain type to use to create the combine_docs_chain, will\u001b[0m\n",
       "\u001b[0;34m                be sent to `load_qa_chain`.\u001b[0m\n",
       "\u001b[0;34m            verbose: Verbosity flag for logging to stdout.\u001b[0m\n",
       "\u001b[0;34m            condense_question_llm: The language model to use for condensing the chat\u001b[0m\n",
       "\u001b[0;34m                history and new question into a standalone question. If none is\u001b[0m\n",
       "\u001b[0;34m                provided, will default to `llm`.\u001b[0m\n",
       "\u001b[0;34m            combine_docs_chain_kwargs: Parameters to pass as kwargs to `load_qa_chain`\u001b[0m\n",
       "\u001b[0;34m                when constructing the combine_docs_chain.\u001b[0m\n",
       "\u001b[0;34m            callbacks: Callbacks to pass to all subchains.\u001b[0m\n",
       "\u001b[0;34m            **kwargs: Additional parameters to pass when initializing\u001b[0m\n",
       "\u001b[0;34m                ConversationalRetrievalChain\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcombine_docs_chain_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcombine_docs_chain_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdoc_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_qa_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mchain_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchain_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mcombine_docs_chain_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0m_llm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcondense_question_llm\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcondense_question_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_llm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondense_question_prompt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mcombine_docs_chain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdoc_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mquestion_generator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcondense_question_chain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConversationalRetrievalChain??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5f012-793c-4bf3-87d6-fbf81e4bdd7f",
   "metadata": {},
   "source": [
    "# 07 Oct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "becbf485-ef46-413c-af77-ea27821e7e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_summarize(chat_history, new_query):\n",
    "\n",
    "    message = f\"\"\" \n",
    "    Progressively summarize the lines of conversations, but keep in mind, that you have to update any code-related thing, or configuration related thing, with great care. You are acting as part of \n",
    "    a customized chatbot, which has to help with advanced tools like Omniverse, ShotGrid, USDA files, and 3D rendering etc. So, most of time your answers will be rectified by User Input, and \n",
    "    **you have to store updated information in summaries.** As, this info will be also saved to vectorstores, for future references. \n",
    "    Current Summary:\n",
    "    {chat_history}\n",
    "    \n",
    "    New lines of conversation :\n",
    "    {new_query}\n",
    "    \n",
    "    New_Summary:\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "                            model=\"gpt-3.5--turbo-16k\",\n",
    "                            messages=message,\n",
    "                            temperature=0.7\n",
    "                                    )\n",
    "    return response['choices'][0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3ad205a2-9b3c-4531-95a8-a437371d6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_context_docs(query, vstore = load_vstore):\n",
    "    '''\n",
    "    will return full text, and also top doc text, for decision to use it or not\n",
    "\n",
    "    '''\n",
    "    docs = vstore.similarity_search(query)\n",
    "    final_str = ''\n",
    "    for one in docs:\n",
    "        final_str += one.page_content\n",
    "        final_str +='\\n\\n\\n\\n'\n",
    "\n",
    "    return final_str, docs[0].page_content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f71d9ec2-36f8-4b49-8437-e96cc191146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a customized Chatbot, and will be helping in different domains; like code helping, omniverse stuff, ShotGrid docs, api stuff etc. \n",
    "You will have memory, and previous messages will be provided to you in following format: \\n\n",
    "### Input  <Human Message>\n",
    "### Response <Your Reply> \n",
    "and also make use of Knowledge base, which will be provided to you with some workflow.\n",
    "-- If any code is to be written, DO WRITE IT INSIDE ``` <CODE> ``` . This is very important, as this way, the UI will be able to display code in well formatted \n",
    "way, by using some post-processing.\n",
    "\n",
    "{context}\n",
    "\n",
    "this is the running chat-history:\n",
    "{chat_history}\n",
    "\n",
    "### Input:  {question}\n",
    "\n",
    "### Response:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "feb05abd-a43c-47c3-be8a-c47d50885c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_chat_fn(query, vstore, chat_summary):\n",
    "\n",
    "    context = return_context_docs(query,load_vstore)\n",
    "    \n",
    "    stale_message = f\"\"\"\n",
    "    You are a customized Chatbot, and will be helping in different domains; like code helping, omniverse stuff, ShotGrid docs, api stuff etc. \n",
    "You will have memory, and previous messages will be provided to you. Also, custom knowlwdgebase, similar chunks of info, will be provided as context.\n",
    "    Context of knowledgebase:\n",
    "    -----\n",
    "    {context}\n",
    "    -----\n",
    "    Running summary of chat:\n",
    "    -----\n",
    "    {chat_summary}\n",
    "    ----\n",
    "    Query : {query}\n",
    "\n",
    "    AI Response:\n",
    "    \n",
    "    \"\"\"\n",
    "    user_message = f\"\"\"\n",
    "    Context of knowledgebase:\n",
    "    -----\n",
    "    {context}\n",
    "    -----\n",
    "    Running summary of chat:\n",
    "    -----\n",
    "    {chat_summary}\n",
    "    ----\n",
    "    Query : {query}\n",
    "\n",
    "    AI Response:\n",
    "\n",
    "    \"\"\"\n",
    "    message = [\n",
    "        {'role':'system','content':'''You are a customized Chatbot, and will be helping in different domains; like code helping, omniverse stuff, ShotGrid docs, api stuff etc. \n",
    "You will have memory, and previous messages will be provided to you. Also, custom knowlwdgebase, similar chunks of info, will be provided as context.'''},\n",
    "        {'role':'user','content':user_message}\n",
    "        \n",
    "        \n",
    "    ]\n",
    "\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "                            model=\"gpt-3.5-turbo-16k\",\n",
    "                            messages=message,\n",
    "                            temperature=0.7\n",
    "                                    )\n",
    "    return response['choices'][0].message.content\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ab291b-12dd-4921-ae7e-90d74d576720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0f9436-310d-4333-b5d7-693120681baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2991933-41d4-44c9-87e9-2c57c4ea5ca5",
   "metadata": {},
   "source": [
    "# separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d059d0d-45f5-4bf0-90f4-06f9f369769c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c764583f-b087-4981-81f6-c91bd729d6de",
   "metadata": {},
   "source": [
    "# Prompt Engineering on messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9358a08-2b09-45bb-b92f-1537d03bb3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    message = [\n",
    "        {'role':'system','content':'''\n",
    "        You are acting as dynamic memory of a chatbot(Chatbot: A customized one, for helping with latest python libraries, tools like omniverse, usd files etc). You have to be\n",
    "        mindful, that in dynamic summary generation, you have to return the corrected code/ any configuration related stuff etc. As, there will be a button in USEr Interface, with which \n",
    "        your running summary(at that point) will be saved to a vectorstore. So the purpose is, for Example, user is chatting about a rcent python library code. and now over few iterations\n",
    "        of conversations, you both were able to get the working code/config. Now the user wants to save this in vectorstore, so later anyone asks somethingspecifc/related to that, \n",
    "        because of your rich-content summary saved, Chatbot will be able to answer correctly maybe in first attempt, or atleast it will improve with time. So you will have Current\n",
    "        -summary, new lines of conversations, and you have to generate new Summary. :: \n",
    "    \n",
    "        '''},\n",
    "        \n",
    "        {'role':'user','content':user_message}\n",
    "        \n",
    "        \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1160697c-5f36-4a0f-af4e-303c5dd93916",
   "metadata": {},
   "source": [
    "### decision_to_use_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6e12ce-2bf5-484c-8c01-3dbf1c4a9c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'role':'user','content':f\"\"\"\n",
    "        Query is : \"got this error\n",
    "AttributeError Traceback (most recent call last) Cell In[8], line 1 ----> 1 classes = dst.classifications.classes 2 print(classes)\n",
    "\n",
    "File ~/.pyenv/versions/env-yolov8/lib/python3.10/site-packages/fiftyone/core/dataset.py:357, in Dataset.getattribute(self, name) 354 if getattr(self, \"_deleted\", False): 355 raise ValueError(\"Dataset '%s' is deleted\" % self.name) --> 357 return super().getattribute(name)\n",
    "\n",
    "AttributeError: 'Dataset' object has no attribute 'classifications'\", \\n\n",
    "        Context docs are : \"# we drop sparse_values as they are not needed for this example\\n\\ndataset.documents.drop([\\n\\n\\'metadata\\'], axis\\n\\n1, inplace\\n\\nTrue)\\n\\ndataset.documents.rename(columns\\n\\n={\\n\\n\\'blob\\':\\n\\n\\'metadata\\'}, inplace\\n\\nTrue)\\n\\ndataset.head()\\n\\n0 \\n       417ede5d-39be-498f-b518-f47ed4e53b90 \\n       [0.005949743557721376, 0.01983247883617878, -0... \\n       {\\'chunk\\': 0, \\'text\\': \\'.rst\\n.pdf\\nWelcome to Lan... \\n     \\n       1 \\n       110f550d-110b-4378-b95e-141397fa21bc \\n       [0.009401749819517136, 0.02443608082830906, 0..\\n\\n\\n\\nGPT4 with\\nRetrieval Augmentation over LangChain Docs\\n\\nIn this notebook we\\'ll work through an example of using GPT-4 with\\nretrieval augmentation to answer questions about the LangChain Python\\nlibrary.\\n\\nTo begin we must install the prerequisite libraries:\\n\\n!pip install -qU \\\\\\n\\nopenai==0.27.7 \\\\\\n\\n\"pinecone-client[grpc]\"==2.2.1 \\\\\\n\\npinecone-datasets==\\'0.5.0rc11\\'\\n\\n🚨 Note: the above pip install is formatted for\\nJupyter notebooks. If running elsewhere you may need to drop the\\n!\\n\\n\\n\\n. As a native object, it can be inspected for further scriptable properties - using table iteration and \"getmetatable\"\\n\\nin Lua and dir, help etc in Python (among other methods). A notable scriptable object above is fusion - it allows access to all existing Fusion scripting functionality.\\n\\nRunning DaVinci Resolve in headless mode\\n\\n----------------------------------------\\n\\nDaVinci Resolve can be launched in a headless mode without the user interface using the -nogui command line option\\n\\n\\n\\n.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")\\n\\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\\n\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\n\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\n\\nfrom langchain.text_splitter import CharacterTextSplitter\\n\\nfrom langchain.vectorstores import Pinecone\\n\\nfrom langchain\\n\\n\\n\\n\"\n",
    "        \"\"\"},\n",
    "        {'role':'assistant','content': 'No'},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "395cc2eb-e6e7-4cfa-8e52-da24abc81deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_to_use_context(docs,query):\n",
    "\n",
    "    messages = [\n",
    "        {'role':'system','content':\"\"\"\n",
    "        We have large store of knowledgebase texts, and we have one api/fn that based on QUERY returns relevant chunks of texts. Now, it is error-prone, \n",
    "        like even if relevant info is not present in knowledgebase, it does return something.\n",
    "        \n",
    "        **Be mindful in cases of code, It should be returning docs of relevant library, \n",
    "        and methods, not of any random one. !! IF ERROR IS IN FIFTYONE DATASETS METHOD/OBJECT LIBRARY RELATED, CONTEXTS SHOULD NOT BE SOME CODE-RELATED TO ERROR/SOLVING ABOUT LANGCHAIN DATASETS/ETC,SHOTGRID ETC\n",
    "        You have to make the decision , Whether than information is good to use or not.**\n",
    "        \n",
    "        You have to only Answer in \"Yes:Small Reason\" and \"No: Small Reason\". There is no option of Not knowing, or any explanation stuff.\n",
    "         \\n\\n\n",
    "                                \"\"\"},\n",
    "        {'role':'user','content':\"\"\"\n",
    "        Query is : \"how to load a dataset in Fiftyone Library?\", \\n\n",
    "        Context docs are : \". If running elsewhere you may need to drop the\\n!.\\n\\nIn this example, we will use a pre-embedding dataset of the LangChain\\ndocs from python.langchain.readthedocs.com/.\\nIf you'd like to see how we perform the data preparation refer to this notebook.\\n\\nLet's go ahead and download the dataset.\\n\\nfrom pinecone_datasets import load_dataset\\n\\ndataset = load_dataset('langchain-python-docs-text-embedding-ada-002')\"\n",
    "        \"\"\"},\n",
    "        {'role':'assistant','content':'No:As query is about Fiftyone library, and context docs are about embeddings, langchain etc'},\n",
    "        {'role':'user','content':\"\"\"\n",
    "        Query is : \"how to load vectorstore in pinecone\", \\n\n",
    "        Context docs are : \"Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass\"\n",
    "        \"\"\"},\n",
    "        {'role':'assistant','content': 'Yes: Query is about Pinecone, and Context is about PineCone as well'},\n",
    "        {'role':'user','content':\"\"\"\n",
    "        Query is : \"got this error\n",
    "        AttributeError Traceback (most recent call last) Cell In[8], line 1 ----> 1 classes = dst.classifications.classes 2 print(classes)\n",
    "        \n",
    "        File ~/.pyenv/versions/env-yolov8/lib/python3.10/site-packages/fiftyone/core/dataset.py:357, in Dataset.getattribute(self, name) 354 if getattr(self, \"_deleted\", False): 355 raise ValueError(\"Dataset '%s' is deleted\" % self.name) --> 357 return super().getattribute(name)\n",
    "        \n",
    "        AttributeError: 'Dataset' object has no attribute 'classifications'\", \\n\n",
    "        Context docs are : \"# we drop sparse_values as they are not needed for this example\\n\\ndataset.documents.drop([\\n\\n\\'metadata\\'], axis\\n\\n1, inplace\\n\\nTrue)\\n\\ndataset.documents.rename(columns\\n\\n={\\n\\n\\'blob\\':\\n\\n\\'metadata\\'}, inplace\\n\\nTrue)\\n\\ndataset.head()\\n\\n0 \\n       417ede5d-39be-498f-b518-f47ed4e53b90 \\n       [0.005949743557721376, 0.01983247883617878, -0... \\n       {\\'chunk\\': 0, \\'text\\': \\'.rst\\n.pdf\\nWelcome to Lan... \\n     \\n       1 \\n       110f550d-110b-4378-b95e-141397fa21bc \\n       [0.009401749819517136, 0.02443608082830906, 0..\\n\\n\\n\\nGPT4 with\\nRetrieval Augmentation over LangChain Docs\\n\\nIn this notebook we\\'ll work through an example of using GPT-4 with\\nretrieval augmentation to answer questions about the LangChain Python\\nlibrary.\\n\\nTo begin we must install the prerequisite libraries:\\n\\n!pip install -qU \\\\\\n\\nopenai==0.27.7 \\\\\\n\\n\"pinecone-client[grpc]\"==2.2.1 \\\\\\n\\npinecone-datasets==\\'0.5.0rc11\\'\\n\\n🚨 Note: the above pip install is formatted for\\nJupyter notebooks. If running elsewhere you may need to drop the\\n!\\n\\n\\n\\n. As a native object, it can be inspected for further scriptable properties - using table iteration and \"getmetatable\"\\n\\nin Lua and dir, help etc in Python (among other methods). A notable scriptable object above is fusion - it allows access to all existing Fusion scripting functionality.\\n\\nRunning DaVinci Resolve in headless mode\\n\\n----------------------------------------\\n\\nDaVinci Resolve can be launched in a headless mode without the user interface using the -nogui command line option\\n\\n\\n\\n.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")\\n\\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\\n\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\n\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\n\\nfrom langchain.text_splitter import CharacterTextSplitter\\n\\nfrom langchain.vectorstores import Pinecone\\n\\nfrom langchain\\n\\n\\n\\n\"\n",
    "        \"\"\"},\n",
    "        {'role':'assistant','content': \"No: As query is for Fiftyone Related, and context is about Langchain, GPT4,etc\"},\n",
    "\n",
    "\n",
    "        {'role':'user','content':f\"\"\"\n",
    "        Query is : \"{query}\" \\n\n",
    "        Context docs are : \"{docs}\"\n",
    "        \"\"\"}\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "                            #model=\"gpt-3.5-turbo-16k\",\n",
    "                            model = 'gpt-4',\n",
    "                            messages=messages,\n",
    "                            temperature=0\n",
    "                                    )\n",
    "    return response['choices'][0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "13b51667-0860-413f-8b96-4762dfea5329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_to_use_context(docs,query):\n",
    "\n",
    "    messages = [\n",
    "        {'role':'system','content':\"\"\"\n",
    "        We have large store of knowledgebase texts, and we have one api/fn that based on QUERY returns relevant chunks of texts. Now, it is error-prone, \n",
    "        like even if relevant info is not present in knowledgebase, it does return something.\n",
    "        \n",
    "        **Be mindful in cases of code, It should be returning docs of relevant library, \n",
    "        and methods, not of any random one. !! IF ERROR IS IN FIFTYONE DATASETS METHOD/OBJECT LIBRARY RELATED, CONTEXTS SHOULD NOT BE SOME CODE-RELATED TO ERROR/SOLVING ABOUT LANGCHAIN DATASETS/ETC,SHOTGRID ETC\n",
    "        You have to make the decision , Whether than information is good to use or not.**\n",
    "        \n",
    "        You have to only Answer in \"Yes:Small Reason\" and \"No: Small Reason\". There is no option of Not knowing, or any explanation stuff.\n",
    "         \\n\\n\n",
    "                                \"\"\"},\n",
    "        {'role':'user','content':\"\"\"\n",
    "        Query is : \"got this error\n",
    "        AttributeError Traceback (most recent call last) Cell In[8], line 1 ----> 1 classes = dst.classifications.classes 2 print(classes)\n",
    "        \n",
    "        File ~/.pyenv/versions/env-yolov8/lib/python3.10/site-packages/fiftyone/core/dataset.py:357, in Dataset.getattribute(self, name) 354 if getattr(self, \"_deleted\", False): 355 raise ValueError(\"Dataset '%s' is deleted\" % self.name) --> 357 return super().getattribute(name)\n",
    "        \n",
    "        AttributeError: 'Dataset' object has no attribute 'classifications'\", \\n\n",
    "        Context docs are : \"# we drop sparse_values as they are not needed for this example\\n\\ndataset.documents.drop([\\n\\n\\'metadata\\'], axis\\n\\n1, inplace\\n\\nTrue)\\n\\ndataset.documents.rename(columns\\n\\n={\\n\\n\\'blob\\':\\n\\n\\'metadata\\'}, inplace\\n\\nTrue)\\n\\ndataset.head()\\n\\n0 \\n       417ede5d-39be-498f-b518-f47ed4e53b90 \\n       [0.005949743557721376, 0.01983247883617878, -0... \\n       {\\'chunk\\': 0, \\'text\\': \\'.rst\\n.pdf\\nWelcome to Lan... \\n     \\n       1 \\n       110f550d-110b-4378-b95e-141397fa21bc \\n       [0.009401749819517136, 0.02443608082830906, 0..\\n\\n\\n\\nGPT4 with\\nRetrieval Augmentation over LangChain Docs\\n\\nIn this notebook we\\'ll work through an example of using GPT-4 with\\nretrieval augmentation to answer questions about the LangChain Python\\nlibrary.\\n\\nTo begin we must install the prerequisite libraries:\\n\\n!pip install -qU \\\\\\n\\nopenai==0.27.7 \\\\\\n\\n\"pinecone-client[grpc]\"==2.2.1 \\\\\\n\\npinecone-datasets==\\'0.5.0rc11\\'\\n\\n🚨 Note: the above pip install is formatted for\\nJupyter notebooks. If running elsewhere you may need to drop the\\n!\\n\\n\\n\\n. As a native object, it can be inspected for further scriptable properties - using table iteration and \"getmetatable\"\\n\\nin Lua and dir, help etc in Python (among other methods). A notable scriptable object above is fusion - it allows access to all existing Fusion scripting functionality.\\n\\nRunning DaVinci Resolve in headless mode\\n\\n----------------------------------------\\n\\nDaVinci Resolve can be launched in a headless mode without the user interface using the -nogui command line option\\n\\n\\n\\n.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")\\n\\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\\n\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\n\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\n\\nfrom langchain.text_splitter import CharacterTextSplitter\\n\\nfrom langchain.vectorstores import Pinecone\\n\\nfrom langchain\\n\\n\\n\\n\"\n",
    "        \"\"\"},\n",
    "        {'role':'assistant','content': \"No: As query is for Fiftyone Related, and context is about Langchain, GPT4,etc\"},\n",
    "\n",
    "\n",
    "        {'role':'user','content':f\"\"\"\n",
    "        Query is : \"{query}\" \\n\n",
    "        Context docs are : \"{docs}\"\n",
    "        \"\"\"}\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "                            #model=\"gpt-3.5-turbo-16k\",\n",
    "                            model = 'gpt-4',\n",
    "                            messages=messages,\n",
    "                            temperature=0\n",
    "                                    )\n",
    "    return response['choices'][0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "806b9953-7191-4a1f-a748-42cc6fcf515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is Davinci Resolve'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cfd4b694-b7e5-4acc-8335-1ccdaaec016c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. Resolve can run one or more Workflow Integration Plugins at the same time.\\n\\nUsers can write their own Workflow Integration Plugin (an Electron app) which could be loaded into DaVinci Resolve Studio. To interact with Resolve, Resolve\\'s JavaScript APIs can be used from the plugin.\\n\\nAlternatively, a Python or Lua script can be invoked, with the option of a user interface built with Resolve\\'s built-in Qt-based UIManager, or with an external GUI manager\\n\\n\\n\\n. When DaVinci Resolve is launched using this option, the user interface is disabled.\\n\\nHowever, the various scripting APIs will continue to work as expected.\\n\\nBasic Resolve API\\n\\n-----------------\\n\\nSome commonly used API functions are described below (*). As with the resolve object, each object is inspectable for properties and functions.\\n\\nResolve\\n\\n  Fusion()                                        --> Fusion\\n\\n\\n\\n#!/usr/bin/env python\\n\\n    import DaVinciResolveScript as dvr_script\\n\\n    resolve = dvr_script.scriptapp(\"Resolve\")\\n\\n    fusion = resolve.Fusion()\\n\\n    projectManager = resolve.GetProjectManager()\\n\\n    projectManager.CreateProject(\"Hello World\")\\n\\nThe resolve object is the fundamental starting point for scripting via Resolve\\n\\n\\n\\n. Apart from this README.txt file, this package contains following folders:\\n\\nExamples: containing some representative sample plugin, and a sample script.\\n\\nScripts: containing some sample workflow scripts to interact with Resolve.\\n\\nOverview\\n\\n--------\\n\\nDaVinci Resolve Studio now supports Workflow Integration Plugins to be loaded and communicate with Resolve. Resolve can run one or more Workflow Integration Plugins at the same time\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = return_context_docs(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7446caa-19f1-413d-860d-d0641991b2e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes: Query is about DaVinci Resolve, and context is also about DaVinci Resolve.'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decision_to_use_context(docs, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "282be29a-b6d3-443c-bcdf-e89bf09f53d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No: As query is about Fiftyone library, and context docs are about embeddings, langchain etc'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"To list the classes in the \"classifications\" field of your loaded FIFTYONE-dataset, you can use the following code:\n",
    "got this error\n",
    "AttributeError Traceback (most recent call last) Cell In[8], line 1 ----> 1 classes = dst.classifications.classes 2 print(classes)\n",
    "\n",
    "File ~/.pyenv/versions/env-yolov8/lib/python3.10/site-packages/fiftyone/core/dataset.py:357, in Dataset.getattribute(self, name) 354 if getattr(self, \"_deleted\", False): 355 raise ValueError(\"Dataset '%s' is deleted\" % self.name) --> 357 return super().getattribute(name)\n",
    "\n",
    "AttributeError: 'Dataset' object has no attribute 'classifications' \"\"\"\n",
    "_,docs = return_context_docs(query)\n",
    "decision_to_use_context(docs, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1790158f-48cb-403c-a790-489f0e105a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\". If running elsewhere you may need to drop the\\n!.\\n\\nIn this example, we will use a pre-embedding dataset of the LangChain\\ndocs from python.langchain.readthedocs.com/.\\nIf you'd like to see how we perform the data preparation refer to this notebook.\\n\\nLet's go ahead and download the dataset.\\n\\nfrom pinecone_datasets import load_dataset\\n\\ndataset = load_dataset('langchain-python-docs-text-embedding-ada-002')\""
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4a539a0c-1290-45ba-b71e-68db3029e44a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No: The context is about a chatbot model and does not provide any information about the Fiftyone error in the query.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"i am getting this error\n",
    "ValueError: Sample field 'classifications' is not a [<class 'fiftyone.core.labels.Classification'>] instance; found <class 'fiftyone.core.labels.Classifications'>\n",
    "with this code\n",
    " \"\"\"\n",
    "_,docs = return_context_docs(query)\n",
    "decision_to_use_context(docs, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "954a59d8-59e3-47c1-9d87-f79680aa6f60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Yes: The context provides information about installing and setting up Pinecone, which is relevant to the query about loading a vectorstore in Pinecone.'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"How to load vectorstore in pinecone?\n",
    " \"\"\"\n",
    "_,docs = return_context_docs(query)\n",
    "decision_to_use_context(docs, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a15214db-4b0c-44a8-9e71-3c713ddd8993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "370a9610-64c1-43f8-a639-48e80d93cd1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'No: The context is about loading a dataset from LangChain, not FiftyOne.'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"This is how I loaded fiftyone dataset: dst = fo.load_dataset('test-dataset-cnt91-v09.09.23')\n",
    "\n",
    "How can I list classes in field: \"classifications\"\n",
    " \"\"\"\n",
    "_,docs = return_context_docs(query)\n",
    "decision_to_use_context(docs, query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9406d5eb-29f2-4079-8117-96ef39efee8a",
   "metadata": {},
   "source": [
    "# Docs fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d62f8a87-4608-4b2b-9720-6d9dec1f2662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mConversationSummaryBufferMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mhuman_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Human'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mai_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AI'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mllm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlanguage_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mprompt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprompt_template\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBasePromptTemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'summary'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'new_lines'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.\\n\\nEXAMPLE\\nCurrent summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.\\n\\nNew lines of conversation:\\nHuman: Why do you think artificial intelligence is a force for good?\\nAI: Because artificial intelligence will help humans reach their full potential.\\n\\nNew summary:\\nThe human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\\nEND OF EXAMPLE\\n\\nCurrent summary:\\n{summary}\\n\\nNew lines of conversation:\\n{new_lines}\\n\\nNew summary:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msummary_message_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;32mclass\u001b[0m \u001b[0;34m'langchain.schema.messages.SystemMessage'\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mchat_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseChatMessageHistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0moutput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0minput_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_messages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'history'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mConversationSummaryBufferMemory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseChatMemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSummarizerMixin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Buffer with summarizer for storing conversation memory.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmemory_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"history\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mmemory_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Will always return list of memory variables.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        :meta private:\u001b[0m\n",
       "\u001b[0;34m        \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mload_memory_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Return history buffer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfirst_messages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseMessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary_message_cls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_messages\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_messages\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfinal_buffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mfinal_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_buffer_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhuman_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuman_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mai_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mai_prefix\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_key\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfinal_buffer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m@\u001b[0m\u001b[0mroot_validator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_prompt_input_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Validate that prompt input variables are consistent.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mprompt_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"prompt\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_variables\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mexpected_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"new_lines\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mexpected_keys\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"Got unexpected prompt input variables. The prompt expects \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34mf\"{prompt_variables}, but it should have {expected_keys}.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mprune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_tokens_from_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mpruned_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mwhile\u001b[0m \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_token_limit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mpruned_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mcurr_buffer_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_num_tokens_from_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_new_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0mpruned_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Clear memory contents.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_summary_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/memory/summary_buffer.py\n",
       "\u001b[0;31mType:\u001b[0m           ModelMetaclass\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ConversationSummaryBufferMemory??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a52214dc-0760-4474-ad65-113f81a1f007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = main_chat_fn('What is PineCone?',load_vstore,'Chatbot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1734fc48-78e3-475b-a54e-059476d152aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pinecone is a vector database with broad functionality. It is used for storing and querying high-dimensional vectors efficiently.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bd58593-dd66-4005-a6e3-365156a30a50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/pinecone_docs.txt'})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = load_vstore.similarity_search('What is pinecone',k=1)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80bb2871-af97-4f4f-adde-e935b6abf868",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "545b2b64-9399-49f7-b3c6-2680a4e4f4c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b326aa0-5a69-4a4e-8a57-901860865e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = load_vstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9f7efbe2-6348-4558-8412-ebd8e7fa08ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass\\n# initialize pinecone pinecone.init( api_key=os.getenv(\"PINECONE_API_KEY\"),  # find at app.pinecone.io environment=os.getenv(\"PINECONE_ENV\"),  # next to api key in console )\\n\\nindex_name = \"langchain\\n\\n\\n\\ndemo\"\\n\\n# First, check if our index already exists. If it doesn\\'t, we create it if index_name not in pinecone.list_indexes(): # we create a new index pinecone.create_index( name=index_name, metric=\\'cosine\\', dimension=1536 )\\n.vectorstores import Pinecone\\n\\nfrom langchain.document_loaders import TextLoader\\n\\nAPI Reference:\\n\\nOpenAIEmbeddings\\n\\nCharacterTextSplitter\\n\\nPinecone\\n\\nTextLoader\\n\\nfrom langchain.document_loaders import TextLoader\\n\\nloader = TextLoader(\"../../../state_of_the_union.txt\")\\n\\ndocuments = loader.load()\\n\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\n\\ndocs = text_splitter.split_documents(documents)\\n\\nembeddings = OpenAIEmbeddings()\\n\\nAPI Reference:\\n\\nTextLoader\\n\\nimport pinecone\\n# initialize connection to pinecone (get API key at app.pinecone.io)\\n\\napi_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\\n\\n# find your environment next to the api key in pinecone console\\n\\nenv = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\\n\\npinecone.init(api_key=api_key, environment=env)\\n\\nindex_name = \\'gpt-4-langchain-docs-fast\\'\\n\\nimport time\\n\\n# check if index already exists (it shouldn\\'t if this is first time)\\n\\nif index_name not in pinecone.list_indexes():\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_context_docs('What is PineCone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3c28cc-35e9-4c01-b07b-820dca344bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-3.10.2",
   "language": "python",
   "name": "langchain-3.10.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
