{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc677196-a7e1-4912-8596-9857f8917278",
   "metadata": {},
   "source": [
    " > Goals:\n",
    "- Create and save a FAISS index, and make fn, that with any update it saves the info in adjoining txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b9f9d4-4c32-40f1-abbf-5d1716f9d56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6fbfa59e-42cf-496e-9c9e-47ab3d24b426",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de6dfc58-22c9-4725-a218-269ea9cf1bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from langchain.document_loaders import NotionDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd09d0f5-88c8-48c6-8ee8-7ebd5dbfee75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, Docx2txtLoader, TextLoader, PyMuPDFLoader, PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3964f69-a7dd-44c3-b4be-93caef88a94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d6fdd02-8306-4215-9b36-195ee71e12e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory, ConversationSummaryBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3adae471-5f06-47ae-be1e-b717b96b2b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf797d87-78aa-4df2-b245-64c1f7afdcc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredHTMLLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b099c353-fd2d-4e38-b69c-c7cef26b349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8989afae-1f9e-4a4f-92cb-963dc0cb0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/ubuntu/workspace/Creds/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b70651c-7cb5-4e68-842e-119f0624f1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai_config import OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2853cf64-902f-43bd-b031-11caed32af44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d562006f-3fa6-4188-bf8b-4888972264b3",
   "metadata": {},
   "source": [
    "# Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c769dbd0-387a-4856-8677-8342b307d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec8d312a-8742-49b5-97a1-6d52385312a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledgebase_folder = '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af80e7e-de29-47ec-ab01-677557f70792",
   "metadata": {},
   "source": [
    "# Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc5a3616-ec23-48aa-a69c-fe37da48ae67",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1095219834.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    txt_doc =\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def create_faiss_index(folder_path):\n",
    "    all_files = os.listdir(folder_path)\n",
    "    for filename in all_files:\n",
    "        if filename.endswith(('.docx', '.txt', '.pdf')):\n",
    "            full_pth = f'{folder_path}/{filename}'\n",
    "            if full_pth.endswith('.docx'):\n",
    "                docx_doc = Docx2txtLoader(full_pth)\n",
    "                docs = docx_doc.load()\n",
    "            elif full_pth.endswith('.txt'):\n",
    "                txt_doc = \n",
    "            elif full_pth.endswith('.html'):\n",
    "                loader = UnstructuredHTMLLoader(full_pth)\n",
    "                html_doc = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21b1b65f-6df7-4351-9643-8f3c70efdd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model = 'text-embedding-ada-002',\n",
    "                              openai_api_key= OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2086d264-70e9-44e9-b072-d229d6b3abf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"#\",\"##\", \"###\", \"\\\\n\\\\n\",\"\\\\n\",\".\", '\\n'],\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4278c0-eacf-4be0-8f15-f665b67af240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "78d5782c-70ab-46cc-9bb6-386cef8a0d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "template = \"\"\"\n",
    "You are a customized Chatbot, and will be helping in different domains; like code helping, omniverse stuff, ShotGrid docs, api stuff etc. \n",
    "It is very important that you do have memory, and can go along with the User, and also make use of Knowledge base, which will be provided to you with some workflow.\n",
    "-- If any code is to be written, DO WRITE IT INSIDE ``` <CODE> ``` . This is very important, as this way, the UI will be able to display code in well formatted \n",
    "way, by using some post-processing.\n",
    "\n",
    "{context}\n",
    "\n",
    "this is the running chat-history:\n",
    "{chat_history}\n",
    "\n",
    "Query:  {question}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['context', 'question', 'chat_history'], template = template\n",
    ")\n",
    "\n",
    "memory = ConversationBufferWindowMemory(\n",
    "    k = 4,\n",
    "    memory_key = 'chat_history',\n",
    "    return_messages = False\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model = 'gpt-3.5-turbo-16k',\n",
    "                openai_api_key= OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "33c44f44-a026-4a28-986f-f064ca30bd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_chain(vector_store = load_vstore, llm = llm, memory = memory):\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "    chain = ConversationalRetrievalChain.from_llm(llm=llm, \n",
    "                                              retriever=retriever, \n",
    "                                              memory=memory, \n",
    "                                              #get_chat_history=lambda h : h,\n",
    "                                              combine_docs_chain_kwargs = {'prompt':prompt},\n",
    "                                              verbose=True)\n",
    "\n",
    "    return chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c0934722-f34c-4154-b1a7-391d4098e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_chain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e2eb9ea0-6ad8-4e03-a1f5-aeba95e9cb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a customized Chatbot, and will be helping in different domains; like code helping, omniverse stuff, ShotGrid docs, api stuff etc. \n",
      "It is very important that you do have memory, and can go along with the User, and also make use of Knowledge base, which will be provided to you with some workflow.\n",
      "-- If any code is to be written, DO WRITE IT INSIDE ``` <CODE> ``` . This is very important, as this way, the UI will be able to display code in well formatted \n",
      "way, by using some post-processing.\n",
      "\n",
      ".getpass(\"Pinecone API Key:\")\n",
      "\n",
      "os.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")\n",
      "\n",
      "We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n",
      "\n",
      "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
      "\n",
      "from langchain.embeddings.openai import OpenAIEmbeddings\n",
      "\n",
      "from langchain.text_splitter import CharacterTextSplitter\n",
      "\n",
      "from langchain.vectorstores import Pinecone\n",
      "\n",
      "from langchain\n",
      "\n",
      ".local/share/DaVinciResolve/Fusion/Scripts\n",
      "\n",
      "The interactive Console window allows for an easy way to execute simple scripting commands, to query or modify properties, and to test scripts. The console accepts commands in Python 2.7, Python 3.6\n",
      "\n",
      "and Lua and evaluates and executes them immediately. For more information on how to use the Console, please refer to the DaVinci Resolve User Manual.\n",
      "\n",
      "This example Python script creates a simple project:\n",
      "\n",
      ".dll\"\n",
      "\n",
      "    PYTHONPATH=\"%PYTHONPATH%;%RESOLVE_SCRIPT_API%\\Modules\\\"\n",
      "\n",
      "    Linux:\n",
      "\n",
      "    RESOLVE_SCRIPT_API=\"/opt/resolve/Developer/Scripting\"\n",
      "\n",
      "    RESOLVE_SCRIPT_LIB=\"/opt/resolve/libs/Fusion/fusionscript.so\"\n",
      "\n",
      "    PYTHONPATH=\"$PYTHONPATH:$RESOLVE_SCRIPT_API/Modules/\"\n",
      "\n",
      "    (Note: For standard ISO Linux installations, the path above may need to be modified to refer to /home/resolve instead of /opt/resolve)\n",
      "\n",
      "As with Fusion scripts, Resolve scripts can also be invoked via the menu and the Console\n",
      "\n",
      "this is the running chat-history:\n",
      "\n",
      "\n",
      "Query:  what is pyenv\n",
      "\n",
      "Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "answer = chain('what is pyenv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "91352c65-0fb2-465e-b13e-74bad87c732d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'what is pyenv',\n",
       " 'chat_history': '',\n",
       " 'answer': 'Pyenv is a tool used for managing multiple versions of Python on a single system. It allows you to easily switch between different Python versions and create virtual environments for your projects. Pyenv is particularly useful for developers who work on projects that require different Python versions or need to test their code on different Python versions. It helps to avoid conflicts between different versions of Python and ensures that your projects run smoothly.'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e96512aa-a1bc-41f9-b2e9-ef5fc7caa8cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing some input keys: {'question'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAnd how to install python with pyenv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/base.py:288\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    254\u001b[0m     inputs: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Any],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m     include_run_info: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    262\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[1;32m    263\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m            `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     callback_manager \u001b[38;5;241m=\u001b[39m CallbackManager\u001b[38;5;241m.\u001b[39mconfigure(\n\u001b[1;32m    290\u001b[0m         callbacks,\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata,\n\u001b[1;32m    297\u001b[0m     )\n\u001b[1;32m    298\u001b[0m     new_arg_supported \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/base.py:445\u001b[0m, in \u001b[0;36mChain.prep_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    443\u001b[0m     external_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mload_memory_variables(inputs)\n\u001b[1;32m    444\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mexternal_context)\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/base.py:199\u001b[0m, in \u001b[0;36mChain._validate_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    197\u001b[0m missing_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_keys)\u001b[38;5;241m.\u001b[39mdifference(inputs)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_keys:\n\u001b[0;32m--> 199\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing some input keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Missing some input keys: {'question'}"
     ]
    }
   ],
   "source": [
    "answer = chain({'prompt':'And how to install python with pyenv'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c4676c77-82d4-446f-bf5c-9003fabfabed",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'st' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer \u001b[38;5;241m=\u001b[39m chain({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_history\u001b[39m\u001b[38;5;124m\"\u001b[39m: [(message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m], message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m \u001b[43mst\u001b[49m\u001b[38;5;241m.\u001b[39msession_state\u001b[38;5;241m.\u001b[39mmessages]})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'st' is not defined"
     ]
    }
   ],
   "source": [
    "answer = chain({\"question\": prompt, \"chat_history\": [(message[\"role\"], message[\"content\"]) for message in st.session_state.messages]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6e11244c-0b0a-42b2-84fe-44433ac46f9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported chat history format: <class 'str'>. Full chat history: Human: What is pinecone\nAI: Pinecone is a vector database with broad functionality. It is used for storing and searching high-dimensional vectors efficiently. It provides features like similarity search, indexing, and retrieval of vectors. You can use Pinecone to store and query vectors for various applications such as recommendation systems, natural language processing, and image recognition. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mWhat is pinecone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/base.py:312\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 312\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    313\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    314\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    315\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    316\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    299\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    300\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    301\u001b[0m     inputs,\n\u001b[1;32m    302\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    303\u001b[0m )\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 306\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    309\u001b[0m     )\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    311\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:121\u001b[0m, in \u001b[0;36mBaseConversationalRetrievalChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    119\u001b[0m question \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    120\u001b[0m get_chat_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_chat_history \u001b[38;5;129;01mor\u001b[39;00m _get_chat_history\n\u001b[0;32m--> 121\u001b[0m chat_history_str \u001b[38;5;241m=\u001b[39m \u001b[43mget_chat_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mchat_history\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chat_history_str:\n\u001b[1;32m    124\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n",
      "File \u001b[0;32m~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/chains/conversational_retrieval/base.py:46\u001b[0m, in \u001b[0;36m_get_chat_history\u001b[0;34m(chat_history)\u001b[0m\n\u001b[1;32m     44\u001b[0m         buffer \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([human, ai])\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 46\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported chat history format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(dialogue_turn)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Full chat history: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchat_history\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m         )\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m buffer\n",
      "\u001b[0;31mValueError\u001b[0m: Unsupported chat history format: <class 'str'>. Full chat history: Human: What is pinecone\nAI: Pinecone is a vector database with broad functionality. It is used for storing and searching high-dimensional vectors efficiently. It provides features like similarity search, indexing, and retrieval of vectors. You can use Pinecone to store and query vectors for various applications such as recommendation systems, natural language processing, and image recognition. "
     ]
    }
   ],
   "source": [
    "answer = chain({'question':'What is pinecone'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e833964f-fdeb-4c76-bea9-0c6db5379d06",
   "metadata": {},
   "source": [
    "# RoughWork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88981e59-14a9-4cd4-9b48-453b5edf3b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredHTMLLoader(\"/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/vertopal_com_Copy_of_gpt_4_langchain_docs.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "815fcf58-6105-46cc-8e3f-02fab0472a2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41f0a61-f83f-446c-b699-14edb7e0c34c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1525a1ed-3a98-4c4d-81bb-82e94e6c5f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    '.pdf': PyPDFLoader,\n",
    "    #'.xml': UnstructuredXMLLoader,\n",
    "    #'.csv': CSVLoader,\n",
    "    '.html': UnstructuredHTMLLoader,\n",
    "    '.docx': Docx2txtLoader,\n",
    "    '.txt' : TextLoader\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af94687c-2f03-4a8a-b189-e60077a49ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {\"encoding\": \"utf-8\"}),\n",
    "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
    "    \".epub\": (UnstructuredEPubLoader, {}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    \".odt\": (UnstructuredODTLoader, {}),\n",
    "    \".pdf\": (PyPDFLoader, {}),\n",
    "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
    "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
    "    \".ipynb\": (NotebookLoader, {}),\n",
    "    \".py\": (PythonLoader, {}),\n",
    " \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccaa1e43-e133-408f-a043-36385e3f1a51",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m \u001b[0mPyMuPDFLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m        \n",
       "\u001b[0;32mclass\u001b[0m \u001b[0mPyMuPDFLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBasePDFLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Load `PDF` files using `PyMuPDF`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Initialize with a file path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mimport\u001b[0m \u001b[0mfitz\u001b[0m  \u001b[0;31m# noqa:F401\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"`PyMuPDF` package not found, please install it with \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m                \u001b[0;34m\"`pip install pymupdf`\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDocument\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Load file.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyMuPDFParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m           ~/.pyenv/versions/langchain-3.10.2/lib/python3.10/site-packages/langchain/document_loaders/pdf.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "PyMuPDFLoader??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1481f5c5-ff55-4b68-a5d9-5e0ee199747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DirectoryLoader(knowledgebase_folder,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2351c983-17b3-4f5d-93cc-76cbeaba6215",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_fname = Path('/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/.ipynb_checkpoints')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc3dbf7b-637e-44e0-8288-378ffc6ba76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.splitext(t_fname.name)[-1][1:].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "920756fb-b97b-4b7b-8720-2d9e61899bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_doc = BSHTMLLoader('/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/chat.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd0b03bc-3600-4d52-aa4d-d8ea5a0f2356",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_docs = chat_doc.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "689743f4-7859-475c-8f01-1c24f3bbb7f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\nChatGPT Data Export\\n\\n\\n\\n\\n\\n\\n', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/chat.html', 'title': 'ChatGPT Data Export'})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cba62839-ad3d-452b-af2d-0515b1fc0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def html_to_text(html_file_path):\n",
    "    with open(html_file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    \n",
    "    # Extract text from the 'message' and 'author' classes\n",
    "    messages = soup.find_all(class_='message')\n",
    "    authors = [message.find(class_='author') for message in messages]\n",
    "\n",
    "    # Combine the author and message texts\n",
    "    combined_texts = []\n",
    "    for message, author in zip(messages, authors):\n",
    "        combined_texts.append(f\"{author.get_text()}: {message.get_text()}\")\n",
    "\n",
    "    text = '\\n'.join(combined_texts)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56a55c9f-fcde-4192-8e1d-455f83544ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = html_to_text('/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/chat.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "52b9a906-4a45-4bfc-a811-f2a0ead2fa9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ea506cff-37fa-4b9d-af39-c61f55d416e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_docs_from_file(full_pth):\n",
    "\n",
    "    loaded_documents = []\n",
    "    full_pth = Path(full_pth)\n",
    "    ext = os.path.splitext(full_pth.name)[-1][1:].lower()\n",
    "    if ext in FILE_LOADER_MAPPING:\n",
    "        loader_class, loader_args = FILE_LOADER_MAPPING[ext]\n",
    "        loader = loader_class(str(full_pth), **loader_args)\n",
    "    else:\n",
    "        loader = UnstructuredFileLoader(str(full_pth))\n",
    "    loaded_documents.extend(loader.load())\n",
    "    return loaded_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5da51aa-7201-45c4-8bca-ec4df71ccb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_faiss_index(folder_path):\n",
    "    all_files = os.listdir(folder_path)\n",
    "    all_files = [i for i in all_files if not i.endswith('.ipynb_checkpoints')]\n",
    "    print(all_files)\n",
    "    loaded_documents = []\n",
    "    for filename in all_files:\n",
    "        full_pth = Path(f'{folder_path}/{filename}')\n",
    "        ext = os.path.splitext(full_pth.name)[-1][1:].lower()\n",
    "        if ext in FILE_LOADER_MAPPING:\n",
    "            loader_class, loader_args = FILE_LOADER_MAPPING[ext]\n",
    "            loader = loader_class(str(full_pth), **loader_args)\n",
    "        else:\n",
    "            loader = UnstructuredFileLoader(str(full_pth))\n",
    "        loaded_documents.extend(loader.load())\n",
    "    return loaded_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a68350f9-e671-4200-bf80-4790902dd59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['resolve-readme-summary.docx', 'pinecone_docs.txt', 'vertopal_com_Copy_of_gpt_4_langchain_docs.html']\n"
     ]
    }
   ],
   "source": [
    "docs = create_faiss_index(knowledgebase_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6188c6d1-49f8-47b6-b5b5-d593e0fa363f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd70dfdc-5116-4b56-84f5-b6b37bc954b0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='SCRIPTING API FOR DAVINCI RESOLVE STUDIO\\n\\nUpdated as of 18 July 2023\\n\\n----------------------------\\n\\nIn this package, you will find a brief introduction to the Scripting API for DaVinci Resolve Studio. Apart from this README.txt file, this package contains folders containing the basic import\\n\\nmodules for scripting access (DaVinciResolve.py) and some representative examples.\\n\\nFrom v16.2.0 onwards, the nodeIndex parameters accepted by SetLUT() and SetCDL() are 1-based instead of 0-based, i.e. 1 <= nodeIndex <= total number of nodes.\\n\\nOverview\\n\\n--------\\n\\nAs with Blackmagic Design Fusion scripts, user scripts written in Lua and Python programming languages are supported. By default, scripts can be invoked from the Console window in the Fusion page,\\n\\nor via command line. This permission can be changed in Resolve Preferences, to be only from Console, or to be invoked from the local network. Please be aware of the security implications when\\n\\nallowing scripting access from outside of the Resolve application.\\n\\nPrerequisites\\n\\n-------------\\n\\nDaVinci Resolve scripting requires one of the following to be installed (for all users):\\n\\n    Lua 5.1\\n\\n    Python 2.7 64-bit\\n\\n    Python >= 3.6 64-bit\\n\\nUsing a script\\n\\n--------------\\n\\nDaVinci Resolve needs to be running for a script to be invoked.\\n\\nFor a Resolve script to be executed from an external folder, the script needs to know of the API location.\\n\\nYou may need to set the these environment variables to allow for your Python installation to pick up the appropriate dependencies as shown below:\\n\\n    Mac OS X:\\n\\n    RESOLVE_SCRIPT_API=\"/Library/Application Support/Blackmagic Design/DaVinci Resolve/Developer/Scripting\"\\n\\n    RESOLVE_SCRIPT_LIB=\"/Applications/DaVinci Resolve/DaVinci Resolve.app/Contents/Libraries/Fusion/fusionscript.so\"\\n\\n    PYTHONPATH=\"$PYTHONPATH:$RESOLVE_SCRIPT_API/Modules/\"\\n\\n    Windows:\\n\\n    RESOLVE_SCRIPT_API=\"%PROGRAMDATA%\\\\Blackmagic Design\\\\DaVinci Resolve\\\\Support\\\\Developer\\\\Scripting\"\\n\\n    RESOLVE_SCRIPT_LIB=\"C:\\\\Program Files\\\\Blackmagic Design\\\\DaVinci Resolve\\\\fusionscript.dll\"\\n\\n    PYTHONPATH=\"%PYTHONPATH%;%RESOLVE_SCRIPT_API%\\\\Modules\\\\\"\\n\\n    Linux:\\n\\n    RESOLVE_SCRIPT_API=\"/opt/resolve/Developer/Scripting\"\\n\\n    RESOLVE_SCRIPT_LIB=\"/opt/resolve/libs/Fusion/fusionscript.so\"\\n\\n    PYTHONPATH=\"$PYTHONPATH:$RESOLVE_SCRIPT_API/Modules/\"\\n\\n    (Note: For standard ISO Linux installations, the path above may need to be modified to refer to /home/resolve instead of /opt/resolve)\\n\\nAs with Fusion scripts, Resolve scripts can also be invoked via the menu and the Console.\\n\\nOn startup, DaVinci Resolve scans the subfolders in the directories shown below and enumerates the scripts found in the Workspace application menu under Scripts.\\n\\nPlace your script under Utility to be listed in all pages, under Comp or Tool to be available in the Fusion page or under folders for individual pages (Edit, Color or Deliver). Scripts under Deliver are additionally listed under render jobs.\\n\\nPlacing your script here and invoking it from the menu is the easiest way to use scripts.\\n\\n    Mac OS X:\\n\\n- All users: /Library/Application Support/Blackmagic Design/DaVinci Resolve/Fusion/Scripts\\n\\n- Specific user:  /Users/<UserName>/Library/Application Support/Blackmagic Design/DaVinci Resolve/Fusion/Scripts\\n\\n    Windows:\\n\\n- All users: %PROGRAMDATA%\\\\Blackmagic Design\\\\DaVinci Resolve\\\\Fusion\\\\Scripts\\n\\n- Specific user: %APPDATA%\\\\Roaming\\\\Blackmagic Design\\\\DaVinci Resolve\\\\Support\\\\Fusion\\\\Scripts\\n\\n    Linux:\\n\\n- All users: /opt/resolve/Fusion/Scripts  (or /home/resolve/Fusion/Scripts/ depending on installation)\\n\\n- Specific user: $HOME/.local/share/DaVinciResolve/Fusion/Scripts\\n\\nThe interactive Console window allows for an easy way to execute simple scripting commands, to query or modify properties, and to test scripts. The console accepts commands in Python 2.7, Python 3.6\\n\\nand Lua and evaluates and executes them immediately. For more information on how to use the Console, please refer to the DaVinci Resolve User Manual.\\n\\nThis example Python script creates a simple project:\\n\\n    #!/usr/bin/env python\\n\\n    import DaVinciResolveScript as dvr_script\\n\\n    resolve = dvr_script.scriptapp(\"Resolve\")\\n\\n    fusion = resolve.Fusion()\\n\\n    projectManager = resolve.GetProjectManager()\\n\\n    projectManager.CreateProject(\"Hello World\")\\n\\nThe resolve object is the fundamental starting point for scripting via Resolve. As a native object, it can be inspected for further scriptable properties - using table iteration and \"getmetatable\"\\n\\nin Lua and dir, help etc in Python (among other methods). A notable scriptable object above is fusion - it allows access to all existing Fusion scripting functionality.\\n\\nRunning DaVinci Resolve in headless mode\\n\\n----------------------------------------\\n\\nDaVinci Resolve can be launched in a headless mode without the user interface using the -nogui command line option. When DaVinci Resolve is launched using this option, the user interface is disabled.\\n\\nHowever, the various scripting APIs will continue to work as expected.\\n\\nBasic Resolve API\\n\\n-----------------\\n\\nSome commonly used API functions are described below (*). As with the resolve object, each object is inspectable for properties and functions.\\n\\nResolve\\n\\n  Fusion()                                        --> Fusion             # Returns the Fusion object. Starting point for Fusion scripts.\\n\\n  GetMediaStorage()                               --> MediaStorage       # Returns the media storage object to query and act on media locations.\\n\\n  GetProjectManager()                             --> ProjectManager     # Returns the project manager object for currently open database.\\n\\n  OpenPage(pageName)                              --> Bool               # Switches to indicated page in DaVinci Resolve. Input can be one of (\"media\", \"cut\", \"edit\", \"fusion\", \"color\", \"fairlight\", \"deliver\").\\n\\n  GetCurrentPage()                                --> String             # Returns the page currently displayed in the main window. Returned value can be one of (\"media\", \"cut\", \"edit\", \"fusion\", \"color\", \"fairlight\", \"deliver\", None).\\n\\n  GetProductName()                                --> string             # Returns product name.\\n\\n  GetVersion()                                    --> [version fields]   # Returns list of product version fields in [major, minor, patch, build, suffix] format.\\n\\n  GetVersionString()                              --> string             # Returns product version in \"major.minor.patch[suffix].build\" format.\\n\\n  LoadLayoutPreset(presetName)                    --> Bool               # Loads UI layout from saved preset named \\'presetName\\'.\\n\\n  UpdateLayoutPreset(presetName)                  --> Bool               # Overwrites preset named \\'presetName\\' with current UI layout.\\n\\n  ExportLayoutPreset(presetName, presetFilePath)  --> Bool               # Exports preset named \\'presetName\\' to path \\'presetFilePath\\'.\\n\\n  DeleteLayoutPreset(presetName)                  --> Bool               # Deletes preset named \\'presetName\\'.\\n\\n  SaveLayoutPreset(presetName)                    --> Bool               # Saves current UI layout as a preset named \\'presetName\\'.\\n\\n  ImportLayoutPreset(presetFilePath, presetName)  --> Bool               # Imports preset from path \\'presetFilePath\\'. The optional argument \\'presetName\\' specifies how the preset shall be named. If not specified, the preset is named based on the filename.\\n\\n  Quit()                                          --> None               # Quits the Resolve App.\\n\\n  ImportRenderPreset(presetPath)                  --> Bool               # Import a preset from presetPath (string) and set it as current preset for rendering.\\n\\n  ExportRenderPreset(presetName, exportPath)      --> Bool               # Export a preset to a given path (string) if presetName(string) exists.\\n\\n  ImportBurnInPreset(presetPath)                  --> Bool               # Import a data burn in preset from a given presetPath (string)\\n\\n  ExportBurnInPreset(presetName, exportPath)      --> Bool               # Export a data burn in preset to a given path (string) if presetName (string) exists.\\n\\nProjectManager\\n\\n  ArchiveProject(projectName,\\n\\n                 filePath,\\n\\n                 isArchiveSrcMedia=True,\\n\\n                 isArchiveRenderCache=True,\\n\\n                 isArchiveProxyMedia=False)       --> Bool               # Archives project to provided file path with the configuration as provided by the optional arguments\\n\\n  CreateProject(projectName)                      --> Project            # Creates and returns a project if projectName (string) is unique, and None if it is not.\\n\\n  DeleteProject(projectName)                      --> Bool               # Delete project in the current folder if not currently loaded\\n\\n  LoadProject(projectName)                        --> Project            # Loads and returns the project with name = projectName (string) if there is a match found, and None if there is no matching Project.\\n\\n  GetCurrentProject()                             --> Project            # Returns the currently loaded Resolve project.\\n\\n  SaveProject()                                   --> Bool               # Saves the currently loaded project with its own name. Returns True if successful.\\n\\n  CloseProject(project)                           --> Bool               # Closes the specified project without saving.\\n\\n  CreateFolder(folderName)                        --> Bool               # Creates a folder if folderName (string) is unique.\\n\\n  DeleteFolder(folderName)                        --> Bool               # Deletes the specified folder if it exists. Returns True in case of success.\\n\\n  GetProjectListInCurrentFolder()                 --> [project names...] # Returns a list of project names in current folder.\\n\\n  GetFolderListInCurrentFolder()                  --> [folder names...]  # Returns a list of folder names in current folder.\\n\\n  GotoRootFolder()                                --> Bool               # Opens root folder in database.\\n\\n  GotoParentFolder()                              --> Bool               # Opens parent folder of current folder in database if current folder has parent.\\n\\n  GetCurrentFolder()                              --> string             # Returns the current folder name.\\n\\n  OpenFolder(folderName)                          --> Bool               # Opens folder under given name.\\n\\n  ImportProject(filePath, projectName=None)       --> Bool               # Imports a project from the file path provided with given project name, if any. Returns True if successful.\\n\\n  ExportProject(projectName, filePath, withStillsAndLUTs=True) --> Bool  # Exports project to provided file path, including stills and LUTs if withStillsAndLUTs is True (enabled by default). Returns True in case of success.\\n\\n  RestoreProject(filePath, projectName=None)      --> Bool               # Restores a project from the file path provided with given project name, if any. Returns True if successful.\\n\\n  GetCurrentDatabase()                            --> {dbInfo}           # Returns a dictionary (with keys \\'DbType\\', \\'DbName\\' and optional \\'IpAddress\\') corresponding to the current database connection\\n\\n  GetDatabaseList()                               --> [{dbInfo}]         # Returns a list of dictionary items (with keys \\'DbType\\', \\'DbName\\' and optional \\'IpAddress\\') corresponding to all the databases added to Resolve\\n\\n  SetCurrentDatabase({dbInfo})                    --> Bool               # Switches current database connection to the database specified by the keys below, and closes any open project.\\n\\n                                                                         # \\'DbType\\': \\'Disk\\' or \\'PostgreSQL\\' (string)\\n\\n                                                                         # \\'DbName\\': database name (string)\\n\\n                                                                         # \\'IpAddress\\': IP address of the PostgreSQL server (string, optional key - defaults to \\'127.0.0.1\\')\\n\\nProject\\n\\n  GetMediaPool()                                  --> MediaPool          # Returns the Media Pool object.\\n\\n  GetTimelineCount()                              --> int                # Returns the number of timelines currently present in the project.\\n\\n  GetTimelineByIndex(idx)                         --> Timeline           # Returns timeline at the given index, 1 <= idx <= project.GetTimelineCount()\\n\\n  GetCurrentTimeline()                            --> Timeline           # Returns the currently loaded timeline.\\n\\n  SetCurrentTimeline(timeline)                    --> Bool               # Sets given timeline as current timeline for the project. Returns True if successful.\\n\\n  GetGallery()                                    --> Gallery            # Returns the Gallery object.\\n\\n  GetName()                                       --> string             # Returns project name.\\n\\n  SetName(projectName)                            --> Bool               # Sets project name if given projectName (string) is unique.\\n\\n  GetPresetList()                                 --> [presets...]       # Returns a list of presets and their information.\\n\\n  SetPreset(presetName)                           --> Bool               # Sets preset by given presetName (string) into project.\\n\\n  AddRenderJob()                                  --> string             # Adds a render job based on current render settings to the render queue. Returns a unique job id (string) for the new render job.\\n\\n  DeleteRenderJob(jobId)                          --> Bool               # Deletes render job for input job id (string).\\n\\n  DeleteAllRenderJobs()                           --> Bool               # Deletes all render jobs in the queue.\\n\\n  GetRenderJobList()                              --> [render jobs...]   # Returns a list of render jobs and their information.\\n\\n  GetRenderPresetList()                           --> [presets...]       # Returns a list of render presets and their information.\\n\\n  StartRendering(jobId1, jobId2, ...)             --> Bool               # Starts rendering jobs indicated by the input job ids.\\n\\n  StartRendering([jobIds...], isInteractiveMode=False)    --> Bool       # Starts rendering jobs indicated by the input job ids.\\n\\n                                                                         # The optional \"isInteractiveMode\", when set, enables error feedback in the UI during rendering.\\n\\n  StartRendering(isInteractiveMode=False)                 --> Bool       # Starts rendering all queued render jobs.\\n\\n                                                                         # The optional \"isInteractiveMode\", when set, enables error feedback in the UI during rendering.\\n\\n  StopRendering()                                 --> None               # Stops any current render processes.\\n\\n  IsRenderingInProgress()                         --> Bool               # Returns True if rendering is in progress.\\n\\n  LoadRenderPreset(presetName)                    --> Bool               # Sets a preset as current preset for rendering if presetName (string) exists.\\n\\n  SaveAsNewRenderPreset(presetName)               --> Bool               # Creates new render preset by given name if presetName(string) is unique.\\n\\n  SetRenderSettings({settings})                   --> Bool               # Sets given settings for rendering. Settings is a dict, with support for the keys:\\n\\n                                                                         # Refer to \"Looking up render settings\" section for information for supported settings\\n\\n  GetRenderJobStatus(jobId)                       --> {status info}      # Returns a dict with job status and completion percentage of the job by given jobId (string).\\n\\n  GetSetting(settingName)                         --> string             # Returns value of project setting (indicated by settingName, string). Check the section below for more information.\\n\\n  SetSetting(settingName, settingValue)           --> Bool               # Sets the project setting (indicated by settingName, string) to the value (settingValue, string). Check the section below for more information.\\n\\n  GetRenderFormats()                              --> {render formats..} # Returns a dict (format -> file extension) of available render formats.\\n\\n  GetRenderCodecs(renderFormat)                   --> {render codecs...} # Returns a dict (codec description -> codec name) of available codecs for given render format (string).\\n\\n  GetCurrentRenderFormatAndCodec()                --> {format, codec}    # Returns a dict with currently selected format \\'format\\' and render codec \\'codec\\'.\\n\\n  SetCurrentRenderFormatAndCodec(format, codec)   --> Bool               # Sets given render format (string) and render codec (string) as options for rendering.\\n\\n  GetCurrentRenderMode()                          --> int                # Returns the render mode: 0 - Individual clips, 1 - Single clip.\\n\\n  SetCurrentRenderMode(renderMode)                --> Bool               # Sets the render mode. Specify renderMode = 0 for Individual clips, 1 for Single clip.\\n\\n  GetRenderResolutions(format, codec)             --> [{Resolution}]     # Returns list of resolutions applicable for the given render format (string) and render codec (string). Returns full list of resolutions if no argument is provided. Each element in the list is a dictionary with 2 keys \"Width\" and \"Height\".\\n\\n  RefreshLUTList()                                --> Bool               # Refreshes LUT List\\n\\n  GetUniqueId()                                   --> string             # Returns a unique ID for the project item\\n\\n  InsertAudioToCurrentTrackAtPlayhead(mediaPath,  --> Bool               # Inserts the media specified by mediaPath (string) with startOffsetInSamples (int) and durationInSamples (int) at the playhead on a selected track on the Fairlight page. Returns True if successful, otherwise False.\\n\\n          startOffsetInSamples, durationInSamples)\\n\\n  LoadBurnInPreset(presetName)                    --> Bool               # Loads user defined data burn in preset for project when supplied presetName (string). Returns true if successful.\\n\\n  ExportCurrentFrameAsStill(filePath)             --> Bool               # Exports current frame as still to supplied filePath. filePath must end in valid export file format. Returns True if succssful, False otherwise.\\n\\nMediaStorage\\n\\n  GetMountedVolumeList()                          --> [paths...]         # Returns list of folder paths corresponding to mounted volumes displayed in Resolves Media Storage.\\n\\n  GetSubFolderList(folderPath)                    --> [paths...]         # Returns list of folder paths in the given absolute folder path.\\n\\n  GetFileList(folderPath)                         --> [paths...]         # Returns list of media and file listings in the given absolute folder path. Note that media listings may be logically consolidated entries.\\n\\n  RevealInStorage(path)                           --> Bool               # Expands and displays given file/folder path in Resolves Media Storage.\\n\\n  AddItemListToMediaPool(item1, item2, ...)       --> [clips...]         # Adds specified file/folder paths from Media Storage into current Media Pool folder. Input is one or more file/folder paths. Returns a list of the MediaPoolItems created.\\n\\n  AddItemListToMediaPool([items...])              --> [clips...]         # Adds specified file/folder paths from Media Storage into current Media Pool folder. Input is an array of file/folder paths. Returns a list of the MediaPoolItems created.\\n\\n  AddItemListToMediaPool([{itemInfo}, ...])       --> [clips...]         # Adds list of itemInfos specified as dict of \"media\", \"startFrame\" (int), \"endFrame\" (int) from Media Storage into current Media Pool folder. Returns a list of the MediaPoolItems created.\\n\\n  AddClipMattesToMediaPool(MediaPoolItem, [paths], stereoEye) --> Bool   # Adds specified media files as mattes for the specified MediaPoolItem. StereoEye is an optional argument for specifying which eye to add the matte to for stereo clips (\"left\" or \"right\"). Returns True if successful.\\n\\n  AddTimelineMattesToMediaPool([paths])           --> [MediaPoolItems]   # Adds specified media files as timeline mattes in current media pool folder. Returns a list of created MediaPoolItems.\\n\\nMediaPool\\n\\n  GetRootFolder()                                 --> Folder             # Returns root Folder of Media Pool\\n\\n  AddSubFolder(folder, name)                      --> Folder             # Adds new subfolder under specified Folder object with the given name.\\n\\n  RefreshFolders()                                --> Bool               # Updates the folders in collaboration mode\\n\\n  CreateEmptyTimeline(name)                       --> Timeline           # Adds new timeline with given name.\\n\\n  AppendToTimeline(clip1, clip2, ...)             --> [TimelineItem]     # Appends specified MediaPoolItem objects in the current timeline. Returns the list of appended timelineItems.\\n\\n  AppendToTimeline([clips])                       --> [TimelineItem]     # Appends specified MediaPoolItem objects in the current timeline. Returns the list of appended timelineItems.\\n\\n  AppendToTimeline([{clipInfo}, ...])             --> [TimelineItem]     # Appends list of clipInfos specified as dict of \"mediaPoolItem\", \"startFrame\" (int), \"endFrame\" (int), (optional) \"mediaType\" (int; 1 - Video only, 2 - Audio only), \"trackIndex\" (int) and \"recordFrame\" (int). Returns the list of appended timelineItems.\\n\\n  CreateTimelineFromClips(name, clip1, clip2,...) --> Timeline           # Creates new timeline with specified name, and appends the specified MediaPoolItem objects.\\n\\n  CreateTimelineFromClips(name, [clips])          --> Timeline           # Creates new timeline with specified name, and appends the specified MediaPoolItem objects.\\n\\n  CreateTimelineFromClips(name, [{clipInfo}])     --> Timeline           # Creates new timeline with specified name, appending the list of clipInfos specified as a dict of \"mediaPoolItem\", \"startFrame\" (int), \"endFrame\" (int), \"recordFrame\" (int).\\n\\n  ImportTimelineFromFile(filePath, {importOptions}) --> Timeline         # Creates timeline based on parameters within given file (AAF/EDL/XML/FCPXML/DRT/ADL) and optional importOptions dict, with support for the keys:\\n\\n                                                                         # \"timelineName\": string, specifies the name of the timeline to be created. Not valid for DRT import\\n\\n                                                                         # \"importSourceClips\": Bool, specifies whether source clips should be imported, True by default. Not valid for DRT import\\n\\n                                                                         # \"sourceClipsPath\": string, specifies a filesystem path to search for source clips if the media is inaccessible in their original path and if \"importSourceClips\" is True\\n\\n                                                                         # \"sourceClipsFolders\": List of Media Pool folder objects to search for source clips if the media is not present in current folder and if \"importSourceClips\" is False. Not valid for DRT import\\n\\n                                                                         # \"interlaceProcessing\": Bool, specifies whether to enable interlace processing on the imported timeline being created. valid only for AAF import\\n\\n  DeleteTimelines([timeline])                     --> Bool               # Deletes specified timelines in the media pool.\\n\\n  GetCurrentFolder()                              --> Folder             # Returns currently selected Folder.\\n\\n  SetCurrentFolder(Folder)                        --> Bool               # Sets current folder by given Folder.\\n\\n  DeleteClips([clips])                            --> Bool               # Deletes specified clips or timeline mattes in the media pool\\n\\n  ImportFolderFromFile(filePath, sourceClipsPath=\"\") --> Bool            # Returns true if import from given DRB filePath is successful, false otherwise\\n\\n                                                                         # sourceClipsPath is a string that specifies a filesystem path to search for source clips if the media is inaccessible in their original path, empty by default\\n\\n  DeleteFolders([subfolders])                     --> Bool               # Deletes specified subfolders in the media pool\\n\\n  MoveClips([clips], targetFolder)                --> Bool               # Moves specified clips to target folder.\\n\\n  MoveFolders([folders], targetFolder)            --> Bool               # Moves specified folders to target folder.\\n\\n  GetClipMatteList(MediaPoolItem)                 --> [paths]            # Get mattes for specified MediaPoolItem, as a list of paths to the matte files.\\n\\n  GetTimelineMatteList(Folder)                    --> [MediaPoolItems]   # Get mattes in specified Folder, as list of MediaPoolItems.\\n\\n  DeleteClipMattes(MediaPoolItem, [paths])        --> Bool               # Delete mattes based on their file paths, for specified MediaPoolItem. Returns True on success.\\n\\n  RelinkClips([MediaPoolItem], folderPath)        --> Bool               # Update the folder location of specified media pool clips with the specified folder path.\\n\\n  UnlinkClips([MediaPoolItem])                    --> Bool               # Unlink specified media pool clips.\\n\\n  ImportMedia([items...])                         --> [MediaPoolItems]   # Imports specified file/folder paths into current Media Pool folder. Input is an array of file/folder paths. Returns a list of the MediaPoolItems created.\\n\\n  ImportMedia([{clipInfo}])                       --> [MediaPoolItems]   # Imports file path(s) into current Media Pool folder as specified in list of clipInfo dict. Returns a list of the MediaPoolItems created.\\n\\n                                                                         # Each clipInfo gets imported as one MediaPoolItem unless \\'Show Individual Frames\\' is turned on.\\n\\n                                                                         # Example: ImportMedia([{\"FilePath\":\"file_%03d.dpx\", \"StartIndex\":1, \"EndIndex\":100}]) would import clip \"file_[001-100].dpx\".\\n\\n  ExportMetadata(fileName, [clips])               --> Bool               # Exports metadata of specified clips to \\'fileName\\' in CSV format.\\n\\n                                                                         # If no clips are specified, all clips from media pool will be used.\\n\\n  GetUniqueId()                                   --> string             # Returns a unique ID for the media pool\\n\\nFolder\\n\\n  GetClipList()                                   --> [clips...]         # Returns a list of clips (items) within the folder.\\n\\n  GetName()                                       --> string             # Returns the media folder name.\\n\\n  GetSubFolderList()                              --> [folders...]       # Returns a list of subfolders in the folder.\\n\\n  GetIsFolderStale()                              --> bool               # Returns true if folder is stale in collaboration mode, false otherwise\\n\\n  GetUniqueId()                                   --> string             # Returns a unique ID for the media pool folder\\n\\n  Export(filePath)                                --> bool               # Returns true if export of DRB folder to filePath is successful, false otherwise\\n\\nMediaPoolItem\\n\\n  GetName()                                       --> string             # Returns the clip name.\\n\\n  GetMetadata(metadataType=None)                  --> string|dict        # Returns the metadata value for the key \\'metadataType\\'.\\n\\n                                                                         # If no argument is specified, a dict of all set metadata properties is returned.\\n\\n  SetMetadata(metadataType, metadataValue)        --> Bool               # Sets the given metadata to metadataValue (string). Returns True if successful.\\n\\n  SetMetadata({metadata})                         --> Bool               # Sets the item metadata with specified \\'metadata\\' dict. Returns True if successful.\\n\\n  GetMediaId()                                    --> string             # Returns the unique ID for the MediaPoolItem.\\n\\n  AddMarker(frameId, color, name, note, duration, --> Bool               # Creates a new marker at given frameId position and with given marker information. \\'customData\\' is optional and helps to attach user specific data to the marker.\\n\\n            customData)\\n\\n  GetMarkers()                                    --> {markers...}       # Returns a dict (frameId -> {information}) of all markers and dicts with their information.\\n\\n                                                                         # Example of output format: {96.0: {\\'color\\': \\'Green\\', \\'duration\\': 1.0, \\'note\\': \\'\\', \\'name\\': \\'Marker 1\\', \\'customData\\': \\'\\'}, ...}\\n\\n                                                                         # In the above example - there is one \\'Green\\' marker at offset 96 (position of the marker)\\n\\n  GetMarkerByCustomData(customData)               --> {markers...}       # Returns marker {information} for the first matching marker with specified customData.\\n\\n  UpdateMarkerCustomData(frameId, customData)     --> Bool               # Updates customData (string) for the marker at given frameId position. CustomData is not exposed via UI and is useful for scripting developer to attach any user specific data to markers.\\n\\n  GetMarkerCustomData(frameId)                    --> string             # Returns customData string for the marker at given frameId position.\\n\\n  DeleteMarkersByColor(color)                     --> Bool               # Delete all markers of the specified color from the media pool item. \"All\" as argument deletes all color markers.\\n\\n  DeleteMarkerAtFrame(frameNum)                   --> Bool               # Delete marker at frame number from the media pool item.\\n\\n  DeleteMarkerByCustomData(customData)            --> Bool               # Delete first matching marker with specified customData.\\n\\n  AddFlag(color)                                  --> Bool               # Adds a flag with given color (string).\\n\\n  GetFlagList()                                   --> [colors...]        # Returns a list of flag colors assigned to the item.\\n\\n  ClearFlags(color)                               --> Bool               # Clears the flag of the given color if one exists. An \"All\" argument is supported and clears all flags.\\n\\n  GetClipColor()                                  --> string             # Returns the item color as a string.\\n\\n  SetClipColor(colorName)                         --> Bool               # Sets the item color based on the colorName (string).\\n\\n  ClearClipColor()                                --> Bool               # Clears the item color.\\n\\n  GetClipProperty(propertyName=None)              --> string|dict        # Returns the property value for the key \\'propertyName\\'.\\n\\n                                                                         # If no argument is specified, a dict of all clip properties is returned. Check the section below for more information.\\n\\n  SetClipProperty(propertyName, propertyValue)    --> Bool               # Sets the given property to propertyValue (string). Check the section below for more information.\\n\\n  LinkProxyMedia(proxyMediaFilePath)              --> Bool               # Links proxy media located at path specified by arg \\'proxyMediaFilePath\\' with the current clip. \\'proxyMediaFilePath\\' should be absolute clip path.\\n\\n  UnlinkProxyMedia()                              --> Bool               # Unlinks any proxy media associated with clip.\\n\\n  ReplaceClip(filePath)                           --> Bool               # Replaces the underlying asset and metadata of MediaPoolItem with the specified absolute clip path.\\n\\n  GetUniqueId()                                   --> string             # Returns a unique ID for the media pool item\\n\\n  TranscribeAudio()                               --> Bool               # Transcribes audio of the MediaPoolItem. Returns True if successful; False otherwise\\n\\n  ClearTranscription()                            --> Bool               # Clears audio transcription of the MediaPoolItem. Returns True if successful; False otherwise.\\n\\nTimeline\\n\\n  GetName()                                       --> string             # Returns the timeline name.\\n\\n  SetName(timelineName)                           --> Bool               # Sets the timeline name if timelineName (string) is unique. Returns True if successful.\\n\\n  GetStartFrame()                                 --> int                # Returns the frame number at the start of timeline.\\n\\n  GetEndFrame()                                   --> int                # Returns the frame number at the end of timeline.\\n\\n  SetStartTimecode(timecode)                      --> Bool               # Set the start timecode of the timeline to the string \\'timecode\\'. Returns true when the change is successful, false otherwise.\\n\\n  GetStartTimecode()                              --> string             # Returns the start timecode for the timeline.\\n\\n  GetTrackCount(trackType)                        --> int                # Returns the number of tracks for the given track type (\"audio\", \"video\" or \"subtitle\").\\n\\n  AddTrack(trackType, optionalSubTrackType)       --> Bool               # Adds track of trackType (\"video\", \"subtitle\", \"audio\"). Second argument optionalSubTrackType is required for \"audio\"\\n\\n                                                                         # optionalSubTrackType can be one of {\"mono\", \"stereo\", \"5.1\", \"5.1film\", \"7.1\", \"7.1film\", \"adaptive1\", ... , \"adaptive24\"}\\n\\n  DeleteTrack(trackType, trackIndex)              --> Bool               # Deletes track of trackType (\"video\", \"subtitle\", \"audio\") and given trackIndex. 1 <= trackIndex <= GetTrackCount(trackType).\\n\\n  SetTrackEnable(trackType, trackIndex, Bool)     --> Bool               # Enables/Disables track with given trackType and trackIndex\\n\\n                                                                         # trackType is one of {\"audio\", \"video\", \"subtitle\"}\\n\\n                                                                         # 1 <= trackIndex <= GetTrackCount(trackType).\\n\\n  GetIsTrackEnabled(trackType, trackIndex)        --> Bool               # Returns True if track with given trackType and trackIndex is enabled and False otherwise.\\n\\n                                                                         # trackType is one of {\"audio\", \"video\", \"subtitle\"}\\n\\n                                                                         # 1 <= trackIndex <= GetTrackCount(trackType).\\n\\n  SetTrackLock(trackType, trackIndex, Bool)       --> Bool               # Locks/Unlocks track with given trackType and trackIndex\\n\\n                                                                         # trackType is one of {\"audio\", \"video\", \"subtitle\"}\\n\\n                                                                         # 1 <= trackIndex <= GetTrackCount(trackType).\\n\\n  GetIsTrackLocked(trackType, trackIndex)         --> Bool               # Returns True if track with given trackType and trackIndex is locked and False otherwise.\\n\\n                                                                         # trackType is one of {\"audio\", \"video\", \"subtitle\"}\\n\\n                                                                         # 1 <= trackIndex <= GetTrackCount(trackType).\\n\\n  DeleteClips([timelineItems], Bool)              --> Bool               # Deletes specified TimelineItems from the timeline, performing ripple delete if the second argument is True. Second argument is optional (The default for this is False)\\n\\n  SetClipsLinked([timelineItems], Bool)           --> Bool               # Links or unlinks the specified TimelineItems depending on second argument.\\n\\n  GetItemListInTrack(trackType, index)            --> [items...]         # Returns a list of timeline items on that track (based on trackType and index). 1 <= index <= GetTrackCount(trackType).\\n\\n  AddMarker(frameId, color, name, note, duration, --> Bool               # Creates a new marker at given frameId position and with given marker information. \\'customData\\' is optional and helps to attach user specific data to the marker.\\n\\n            customData)\\n\\n  GetMarkers()                                    --> {markers...}       # Returns a dict (frameId -> {information}) of all markers and dicts with their information.\\n\\n                                                                         # Example: a value of {96.0: {\\'color\\': \\'Green\\', \\'duration\\': 1.0, \\'note\\': \\'\\', \\'name\\': \\'Marker 1\\', \\'customData\\': \\'\\'}, ...} indicates a single green marker at timeline offset 96\\n\\n  GetMarkerByCustomData(customData)               --> {markers...}       # Returns marker {information} for the first matching marker with specified customData.\\n\\n  UpdateMarkerCustomData(frameId, customData)     --> Bool               # Updates customData (string) for the marker at given frameId position. CustomData is not exposed via UI and is useful for scripting developer to attach any user specific data to markers.\\n\\n  GetMarkerCustomData(frameId)                    --> string             # Returns customData string for the marker at given frameId position.\\n\\n  DeleteMarkersByColor(color)                     --> Bool               # Deletes all timeline markers of the specified color. An \"All\" argument is supported and deletes all timeline markers.\\n\\n  DeleteMarkerAtFrame(frameNum)                   --> Bool               # Deletes the timeline marker at the given frame number.\\n\\n  DeleteMarkerByCustomData(customData)            --> Bool               # Delete first matching marker with specified customData.\\n\\n  ApplyGradeFromDRX(path, gradeMode, item1, item2, ...)--> Bool          # Loads a still from given file path (string) and applies grade to Timeline Items with gradeMode (int): 0 - \"No keyframes\", 1 - \"Source Timecode aligned\", 2 - \"Start Frames aligned\".\\n\\n  ApplyGradeFromDRX(path, gradeMode, [items])     --> Bool               # Loads a still from given file path (string) and applies grade to Timeline Items with gradeMode (int): 0 - \"No keyframes\", 1 - \"Source Timecode aligned\", 2 - \"Start Frames aligned\".\\n\\n  GetCurrentTimecode()                            --> string             # Returns a string timecode representation for the current playhead position, while on Cut, Edit, Color, Fairlight and Deliver pages.\\n\\n  SetCurrentTimecode(timecode)                    --> Bool               # Sets current playhead position from input timecode for Cut, Edit, Color, Fairlight and Deliver pages.\\n\\n  GetCurrentVideoItem()                           --> item               # Returns the current video timeline item.\\n\\n  GetCurrentClipThumbnailImage()                  --> {thumbnailData}    # Returns a dict (keys \"width\", \"height\", \"format\" and \"data\") with data containing raw thumbnail image data (RGB 8-bit image data encoded in base64 format) for current media in the Color Page.\\n\\n                                                                         # An example of how to retrieve and interpret thumbnails is provided in 6_get_current_media_thumbnail.py in the Examples folder.\\n\\n  GetTrackName(trackType, trackIndex)             --> string             # Returns the track name for track indicated by trackType (\"audio\", \"video\" or \"subtitle\") and index. 1 <= trackIndex <= GetTrackCount(trackType).\\n\\n  SetTrackName(trackType, trackIndex, name)       --> Bool               # Sets the track name (string) for track indicated by trackType (\"audio\", \"video\" or \"subtitle\") and index. 1 <= trackIndex <= GetTrackCount(trackType).\\n\\n  DuplicateTimeline(timelineName)                 --> timeline           # Duplicates the timeline and returns the created timeline, with the (optional) timelineName, on success.\\n\\n  CreateCompoundClip([timelineItems], {clipInfo}) --> timelineItem       # Creates a compound clip of input timeline items with an optional clipInfo map: {\"startTimecode\" : \"00:00:00:00\", \"name\" : \"Compound Clip 1\"}. It returns the created timeline item.\\n\\n  CreateFusionClip([timelineItems])               --> timelineItem       # Creates a Fusion clip of input timeline items. It returns the created timeline item.\\n\\n  ImportIntoTimeline(filePath, {importOptions})   --> Bool               # Imports timeline items from an AAF file and optional importOptions dict into the timeline, with support for the keys:\\n\\n                                                                         # \"autoImportSourceClipsIntoMediaPool\": Bool, specifies if source clips should be imported into media pool, True by default\\n\\n                                                                         # \"ignoreFileExtensionsWhenMatching\": Bool, specifies if file extensions should be ignored when matching, False by default\\n\\n                                                                         # \"linkToSourceCameraFiles\": Bool, specifies if link to source camera files should be enabled, False by default\\n\\n                                                                         # \"useSizingInfo\": Bool, specifies if sizing information should be used, False by default\\n\\n                                                                         # \"importMultiChannelAudioTracksAsLinkedGroups\": Bool, specifies if multi-channel audio tracks should be imported as linked groups, False by default\\n\\n                                                                         # \"insertAdditionalTracks\": Bool, specifies if additional tracks should be inserted, True by default\\n\\n                                                                         # \"insertWithOffset\": string, specifies insert with offset value in timecode format - defaults to \"00:00:00:00\", applicable if \"insertAdditionalTracks\" is False\\n\\n                                                                         # \"sourceClipsPath\": string, specifies a filesystem path to search for source clips if the media is inaccessible in their original path and if \"ignoreFileExtensionsWhenMatching\" is True\\n\\n                                                                         # \"sourceClipsFolders\": string, list of Media Pool folder objects to search for source clips if the media is not present in current folder\\n\\n  Export(fileName, exportType, exportSubtype)     --> Bool               # Exports timeline to \\'fileName\\' as per input exportType & exportSubtype format.\\n\\n                                                                         # Refer to section \"Looking up timeline export properties\" for information on the parameters.\\n\\n  GetSetting(settingName)                         --> string             # Returns value of timeline setting (indicated by settingName : string). Check the section below for more information.\\n\\n  SetSetting(settingName, settingValue)           --> Bool               # Sets timeline setting (indicated by settingName : string) to the value (settingValue : string). Check the section below for more information.\\n\\n  InsertGeneratorIntoTimeline(generatorName)      --> TimelineItem       # Inserts a generator (indicated by generatorName : string) into the timeline.\\n\\n  InsertFusionGeneratorIntoTimeline(generatorName) --> TimelineItem      # Inserts a Fusion generator (indicated by generatorName : string) into the timeline.\\n\\n  InsertFusionCompositionIntoTimeline()           --> TimelineItem       # Inserts a Fusion composition into the timeline.\\n\\n  InsertOFXGeneratorIntoTimeline(generatorName)   --> TimelineItem       # Inserts an OFX generator (indicated by generatorName : string) into the timeline.\\n\\n  InsertTitleIntoTimeline(titleName)              --> TimelineItem       # Inserts a title (indicated by titleName : string) into the timeline.\\n\\n  InsertFusionTitleIntoTimeline(titleName)        --> TimelineItem       # Inserts a Fusion title (indicated by titleName : string) into the timeline.\\n\\n  GrabStill()                                     --> galleryStill       # Grabs still from the current video clip. Returns a GalleryStill object.\\n\\n  GrabAllStills(stillFrameSource)                 --> [galleryStill]     # Grabs stills from all the clips of the timeline at \\'stillFrameSource\\' (1 - First frame, 2 - Middle frame). Returns the list of GalleryStill objects.\\n\\n  GetUniqueId()                                   --> string             # Returns a unique ID for the timeline\\n\\n  CreateSubtitlesFromAudio()                      --> Bool               # Creates subtitles from audio for the timeline. Returns True on success, False otherwise.\\n\\n  DetectSceneCuts()                               --> Bool               # Detects and makes scene cuts along the timeline. Returns True if successful, False otherwise.\\n\\nTimelineItem\\n\\n  GetName()                                       --> string             # Returns the item name.\\n\\n  GetDuration()                                   --> int                # Returns the item duration.\\n\\n  GetEnd()                                        --> int                # Returns the end frame position on the timeline.\\n\\n  GetFusionCompCount()                            --> int                # Returns number of Fusion compositions associated with the timeline item.\\n\\n  GetFusionCompByIndex(compIndex)                 --> fusionComp         # Returns the Fusion composition object based on given index. 1 <= compIndex <= timelineItem.GetFusionCompCount()\\n\\n  GetFusionCompNameList()                         --> [names...]         # Returns a list of Fusion composition names associated with the timeline item.\\n\\n  GetFusionCompByName(compName)                   --> fusionComp         # Returns the Fusion composition object based on given name.\\n\\n  GetLeftOffset()                                 --> int                # Returns the maximum extension by frame for clip from left side.\\n\\n  GetRightOffset()                                --> int                # Returns the maximum extension by frame for clip from right side.\\n\\n  GetStart()                                      --> int                # Returns the start frame position on the timeline.\\n\\n  SetProperty(propertyKey, propertyValue)         --> Bool               # Sets the value of property \"propertyKey\" to value \"propertyValue\"\\n\\n                                                                         # Refer to \"Looking up Timeline item properties\" for more information\\n\\n  GetProperty(propertyKey)                        --> int/[key:value]    # returns the value of the specified key\\n\\n                                                                         # if no key is specified, the method returns a dictionary(python) or table(lua) for all supported keys\\n\\n  AddMarker(frameId, color, name, note, duration, --> Bool               # Creates a new marker at given frameId position and with given marker information. \\'customData\\' is optional and helps to attach user specific data to the marker.\\n\\n            customData)\\n\\n  GetMarkers()                                    --> {markers...}       # Returns a dict (frameId -> {information}) of all markers and dicts with their information.\\n\\n                                                                         # Example: a value of {96.0: {\\'color\\': \\'Green\\', \\'duration\\': 1.0, \\'note\\': \\'\\', \\'name\\': \\'Marker 1\\', \\'customData\\': \\'\\'}, ...} indicates a single green marker at clip offset 96\\n\\n  GetMarkerByCustomData(customData)               --> {markers...}       # Returns marker {information} for the first matching marker with specified customData.\\n\\n  UpdateMarkerCustomData(frameId, customData)     --> Bool               # Updates customData (string) for the marker at given frameId position. CustomData is not exposed via UI and is useful for scripting developer to attach any user specific data to markers.\\n\\n  GetMarkerCustomData(frameId)                    --> string             # Returns customData string for the marker at given frameId position.\\n\\n  DeleteMarkersByColor(color)                     --> Bool               # Delete all markers of the specified color from the timeline item. \"All\" as argument deletes all color markers.\\n\\n  DeleteMarkerAtFrame(frameNum)                   --> Bool               # Delete marker at frame number from the timeline item.\\n\\n  DeleteMarkerByCustomData(customData)            --> Bool               # Delete first matching marker with specified customData.\\n\\n  AddFlag(color)                                  --> Bool               # Adds a flag with given color (string).\\n\\n  GetFlagList()                                   --> [colors...]        # Returns a list of flag colors assigned to the item.\\n\\n  ClearFlags(color)                               --> Bool               # Clear flags of the specified color. An \"All\" argument is supported to clear all flags.\\n\\n  GetClipColor()                                  --> string             # Returns the item color as a string.\\n\\n  SetClipColor(colorName)                         --> Bool               # Sets the item color based on the colorName (string).\\n\\n  ClearClipColor()                                --> Bool               # Clears the item color.\\n\\n  AddFusionComp()                                 --> fusionComp         # Adds a new Fusion composition associated with the timeline item.\\n\\n  ImportFusionComp(path)                          --> fusionComp         # Imports a Fusion composition from given file path by creating and adding a new composition for the item.\\n\\n  ExportFusionComp(path, compIndex)               --> Bool               # Exports the Fusion composition based on given index to the path provided.\\n\\n  DeleteFusionCompByName(compName)                --> Bool               # Deletes the named Fusion composition.\\n\\n  LoadFusionCompByName(compName)                  --> fusionComp         # Loads the named Fusion composition as the active composition.\\n\\n  RenameFusionCompByName(oldName, newName)        --> Bool               # Renames the Fusion composition identified by oldName.\\n\\n  AddVersion(versionName, versionType)            --> Bool               # Adds a new color version for a video clip based on versionType (0 - local, 1 - remote).\\n\\n  GetCurrentVersion()                             --> {versionName...}   # Returns the current version of the video clip. The returned value will have the keys versionName and versionType(0 - local, 1 - remote).\\n\\n  DeleteVersionByName(versionName, versionType)   --> Bool               # Deletes a color version by name and versionType (0 - local, 1 - remote).\\n\\n  LoadVersionByName(versionName, versionType)     --> Bool               # Loads a named color version as the active version. versionType: 0 - local, 1 - remote.\\n\\n  RenameVersionByName(oldName, newName, versionType)--> Bool             # Renames the color version identified by oldName and versionType (0 - local, 1 - remote).\\n\\n  GetVersionNameList(versionType)                 --> [names...]         # Returns a list of all color versions for the given versionType (0 - local, 1 - remote).\\n\\n  GetMediaPoolItem()                              --> MediaPoolItem      # Returns the media pool item corresponding to the timeline item if one exists.\\n\\n  GetStereoConvergenceValues()                    --> {keyframes...}     # Returns a dict (offset -> value) of keyframe offsets and respective convergence values.\\n\\n  GetStereoLeftFloatingWindowParams()             --> {keyframes...}     # For the LEFT eye -> returns a dict (offset -> dict) of keyframe offsets and respective floating window params. Value at particular offset includes the left, right, top and bottom floating window values.\\n\\n  GetStereoRightFloatingWindowParams()            --> {keyframes...}     # For the RIGHT eye -> returns a dict (offset -> dict) of keyframe offsets and respective floating window params. Value at particular offset includes the left, right, top and bottom floating window values.\\n\\n  GetNumNodes()                                   --> int                # Returns the number of nodes in the current graph for the timeline item\\n\\n  ApplyArriCdlLut()                               --> Bool               # Applies ARRI CDL and LUT. Returns True if successful, False otherwise.\\n\\n  SetLUT(nodeIndex, lutPath)                      --> Bool               # Sets LUT on the node mapping the node index provided, 1 <= nodeIndex <= total number of nodes.\\n\\n                                                                         # The lutPath can be an absolute path, or a relative path (based off custom LUT paths or the master LUT path).\\n\\n                                                                         # The operation is successful for valid lut paths that Resolve has already discovered (see Project.RefreshLUTList).\\n\\n  GetLUT(nodeIndex)                               --> String             # Gets relative LUT path based on the node index provided, 1 <= nodeIndex <= total number of nodes.\\n\\n  SetCDL([CDL map])                               --> Bool               # Keys of map are: \"NodeIndex\", \"Slope\", \"Offset\", \"Power\", \"Saturation\", where 1 <= NodeIndex <= total number of nodes.\\n\\n                                                                         # Example python code - SetCDL({\"NodeIndex\" : \"1\", \"Slope\" : \"0.5 0.4 0.2\", \"Offset\" : \"0.4 0.3 0.2\", \"Power\" : \"0.6 0.7 0.8\", \"Saturation\" : \"0.65\"})\\n\\n  AddTake(mediaPoolItem, startFrame, endFrame)    --> Bool               # Adds mediaPoolItem as a new take. Initializes a take selector for the timeline item if needed. By default, the full clip extents is added. startFrame (int) and endFrame (int) are optional arguments used to specify the extents.\\n\\n  GetSelectedTakeIndex()                          --> int                # Returns the index of the currently selected take, or 0 if the clip is not a take selector.\\n\\n  GetTakesCount()                                 --> int                # Returns the number of takes in take selector, or 0 if the clip is not a take selector.\\n\\n  GetTakeByIndex(idx)                             --> {takeInfo...}      # Returns a dict (keys \"startFrame\", \"endFrame\" and \"mediaPoolItem\") with take info for specified index.\\n\\n  DeleteTakeByIndex(idx)                          --> Bool               # Deletes a take by index, 1 <= idx <= number of takes.\\n\\n  SelectTakeByIndex(idx)                          --> Bool               # Selects a take by index, 1 <= idx <= number of takes.\\n\\n  FinalizeTake()                                  --> Bool               # Finalizes take selection.\\n\\n  CopyGrades([tgtTimelineItems])                  --> Bool               # Copies the current grade to all the items in tgtTimelineItems list. Returns True on success and False if any error occurred.\\n\\n  SetClipEnabled(Bool)                            --> Bool               # Sets clip enabled based on argument.\\n\\n  GetClipEnabled()                                --> Bool               # Gets clip enabled status.\\n\\n  UpdateSidecar()                                 --> Bool               # Updates sidecar file for BRAW clips or RMD file for R3D clips.\\n\\n  GetUniqueId()                                   --> string             # Returns a unique ID for the timeline item\\n\\n  LoadBurnInPreset(presetName)                    --> Bool               # Loads user defined data burn in preset for clip when supplied presetName (string). Returns true if successful.\\n\\n  GetNodeLabel(nodeIndex)                         --> string             # Returns the label of the node at nodeIndex.\\n\\n  CreateMagicMask(mode)                           --> Bool               # Returns True if magic mask was created successfully, False otherwise. mode can \"F\" (forward), \"B\" (backward), or \"BI\" (bidirection)\\n\\n  RegenerateMagicMask()                           --> Bool               # Returns True if magic mask was regenerated successfully, False otherwise.\\n\\n  Stabilize()                                     --> Bool               # Returns True if stabilization was successful, False otherwise\\n\\n  SmartReframe()                                  --> Bool               # Performs Smart Reframe. Returns True if successful, False otherwise.\\n\\nGallery\\n\\n  GetAlbumName(galleryStillAlbum)                 --> string             # Returns the name of the GalleryStillAlbum object \\'galleryStillAlbum\\'.\\n\\n  SetAlbumName(galleryStillAlbum, albumName)      --> Bool               # Sets the name of the GalleryStillAlbum object \\'galleryStillAlbum\\' to \\'albumName\\'.\\n\\n  GetCurrentStillAlbum()                          --> galleryStillAlbum  # Returns current album as a GalleryStillAlbum object.\\n\\n  SetCurrentStillAlbum(galleryStillAlbum)         --> Bool               # Sets current album to GalleryStillAlbum object \\'galleryStillAlbum\\'.\\n\\n  GetGalleryStillAlbums()                         --> [galleryStillAlbum] # Returns the gallery albums as a list of GalleryStillAlbum objects.\\n\\nGalleryStillAlbum\\n\\n  GetStills()                                     --> [galleryStill]     # Returns the list of GalleryStill objects in the album.\\n\\n  GetLabel(galleryStill)                          --> string             # Returns the label of the galleryStill.\\n\\n  SetLabel(galleryStill, label)                   --> Bool               # Sets the new \\'label\\' to GalleryStill object \\'galleryStill\\'.\\n\\n  ImportStills([filePaths])                       --> Bool               # Imports GalleryStill from each filePath in [filePaths] list. True if at least one still is imported successfully. False otherwise.\\n\\n  ExportStills([galleryStill], folderPath, filePrefix, format) --> Bool  # Exports list of GalleryStill objects \\'[galleryStill]\\' to directory \\'folderPath\\', with filename prefix \\'filePrefix\\', using file format \\'format\\' (supported formats: dpx, cin, tif, jpg, png, ppm, bmp, xpm, drx).\\n\\n  DeleteStills([galleryStill])                    --> Bool               # Deletes specified list of GalleryStill objects \\'[galleryStill]\\'.\\n\\nGalleryStill                                                             # This class does not provide any API functions but the object type is used by functions in other classes.\\n\\nList and Dict Data Structures\\n\\n-----------------------------\\n\\nBeside primitive data types, Resolve\\'s Python API mainly uses list and dict data structures. Lists are denoted by [ ... ] and dicts are denoted by { ... } above.\\n\\nAs Lua does not support list and dict data structures, the Lua API implements \"list\" as a table with indices, e.g. { [1] = listValue1, [2] = listValue2, ... }.\\n\\nSimilarly the Lua API implements \"dict\" as a table with the dictionary key as first element, e.g. { [dictKey1] = dictValue1, [dictKey2] = dictValue2, ... }.\\n\\nLooking up Project and Clip properties\\n\\n--------------------------------------\\n\\nThis section covers additional notes for the functions \"Project:GetSetting\", \"Project:SetSetting\", \"Timeline:GetSetting\", \"Timeline:SetSetting\", \"MediaPoolItem:GetClipProperty\" and\\n\\n\"MediaPoolItem:SetClipProperty\". These functions are used to get and set properties otherwise available to the user through the Project Settings and the Clip Attributes dialogs.\\n\\nThe functions follow a key-value pair format, where each property is identified by a key (the settingName or propertyName parameter) and possesses a value (typically a text value). Keys and values are\\n\\ndesigned to be easily correlated with parameter names and values in the Resolve UI. Explicitly enumerated values for some parameters are listed below.\\n\\nSome properties may be read only - these include intrinsic clip properties like date created or sample rate, and properties that can be disabled in specific application contexts (e.g. custom colorspaces\\n\\nin an ACES workflow, or output sizing parameters when behavior is set to match timeline)\\n\\nGetting values:\\n\\nInvoke \"Project:GetSetting\", \"Timeline:GetSetting\" or \"MediaPoolItem:GetClipProperty\" with the appropriate property key. To get a snapshot of all queryable properties (keys and values), you can call\\n\\n\"Project:GetSetting\", \"Timeline:GetSetting\" or \"MediaPoolItem:GetClipProperty\" without parameters (or with a NoneType or a blank property key). Using specific keys to query individual properties will\\n\\nbe faster. Note that getting a property using an invalid key will return a trivial result.\\n\\nSetting values:\\n\\nInvoke \"Project:SetSetting\", \"Timeline:SetSetting\" or \"MediaPoolItem:SetClipProperty\" with the appropriate property key and a valid value. When setting a parameter, please check the return value to\\n\\nensure the success of the operation. You can troubleshoot the validity of keys and values by setting the desired result from the UI and checking property snapshots before and after the change.\\n\\nThe following Project properties have specifically enumerated values:\\n\\n\"superScale\" - the property value is an enumerated integer between 0 and 4 with these meanings: 0=Auto, 1=no scaling, and 2, 3 and 4 represent the Super Scale multipliers 2x, 3x and 4x.\\n\\n               for super scale multiplier \\'2x Enhanced\\', exactly 4 arguments must be passed as outlined below. If less than 4 arguments are passed, it will default to 2x.\\n\\nAffects:\\n\\nx = Project:GetSetting(\\'superScale\\') and Project:SetSetting(\\'superScale\\', x)\\n\\nfor \\'2x Enhanced\\' --> Project:SetSetting(\\'superScale\\', 2, sharpnessValue, noiseReductionValue), where sharpnessValue is a float in the range [0.0, 1.0] and noiseReductionValue is a float in the range [0.0, 1.0]\\n\\n\"timelineFrameRate\" - the property value is one of the frame rates available to the user in project settings under \"Timeline frame rate\" option. Drop Frame can be configured for supported frame rates\\n\\n                      by appending the frame rate with \"DF\", e.g. \"29.97 DF\" will enable drop frame and \"29.97\" will disable drop frame\\n\\nAffects:\\n\\nx = Project:GetSetting(\\'timelineFrameRate\\') and Project:SetSetting(\\'timelineFrameRate\\', x)\\n\\nThe following Clip properties have specifically enumerated values:\\n\\n\"Super Scale\" - the property value is an enumerated integer between 1 and 4 with these meanings: 1=no scaling, and 2, 3 and 4 represent the Super Scale multipliers 2x, 3x and 4x.\\n\\n                for super scale multiplier \\'2x Enhanced\\', exactly 4 arguments must be passed as outlined below. If less than 4 arguments are passed, it will default to 2x.\\n\\nAffects:\\n\\nx = MediaPoolItem:GetClipProperty(\\'Super Scale\\') and MediaPoolItem:SetClipProperty(\\'Super Scale\\', x)\\n\\nfor \\'2x Enhanced\\' --> MediaPoolItem:SetClipProperty(\\'Super Scale\\', 2, sharpnessValue, noiseReductionValue), where sharpnessValue is a float in the range [0.0, 1.0] and noiseReductionValue is a float in the range [0.0, 1.0]\\n\\nLooking up Render Settings\\n\\n--------------------------\\n\\nThis section covers the supported settings for the method SetRenderSettings({settings})\\n\\nThe parameter setting is a dictionary containing the following keys:\\n\\n- \"SelectAllFrames\": Bool (when set True, the settings MarkIn and MarkOut are ignored)\\n\\n- \"MarkIn\": int\\n\\n- \"MarkOut\": int\\n\\n- \"TargetDir\": string\\n\\n- \"CustomName\": string\\n\\n- \"UniqueFilenameStyle\": 0 - Prefix, 1 - Suffix.\\n\\n- \"ExportVideo\": Bool\\n\\n- \"ExportAudio\": Bool\\n\\n- \"FormatWidth\": int\\n\\n- \"FormatHeight\": int\\n\\n- \"FrameRate\": float (examples: 23.976, 24)\\n\\n- \"PixelAspectRatio\": string (for SD resolution: \"16_9\" or \"4_3\") (other resolutions: \"square\" or \"cinemascope\")\\n\\n- \"VideoQuality\" possible values for current codec (if applicable):\\n\\n-    0 (int) - will set quality to automatic\\n\\n-    [1 -> MAX] (int) - will set input bit rate\\n\\n-    [\"Least\", \"Low\", \"Medium\", \"High\", \"Best\"] (String) - will set input quality level\\n\\n- \"AudioCodec\": string (example: \"aac\")\\n\\n- \"AudioBitDepth\": int\\n\\n- \"AudioSampleRate\": int\\n\\n- \"ColorSpaceTag\" : string (example: \"Same as Project\", \"AstroDesign\")\\n\\n- \"GammaTag\" : string (example: \"Same as Project\", \"ACEScct\")\\n\\n- \"ExportAlpha\": Bool\\n\\n- \"EncodingProfile\": string (example: \"Main10\"). Can only be set for H.264 and H.265.\\n\\n- \"MultiPassEncode\": Bool. Can only be set for H.264.\\n\\n- \"AlphaMode\": 0 - Premultiplied, 1 - Straight. Can only be set if \"ExportAlpha\" is true.\\n\\n- \"NetworkOptimization\": Bool. Only supported by QuickTime and MP4 formats.\\n\\nLooking up timeline export properties\\n\\n-------------------------------------\\n\\nThis section covers the parameters for the argument Export(fileName, exportType, exportSubtype).\\n\\nexportType can be one of the following constants:\\n\\n- resolve.EXPORT_AAF\\n\\n- resolve.EXPORT_DRT\\n\\n- resolve.EXPORT_EDL\\n\\n- resolve.EXPORT_FCP_7_XML\\n\\n- resolve.EXPORT_FCPXML_1_8\\n\\n- resolve.EXPORT_FCPXML_1_9\\n\\n- resolve.EXPORT_FCPXML_1_10\\n\\n- resolve.EXPORT_HDR_10_PROFILE_A\\n\\n- resolve.EXPORT_HDR_10_PROFILE_B\\n\\n- resolve.EXPORT_TEXT_CSV\\n\\n- resolve.EXPORT_TEXT_TAB\\n\\n- resolve.EXPORT_DOLBY_VISION_VER_2_9\\n\\n- resolve.EXPORT_DOLBY_VISION_VER_4_0\\n\\n- resolve.EXPORT_DOLBY_VISION_VER_5_1\\n\\n- resolve.EXPORT_OTIO\\n\\nexportSubtype can be one of the following enums:\\n\\n- resolve.EXPORT_NONE\\n\\n- resolve.EXPORT_AAF_NEW\\n\\n- resolve.EXPORT_AAF_EXISTING\\n\\n- resolve.EXPORT_CDL\\n\\n- resolve.EXPORT_SDL\\n\\n- resolve.EXPORT_MISSING_CLIPS\\n\\nPlease note that exportSubType is a required parameter for resolve.EXPORT_AAF and resolve.EXPORT_EDL. For rest of the exportType, exportSubtype is ignored.\\n\\nWhen exportType is resolve.EXPORT_AAF, valid exportSubtype values are resolve.EXPORT_AAF_NEW and resolve.EXPORT_AAF_EXISTING.\\n\\nWhen exportType is resolve.EXPORT_EDL, valid exportSubtype values are resolve.EXPORT_CDL, resolve.EXPORT_SDL, resolve.EXPORT_MISSING_CLIPS and resolve.EXPORT_NONE.\\n\\nNote: Replace \\'resolve.\\' when using the constants above, if a different Resolve class instance name is used.\\n\\nUnsupported exportType types\\n\\n---------------------------------\\n\\nStarting with DaVinci Resolve 18.1, the following export types are not supported:\\n\\n- resolve.EXPORT_FCPXML_1_3\\n\\n- resolve.EXPORT_FCPXML_1_4\\n\\n- resolve.EXPORT_FCPXML_1_5\\n\\n- resolve.EXPORT_FCPXML_1_6\\n\\n- resolve.EXPORT_FCPXML_1_7\\n\\nLooking up Timeline item properties\\n\\n-----------------------------------\\n\\nThis section covers additional notes for the function \"TimelineItem:SetProperty\" and \"TimelineItem:GetProperty\". These functions are used to get and set properties mentioned.\\n\\nThe supported keys with their accepted values are:\\n\\n  \"Pan\" : floating point values from -4.0*width to 4.0*width\\n\\n  \"Tilt\" : floating point values from -4.0*height to 4.0*height\\n\\n  \"ZoomX\" : floating point values from 0.0 to 100.0\\n\\n  \"ZoomY\" : floating point values from 0.0 to 100.0\\n\\n  \"ZoomGang\" : a boolean value\\n\\n  \"RotationAngle\" : floating point values from -360.0 to 360.0\\n\\n  \"AnchorPointX\" : floating point values from -4.0*width to 4.0*width\\n\\n  \"AnchorPointY\" : floating point values from -4.0*height to 4.0*height\\n\\n  \"Pitch\" : floating point values from -1.5 to 1.5\\n\\n  \"Yaw\" : floating point values from -1.5 to 1.5\\n\\n  \"FlipX\" : boolean value for flipping horizontally\\n\\n  \"FlipY\" : boolean value for flipping vertically\\n\\n  \"CropLeft\" : floating point values from 0.0 to width\\n\\n  \"CropRight\" : floating point values from 0.0 to width\\n\\n  \"CropTop\" : floating point values from 0.0 to height\\n\\n  \"CropBottom\" : floating point values from 0.0 to height\\n\\n  \"CropSoftness\" : floating point values from -100.0 to 100.0\\n\\n  \"CropRetain\" : boolean value for \"Retain Image Position\" checkbox\\n\\n  \"DynamicZoomEase\" : A value from the following constants\\n\\n- DYNAMIC_ZOOM_EASE_LINEAR = 0\\n\\n- DYNAMIC_ZOOM_EASE_IN\\n\\n- DYNAMIC_ZOOM_EASE_OUT\\n\\n- DYNAMIC_ZOOM_EASE_IN_AND_OUT\\n\\n  \"CompositeMode\" : A value from the following constants\\n\\n- COMPOSITE_NORMAL = 0\\n\\n- COMPOSITE_ADD\\n\\n- COMPOSITE_SUBTRACT\\n\\n- COMPOSITE_DIFF\\n\\n- COMPOSITE_MULTIPLY\\n\\n- COMPOSITE_SCREEN\\n\\n- COMPOSITE_OVERLAY\\n\\n- COMPOSITE_HARDLIGHT\\n\\n- COMPOSITE_SOFTLIGHT\\n\\n- COMPOSITE_DARKEN\\n\\n- COMPOSITE_LIGHTEN\\n\\n- COMPOSITE_COLOR_DODGE\\n\\n- COMPOSITE_COLOR_BURN\\n\\n- COMPOSITE_EXCLUSION\\n\\n- COMPOSITE_HUE\\n\\n- COMPOSITE_SATURATE\\n\\n- COMPOSITE_COLORIZE\\n\\n- COMPOSITE_LUMA_MASK\\n\\n- COMPOSITE_DIVIDE\\n\\n- COMPOSITE_LINEAR_DODGE\\n\\n- COMPOSITE_LINEAR_BURN\\n\\n- COMPOSITE_LINEAR_LIGHT\\n\\n- COMPOSITE_VIVID_LIGHT\\n\\n- COMPOSITE_PIN_LIGHT\\n\\n- COMPOSITE_HARD_MIX\\n\\n- COMPOSITE_LIGHTER_COLOR\\n\\n- COMPOSITE_DARKER_COLOR\\n\\n- COMPOSITE_FOREGROUND\\n\\n- COMPOSITE_ALPHA\\n\\n- COMPOSITE_INVERTED_ALPHA\\n\\n- COMPOSITE_LUM\\n\\n- COMPOSITE_INVERTED_LUM\\n\\n  \"Opacity\" : floating point value from 0.0 to 100.0\\n\\n  \"Distortion\" : floating point value from -1.0 to 1.0\\n\\n  \"RetimeProcess\" : A value from the following constants\\n\\n- RETIME_USE_PROJECT = 0\\n\\n- RETIME_NEAREST\\n\\n- RETIME_FRAME_BLEND\\n\\n- RETIME_OPTICAL_FLOW\\n\\n  \"MotionEstimation\" : A value from the following constants\\n\\n- MOTION_EST_USE_PROJECT = 0\\n\\n- MOTION_EST_STANDARD_FASTER\\n\\n- MOTION_EST_STANDARD_BETTER\\n\\n- MOTION_EST_ENHANCED_FASTER\\n\\n- MOTION_EST_ENHANCED_BETTER\\n\\n- MOTION_EST_SPEED_WRAP\\n\\n  \"Scaling\" : A value from the following constants\\n\\n- SCALE_USE_PROJECT = 0\\n\\n- SCALE_CROP\\n\\n- SCALE_FIT\\n\\n- SCALE_FILL\\n\\n- SCALE_STRETCH\\n\\n  \"ResizeFilter\" : A value from the following constants\\n\\n- RESIZE_FILTER_USE_PROJECT = 0\\n\\n- RESIZE_FILTER_SHARPER\\n\\n- RESIZE_FILTER_SMOOTHER\\n\\n- RESIZE_FILTER_BICUBIC\\n\\n- RESIZE_FILTER_BILINEAR\\n\\n- RESIZE_FILTER_BESSEL\\n\\n- RESIZE_FILTER_BOX\\n\\n- RESIZE_FILTER_CATMULL_ROM\\n\\n- RESIZE_FILTER_CUBIC\\n\\n- RESIZE_FILTER_GAUSSIAN\\n\\n- RESIZE_FILTER_LANCZOS\\n\\n- RESIZE_FILTER_MITCHELL\\n\\n- RESIZE_FILTER_NEAREST_NEIGHBOR\\n\\n- RESIZE_FILTER_QUADRATIC\\n\\n- RESIZE_FILTER_SINC\\n\\n- RESIZE_FILTER_LINEAR\\n\\nValues beyond the range will be clipped\\n\\nwidth and height are same as the UI max limits\\n\\nThe arguments can be passed as a key and value pair or they can be grouped together into a dictionary (for python) or table (for lua) and passed\\n\\nas a single argument.\\n\\nGetting the values for the keys that uses constants will return the number which is in the constant\\n\\nDeprecated Resolve API Functions\\n\\n--------------------------------\\n\\nThe following API functions are deprecated.\\n\\nProjectManager\\n\\n  GetProjectsInCurrentFolder()                    --> {project names...} # Returns a dict of project names in current folder.\\n\\n  GetFoldersInCurrentFolder()                     --> {folder names...}  # Returns a dict of folder names in current folder.\\n\\nProject\\n\\n  GetPresets()                                    --> {presets...}       # Returns a dict of presets and their information.\\n\\n  GetRenderJobs()                                 --> {render jobs...}   # Returns a dict of render jobs and their information.\\n\\n  GetRenderPresets()                              --> {presets...}       # Returns a dict of render presets and their information.\\n\\nMediaStorage\\n\\n  GetMountedVolumes()                             --> {paths...}         # Returns a dict of folder paths corresponding to mounted volumes displayed in Resolves Media Storage.\\n\\n  GetSubFolders(folderPath)                       --> {paths...}         # Returns a dict of folder paths in the given absolute folder path.\\n\\n  GetFiles(folderPath)                            --> {paths...}         # Returns a dict of media and file listings in the given absolute folder path. Note that media listings may be logically consolidated entries.\\n\\n  AddItemsToMediaPool(item1, item2, ...)          --> {clips...}         # Adds specified file/folder paths from Media Storage into current Media Pool folder. Input is one or more file/folder paths. Returns a dict of the MediaPoolItems created.\\n\\n  AddItemsToMediaPool([items...])                 --> {clips...}         # Adds specified file/folder paths from Media Storage into current Media Pool folder. Input is an array of file/folder paths. Returns a dict of the MediaPoolItems created.\\n\\nFolder\\n\\n  GetClips()                                      --> {clips...}         # Returns a dict of clips (items) within the folder.\\n\\n  GetSubFolders()                                 --> {folders...}       # Returns a dict of subfolders in the folder.\\n\\nMediaPoolItem\\n\\n  GetFlags()                                      --> {colors...}        # Returns a dict of flag colors assigned to the item.\\n\\nTimeline\\n\\n  GetItemsInTrack(trackType, index)               --> {items...}         # Returns a dict of Timeline items on the video or audio track (based on trackType) at specified\\n\\nTimelineItem\\n\\n  GetFusionCompNames()                            --> {names...}         # Returns a dict of Fusion composition names associated with the timeline item.\\n\\n  GetFlags()                                      --> {colors...}        # Returns a dict of flag colors assigned to the item.\\n\\n  GetVersionNames(versionType)                    --> {names...}         # Returns a dict of version names by provided versionType: 0 - local, 1 - remote.\\n\\nUnsupported Resolve API Functions\\n\\n---------------------------------\\n\\nThe following API (functions and parameters) are no longer supported. Use job IDs instead of indices.\\n\\nProject\\n\\n  StartRendering(index1, index2, ...)             --> Bool               # Please use unique job ids (string) instead of indices.\\n\\n  StartRendering([idxs...])                       --> Bool               # Please use unique job ids (string) instead of indices.\\n\\n  DeleteRenderJobByIndex(idx)                     --> Bool               # Please use unique job ids (string) instead of indices.\\n\\n  GetRenderJobStatus(idx)                         --> {status info}      # Please use unique job ids (string) instead of indices.\\n\\n  GetSetting and SetSetting                       --> {}                 # settingName videoMonitorUseRec601For422SDI is now replaced with videoMonitorUseMatrixOverrideFor422SDI and videoMonitorMatrixOverrideFor422SDI.\\n\\n                                                                         # settingName perfProxyMediaOn is now replaced with perfProxyMediaMode which takes values 0 - disabled, 1 - when available, 2 - when source not available.\\n\\nWORKFLOW INTEGRATIONS\\n\\nUpdated as of 25 August, 2020\\n\\n---------------------------\\n\\nIn this package, you will find a brief introduction to the Workflow Integration Plugins support for DaVinci Resolve Studio. Apart from this README.txt file, this package contains following folders:\\n\\nExamples: containing some representative sample plugin, and a sample script.\\n\\nScripts: containing some sample workflow scripts to interact with Resolve.\\n\\nOverview\\n\\n--------\\n\\nDaVinci Resolve Studio now supports Workflow Integration Plugins to be loaded and communicate with Resolve. Resolve can run one or more Workflow Integration Plugins at the same time.\\n\\nUsers can write their own Workflow Integration Plugin (an Electron app) which could be loaded into DaVinci Resolve Studio. To interact with Resolve, Resolve\\'s JavaScript APIs can be used from the plugin.\\n\\nAlternatively, a Python or Lua script can be invoked, with the option of a user interface built with Resolve\\'s built-in Qt-based UIManager, or with an external GUI manager. See the \"Sample Workflow Integration Script\" section below for details.\\n\\nSample Workflow Integration Plugin\\n\\n----------------------------------\\n\\nA sample Workflow Integration Plugin is available in the \"Examples/SamplePlugin\" directory. In order for Resolve to register this plugin, this directory needs to be copied to \\'Workflow Integration Plugins\\' root directory (mentioned in below section).\\n\\nOnce a plugin is registered, plugin can be loaded from UI sub-menu under \\'Workspace->Workflow Integrations\\'. This will load the plugin and show the plugin HTML page in a separate window.\\n\\nSample plugin helps to understand how a plugin should be structured and how it works with Resolve. Please refer to the directory/file structure, manifest file info, plugin loading, JavaScript API usage examples, etc.\\n\\nThis sample plugin and scripts demonstrates few basic scriptable JavaScript API usages to interact with Resolve.\\n\\nLoading Workflow Integration Plugin\\n\\n-----------------------------------\\n\\nOn startup, DaVinci Resolve Studio scans the Workflow Integration Plugins root directory and enumerates all plugin modules. For each valid plugin module, it creates a UI sub-menu entry under \\'Workspace->Workflow Integrations\\' menu.\\n\\nDaVinci Resolve Studio reads the basic details of the plugin from its manifest.xml file during load time. Once plugin is loaded, user can click on the \\'Workflow Integrations\\' sub-menu to load the corresponding plugin.\\n\\nWorkflow Integration Plugin directory structure\\n\\n-----------------------------------------------\\n\\ncom.<company>.<plugin_name>/\\n\\n    package.js\\n\\n    main.js\\n\\n    index.html\\n\\n    manifest.xml\\n\\n    node_modules/\\n\\n        <Node.js modules>\\n\\n    js/\\n\\n        <supporting js files>\\n\\n    css/\\n\\n        <css files containing styling info>\\n\\n    img/\\n\\n        <image files>\\n\\nWorkflow Integration Plugins root directory\\n\\n-------------------------------------------\\n\\nUser should place their Workflow Integration Plugin under the following directory:\\n\\n    Mac OS X:\\n\\n        \"/Library/Application Support/Blackmagic Design/DaVinci Resolve/Workflow Integration Plugins/\"\\n\\n    Windows:\\n\\n        \"%PROGRAMDATA%\\\\Blackmagic Design\\\\DaVinci Resolve\\\\Support\\\\Workflow Integration Plugins\\\\\"\\n\\nSupported platforms\\n\\n-------------------\\n\\nPlugins: Windows, Mac OS X (not supported on Linux currently)\\n\\nScripts: Windows, Mac OS X, Linux\\n\\nUsing scriptable JavaScript API\\n\\n-------------------------------\\n\\nScriptable JavaScript API execution happens under HTML environment like any typical website. Once HTML page is loaded it can execute scriptable JavaScript API as needed (like clicking on a button, etc.)\\n\\nThis example JavaScript snippet creates a simple project in DaVinci Resolve Studio:\\n\\n    const WorkflowIntegration = require(\\'./WorkflowIntegration.node\\');\\n\\n    isInitialized = WorkflowIntegration.Initialize(\\'com.blackmagicdesign.resolve.sampleplugin\\');\\n\\n    if (isInitialized) {\\n\\n        resolve = WorkflowIntegration.GetResolve();\\n\\n        resolve.GetProjectManager().CreateProject(\"Hello World\");\\n\\n    }\\n\\nThe resolve object is the fundamental starting point for scripting via Resolve. As a native object, it can be inspected for further scriptable properties and functions in JavaScript.\\n\\nWorkflowIntegration module API\\n\\n-------------------------------\\n\\nTo interact with Resolve you need to use \\'WorkflowIntegration.node\\' Node.js module file in your plugin app. Below are the WorkflowIntegration (module) JavaScript API functions to communicate with Resolve.\\n\\nWorkflowIntegration\\n\\n  Initialize(<pluginId>)                          --> Bool               # Returns true if initialization is successful, false otherwise. <pluginId> is the unique plugin id string configured in the manifest.xml file.\\n\\n  GetResolve()                                    --> Resolve            # Returns Resolve object.\\n\\n  RegisterCallback(callbackName, callbackFunc)    --> Bool               # Returns true if input callback name/function is registered successfully, false otherwise.\\n\\n                                                                         # \\'callbackName\\' should be a valid supported callback string name (refer to the below section \\'Supported callbacks\\').\\n\\n                                                                         # \\'callbackFunc\\' should be a valid JavaScript function without any arguments.\\n\\n  DeregisterCallback(callbackName)                --> Bool               # Returns true if input callback name is deregistered successfully, false otherwise.\\n\\n  CleanUp()                                       --> Bool               # Returns true if cleanup is successful, false otherwise. This should be called during plugin app quit.\\n\\n  SetAPITimeout(valueInSecs)                      --> Bool               # By default, apis dont timeout. In order to enable timeout, set a non-zero positive integer value in the arg \\'valueInSecs\\'.\\n\\n                                                                         # Setting it to 0 will disable timeout. This function will return true if the timeout is set/reset successfully.\\n\\nSupported callbacks\\n\\n-------------------\\n\\n\\'RenderStart\\'\\n\\n\\'RenderStop\\'\\n\\nPlease note that there is no console based support for JavaScript API.\\n\\nSample Workflow Integration Script\\n\\n----------------------------------\\n\\nA sample Workflow Integration Python script is also available in the \"Examples\" directory. In order for Resolve to register this script, it needs to be copied to the \\'Workflow Integration Plugins\\' root directory (mentioned in the above section).\\n\\nOnce a script is registered, it can be also loaded from the \\'Workspace\\' menu, under \\'Workflow Integrations\\'. This will invoke the script and show the sample UIManager window.\\n\\nWorkflow Integration scripts work similarly to other scripts in Resolve, and use the same scripting API. This example script provides a basic introduction into creating a popup Workflow application using a UIManager window, with simple layout of text fields and buttons, and event handlers to dispatch functions for integration with the user\\'s facility. Alternatively, third-party UI managers such PyQt may be used instead, or no GUI at all.\\n\\nWhen launched by Resolve, plugin scripts are automatically provided with \\'resolve\\' and \\'project\\' variables for immediate and easy access to Resolve\\'s scripting API. Additional third-party modules may be imported for access to asset-management systems as desired.\\n\\nUIManager Introduction\\n\\n----------------------\\n\\nThere are two main objects needed to manage a window, the UIManager that handles layout, and the UIDispatcher that manages interaction events, accessed as follows:\\n\\n\\tui = fusion.UIManager()\\n\\n\\tdispatcher = bmd.UIDispatcher(ui)\\n\\nWindows are created with the the UIDispatcher, passing a dictionary of attributes like ID and Text, with GUI elements in nested layouts all created with the UIManager.\\n\\nUIDispatcher Functions\\n\\n--------------------\\n\\nThe UIDispatcher object has a few important functions to manage processing of events. The most important are:\\n\\n\\tAddWindow(props, children):\\tAccepts a dictionary of properties and a list of children, returns a Window object\\n\\n\\tAddDialog(props, children):\\tAccepts a dictionary of properties and a list of children, returns a Dialog object\\n\\n\\tint RunLoop():\\t\\t\\t\\tCall when your window is ready to receive user clicks and other events\\n\\n\\tExitLoop(int):\\t\\t\\t\\tTerminates the event processing, and returns any supplied exit code from RunLoop()\\n\\nCommon usage is to create your window and set up any event handlers, including a Close handler for the window that calls ExitLoop(), then Show() your window and call RunLoop() to wait for user interaction:\\n\\n\\tui = fusion.UIManager\\n\\n\\tdispatcher = bmd.UIDispatcher(ui)\\n\\n\\twin = dispatcher.AddWindow({ \\'ID\\': \\'myWindow\\' }, [ ui.Label({ \\'Text\\': \\'Hello World!\\' }) ])\\n\\n\\tdef OnClose(ev):\\n\\n\\t\\tdispatcher.ExitLoop()\\n\\n\\twin.On.myWindow.Close = OnClose\\n\\n\\twin.Show()\\n\\n\\tdispatcher.RunLoop()\\n\\nAddWindow() will also accept a single child without needing a list, or a single dictionary containing both proprties and child elements, for ease of use.\\n\\nAs well as constructing new child elements and layouts, the UIManager also offers a few useful functions:\\n\\n\\tFindWindow(ID):\\t\\t\\t\\t\\t\\tReturns an element with matching ID\\n\\n\\tFindWindows(ID):\\t\\t\\t\\t\\tReturns a list of all elements with matching ID\\n\\n\\tQueueEvent(element, event, info):\\tCalls the element\\'s event handler for \\'event\\', passing it the dictionary \\'info\\'\\n\\nUIManager Elements\\n\\n------------------\\n\\nThe element\\'s ID is used to find, manage, and dispatch events for that element. GUI elements also support a set of common attributes including Enabled, Hidden, Visible, Font, WindowTitle, BackgroundColor, Geometry, ToolTip, StatusTip, StyleSheet, WindowOpacity, MinimumSize, MaximumSize, and FixedSize. Some other common GUI elements and their main attributes include:\\n\\n\\tLabel:\\t\\tText, Alignment, FrameStyle, WordWrap, Indent, Margin\\n\\n\\tButton:\\t\\tText, Down, Checkable, Checked, Icon, IconSize, Flat\\n\\n\\tCheckBox:\\tText, Down, Checkable, Checked, Tristate, CheckState\\n\\n\\tComboBox:\\tItemText, Editable, CurrentIndex, CurrentText, Count\\n\\n\\tSpinBox:\\tValue, Minimum, Maximum, SingleStep, Prefix, Suffix, Alignment, ReadOnly, Wrapping\\n\\n\\tSlider:\\t\\tValue, Minimum, Maximum, SingleStep, PageStep, Orientation, Tracking, SliderPosition\\n\\n\\tLineEdit:\\tText, PlaceholderText, Font, MaxLength, ReadOnly, Modified, ClearButtonEnabled\\n\\n\\tTextEdit:\\tText, PlaceholderText, HTML, Font, Alignment, ReadOnly, TextColor, TextBackgroundColor, TabStopWidth, Lexer, LexerColors\\n\\n\\tColorPicker: Text, Color, Tracking, DoAlpha\\n\\n\\tFont:\\t\\tFamily, StyleName, PointSize, PixelSize, Bold, Italic, Underline, Overline, StrikeOut, Kerning, Weight, Stretch, MonoSpaced\\n\\n\\tIcon: \\t\\tFile\\n\\n\\tTabBar:\\t\\tCurrentIndex, TabsClosable, Expanding, AutoHide, Movable, DrawBase, UsesScrollButtons, DocumentMode, ChangeCurrentOnDrag\\n\\n\\tTree:\\t\\tColumnCount, SortingEnabled, ItemsExpandable, ExpandsOnDoubleClick, AutoExpandDelay, HeaderHidden, IconSize, RootIsDecorated,\\n\\n\\t\\t\\t\\tAnimated, AllColumnsShowFocus, WordWrap, TreePosition, SelectionBehavior, SelectionMode, UniformRowHeights, Indentation, \\n\\n\\t\\t\\t\\tVerticalScrollMode, HorizontalScrollMode, AutoScroll, AutoScrollMargin, TabKeyNavigation, AlternatingRowColors,\\n\\n\\t\\t\\t\\tFrameStyle, LineWidth, MidLineWidth, FrameRect, FrameShape, FrameShadow\\n\\n\\tTreeItem:\\tSelected, Hidden, Expanded, Disabled, FirstColumnSpanned, Flags, ChildIndicatorPolicy\\n\\nSome elements also have property arrays, indexed by item or column (zero-based), e.g. newItem.Text[2] = \\'Third column text\\'\\n\\n\\tCombo:\\t\\tItemText[]\\n\\n\\tTabBar:\\t\\tTabText[], TabToolTip[], TabWhatsThis[], TabTextColor[]\\n\\n\\tTree:\\t\\tColumnWidth[]\\n\\n\\tTreeitem: \\tText[], StatusTip[], ToolTip[], WhatsThis[], SizeHint[], TextAlignment[], CheckState[], BackgroundColor[], TextColor[], Icon[], Font[]\\n\\nSome elements like Label and Button will automatically recognise and render basic HTML in their Text attributes, and TextEdit is capable of displaying and returning HTML too. Element attributes can be specified when creating the element, or can be read or changed later:\\n\\n\\twin.Find(\\'myButton\\').Text = \"Processing...\"\\n\\nMost elements have functions that can be called from them as well:\\n\\n\\tShow()\\n\\n\\tHide()\\n\\n\\tRaise()\\n\\n\\tLower()\\n\\n\\tClose()\\t\\t\\t\\tReturns boolean\\n\\n\\tFind(ID)\\t\\t\\tReturns child element with matching ID\\n\\n\\tGetChildren()\\t\\tReturns list\\n\\n\\tAddChild(element)\\n\\n\\tRemoveChild(element)\\n\\n\\tSetParent(element)\\n\\n\\tMove(point)\\n\\n\\tResize(size)\\n\\n\\tSize()\\t\\t\\t\\tReturns size\\n\\n\\tPos()\\t\\t\\t\\tReturns position\\n\\n\\tHasFocus()\\t\\t\\tReturns boolean\\n\\n\\tSetFocus(reason)\\tAccepts string \"MouseFocusReason\", \"TabFocusReason\", \"ActiveWindowFocusReason\", \"OtherFocusreason\", etc\\n\\n\\tFocusWidget()\\t\\tReturns element\\n\\n\\tIsActiveWindow()\\tReturns boolean\\n\\n\\tSetTabOrder(element)\\n\\n\\tUpdate()\\n\\n\\tRepaint()\\n\\n\\tSetPaletteColor(r,g,b)\\n\\n\\tQueueEvent(name, info)  Accepts event name string and dictionary of event attributes\\n\\n\\tGetItems()\\t\\t\\tReturns dictionary of all child elements\\n\\nSome elements have extra functions of their own:\\n\\n\\tLabel:\\t\\t\\t\\tSetSelection(int, int), bool HasSelection(), string SelectedText(), int SelectionStart()\\n\\n\\tButton:\\t\\t\\t\\tClick(), Toggle(), AnimateClick()\\n\\n\\tCheckBox:\\t\\t\\tClick(), Toggle(), AnimateClick()\\n\\n\\tComboBox:\\t\\t\\tAddItem(string), InsertItem(string), AddItems(list), InsertItems(int, list), InsertSeparator(int), RemoveItem(int), Clear(),\\n\\n\\t\\t\\t\\t\\t\\tSetEditText(string), ClearEditText(), Count(), ShowPopup(), HidePopup()\\n\\n\\tSpinBox:\\t\\t\\tSetRange(int, int), StepBy(int), StepUp(), StepDown(), SelectAll(), Clear()\\n\\n\\tSlider:\\t\\t\\t\\tSetRange(int, int), TriggerAction(string)\\n\\n\\tLineEdit:\\t\\t\\tSetSelection(int, int), bool HasSelectedText(), string SelectedText(), int SelectionStart(), SelectAll(), Clear(), Cut(), Copy(), Paste(),\\n\\n\\t\\t\\t\\t\\t\\tUndo(), Redo(), Deselect(), Insert(string), Backspace(), Del(), Home(bool), End(bool), int CursorPositionAt(point)\\n\\n\\tTextEdit:\\t\\t\\tInsertPlainText(string), InsertHTML(string), Append(string), SelectAll(), Clear(), Cut(), Copy(), Paste(), Undo(), Redo(), \\n\\n\\t\\t\\t\\t\\t\\tScrollToAnchor(string), ZoomIn(int), ZoomOut(int), EnsureCursorVisible(), MoveCursor(moveOperation, moveMode), bool CanPaste(),\\n\\n\\t\\t\\t\\t\\t\\tstring AnchorAt(point), bool Find(string, findFlags)\\n\\n\\tTabBar:\\t\\t\\t\\tint AddTab(strubg), int InsertTab(string), int Count(), RemoveTab(int), MoveTab(int, int)\\n\\n\\tTree:\\t\\t\\t\\tAddTopLevelItem(item), InsertTopLevelItem(item), SetHeaderLabel(string), int CurrentColumn(), int SortColumn(),\\n\\n\\t\\t\\t\\t\\t\\tint TopLevelItemCount(), item CurrentItem(), item TopLevelItem(int), item TakeTopLevelItem(int), item InvisibleRootItem(),\\n\\n\\t\\t\\t\\t\\t\\titem HeaderItem(), int IndexOfTopLevelItem(item), item ItemAbove(item), item ItemBelow(item), item ItemAt(point), \\n\\n\\t\\t\\t\\t\\t\\tClear(), rect VisualItemRect(item), SetHeaderLabels(list), SetHeaderItem(item), InsertTopLevelItems(list), AddTopLevelItems(list),\\n\\n\\t\\t\\t\\t\\t\\tlist SelectedItems(), list FindItems(string, flags), SortItems(int, order), ScrollToItem(item), ResetIndentation(), \\n\\n\\t\\t\\t\\t\\t\\tSortByColumn(int, order), int FrameWidth()\\n\\n\\tTreeItem:\\t\\t\\tAddChild(item), InsertChild(item), RemoveChild(iitem), SortChildren(int, order), InsertChildren(int, list), AddChildren(list),\\n\\n\\t\\t\\t\\t\\t\\tint IndexOfChild(item), item Clone(), tree TreeWidget(), item Parent(), item Child(int), item TakeChild(int),\\n\\n\\t\\t\\t\\t\\t\\tint ChildCount(), int ColumnCount()\\n\\n\\tWindow:\\t\\t\\t\\tShow(), Hide(), RecalcLayout()\\n\\n\\tDialog:\\t\\t\\t\\tExec(), IsRunning(), Done(), RecalcLayout()\\n\\nElements can be accessed by the window\\'s FindWindow(id) function, or by assigning them to a variable for later usage, which is more efficient. The GetItems() function will return a dictionary of all child elements for ease of access.\\n\\nUIManager Layout\\n\\n----------------\\n\\nAdditionally, elements can be nested to define layout, using the HGroup and VGroup elements. As with Window and other elements, tou can pass a single dictionary or list with all properties and children, or separate them into a dict of properties and list of children, for convenience:\\n\\n\\twinLayout = ui.VGroup([\\n\\n\\t\\tui.Label({ \\'Text\\': \"A 2x2 grid of buttons\", \\'Weight\\': 1 }),\\n\\n\\t\\tui.HGroup({ \\'Weight\\': 5 }, [\\n\\n\\t\\t\\tui.Button({ \\'ID\\': \"myButton1\",  \\'Text\\': \"Go\" }),\\n\\n\\t\\t\\tui.Button({ \\'ID\\': \"myButton2\",  \\'Text\\': \"Stop\" }),\\n\\n\\t\\t\\t]),\\n\\n\\t\\tui.VGap(2),\\n\\n\\t\\tui.HGroup({ \\'Weight\\': 5 }, [\\n\\n\\t\\t\\tui.Button({ \\'ID\\': \"myButtonA\",  \\'Text\\': \"Begin\" }),\\n\\n\\t\\t\\tui.Button({ \\'ID\\': \"myButtonB\",  \\'Text\\': \"End\" }),\\n\\n\\t\\t\\t]),\\n\\n\\t\\t]),\\n\\n\\twin = dispatcher.AddWindow({ \\'ID\\': \"myWindow\" }, winLayout)\\n\\nHGap and VGap elements can included for finer spacing control. Note also the Weight attribute, which can be applied to most elements to control how they adjust their relative sizes. A Weight of 0 will use the element\\'s minimum size.\\n\\nEvent Handlers\\n\\n--------------\\n\\nWindow objects will call user-defined event handler functions in response to various interaction events. Event handlers are managed using a window member called \\'On\\'. This has sub-members for each GUI element with an ID, and those have members for each available event. To set up an event handler, define a function for it, then assign the function to the window\\'s On.ID.Event member as follows:\\n\\n\\tdef OnClose(ev):\\n\\n\\t\\tdispatcher.ExitLoop()\\n\\n\\twin.On.myWindow.Close = OnClose\\n\\nAlternatively, if your object\\'s ID is stored in a string variable called \\'buttonID\\', you could use:\\n\\n\\twin.On[buttonID].Clicked = OnButtonClicked\\n\\nMany objects have specific events that can be handled:\\n\\n\\tButton:\\t\\t\\t\\tClicked, Toggled, Pressed, Released\\n\\n\\tCheckBox:\\t\\t\\tClicked, Toggled, Pressed, Released\\n\\n\\tComboBox:\\t\\t\\tCurrentIndexChanged, CurrentTextChanged, TextEdited, EditTextChanged, EditingFinished, ReturnPressed, Activated\\n\\n\\tSpinBox:\\t\\t\\tValueChanged, EditingFinished\\n\\n\\tSlider:\\t\\t\\t\\tValueChanged, SliderMoved, ActionTriggered, SliderPressed, SliderReleased, RangeChanged\\n\\n\\tLineEdit:\\t\\t\\tTextChanged, TextEdited, EditingFinished, ReturnPressed, SelectionChanged, CursorPositionChanged\\n\\n\\tTextEdit:\\t\\t\\tTextChanged, SelectionChanged, CursorPositionChanged\\n\\n\\tColorPicker:\\t\\tColorChanged\\n\\n\\tTabBar:\\t\\t\\t\\tCurrentChanged, CloseRequested, TabMoved, TabBarClicked, TabBarDoubleClicked\\n\\n\\tTree:\\t\\t\\t\\tCurrentItemChanged, ItemClicked, ItemPressed, ItemActivated, ItemDoubleClicked, ItemChanged, ItemEntered, \\n\\n\\t\\t\\t\\t\\t\\tItemExpanded, ItemCollapsed, CurrentItemChanged, ItemSelectionChanged\\n\\n\\tWindow:\\t\\t\\t\\tClose, Show, Hide, Resize, MousePress, MouseRelease, MouseDoubleClick, MouseMove, Wheel, KeyPress, KeyRelease,\\n\\n\\t\\t\\t\\t\\t\\tFocusIn, FocusOut, ContextMenu, Enter, Leave\\n\\nEvent handler functions are called with a dictionary of related attributes such as who, what, when, sender, and modifiers. Common events and some additional attributes they receive include:\\n\\n\\tMousePress:\\t\\t\\tPos, GlobalPos, Button, Buttons\\n\\n\\tMouseRelease:\\t\\tPos, GlobalPos, Button, Buttons \\n\\n\\tMouseDoubleClick:\\tPos, GlobalPos, Button, Buttons \\n\\n\\tMouseMove:\\t\\t\\tPos, GlobalPos, Button, Buttons\\n\\n\\tWheel:\\t\\t\\t\\tPos, GlobalPos, Buttons, Delta, PixelDelta, AngleDelta, Orientiation, Phase\\n\\n\\tKeyPress:\\t\\t\\tKey, Text, IsAutoRepeat, Count\\n\\n\\tKeyRelease:\\t\\t\\tKey, Text, IsAutoRepeat, Count\\n\\n\\tContextMenu:\\t\\tPos, GlobalPos\\n\\n\\tMove:\\t\\t\\t\\tPos, OldPos\\n\\n\\tFocusIn:\\t\\t\\tReason\\n\\n\\tFocusOut:\\t\\t\\tReason\\n\\nEvent handlers can be enabled or disabled for a given element by turning them on or off in the Events attribute:\\n\\n\\tui.Slider({ \\'ID\\': \\'mySlider\\', \\'Events\\': { \\'SliderMoved\\': true } })\\n\\nSome common events like Clicked or Close are enabled by default.\\n\\nBasic Resolve API\\n\\n-----------------\\n\\nPlease refer to the [Basic Resolve API] section in \\'../Developer/Scripting/README.txt\\' file for the list of the functions that Resolve offers for scripted control. For plugin scripts, the \\'resolve\\' and \\'project\\' variables are automatically set up for you, and may be used to access any part of Resolve\\'s API.\\n\\nFurther Information\\n\\n-------------------\\n\\nThis document provides a basic introduction only, and does not list all available UIManager elements and attributes. As UIManager is based on Qt, you can refer to the Qt documentation at https://doc.qt.io/qt-5/qwidget.html for more information on element types and their attributes. There are also many third-party examples and discussions available on user forums for DaVinci Resolve and Fusion Studio.', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/resolve-readme-summary.docx'}),\n",
       " Document(page_content='Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")\\n\\nWe want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\\n\\nos.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\\n\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\n\\nfrom langchain.text_splitter import CharacterTextSplitter\\n\\nfrom langchain.vectorstores import Pinecone\\n\\nfrom langchain.document_loaders import TextLoader\\n\\nAPI Reference:\\n\\nOpenAIEmbeddings\\n\\nCharacterTextSplitter\\n\\nPinecone\\n\\nTextLoader\\n\\nfrom langchain.document_loaders import TextLoader\\n\\nloader = TextLoader(\"../../../state_of_the_union.txt\")\\n\\ndocuments = loader.load()\\n\\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\\n\\ndocs = text_splitter.split_documents(documents)\\n\\nembeddings = OpenAIEmbeddings()\\n\\nAPI Reference:\\n\\nTextLoader\\n\\nimport pinecone\\n\\n# initialize pinecone pinecone.init( api_key=os.getenv(\"PINECONE_API_KEY\"),  # find at app.pinecone.io environment=os.getenv(\"PINECONE_ENV\"),  # next to api key in console )\\n\\nindex_name = \"langchain\\n\\n\\n\\ndemo\"\\n\\n# First, check if our index already exists. If it doesn\\'t, we create it if index_name not in pinecone.list_indexes(): # we create a new index pinecone.create_index( name=index_name, metric=\\'cosine\\', dimension=1536 ) # The OpenAI embedding model `text-embedding-ada-002 uses 1536 dimensions` docsearch = Pinecone.from_documents(docs, embeddings, index_name=index_name)\\n\\n# if you already have an index, you can load it like this # docsearch = Pinecone.from_existing_index(index_name, embeddings)\\n\\nquery = \"What did the president say about Ketanji Brown Jackson\" docs = docsearch.similarity_search(query)\\n\\nprint(docs[0].page_content)\\n\\nAdding More Text to an Existing Index More text can embedded and upserted to an existing Pinecone index using the add_texts function\\n\\nindex = pinecone.Index(\"langchain-demo\") vectorstore = Pinecone(index, embeddings.embed_query, \"text\")\\n\\nvectorstore.add_texts(\"More text!\")\\n\\nMaximal Marginal Relevance Searches In addition to using similarity search in the retriever object, you can also use mmr as retriever.\\n\\nretriever = docsearch.as_retriever(search_type=\"mmr\") matched_docs = retriever.get_relevant_documents(query) for i, d in enumerate(matched_docs): print(f\"\\\\n## Document {i}\\\\n\") print(d.page_content)\\n\\nOr use max_marginal_relevance_search directly:\\n\\nfound_docs = docsearch.max_marginal_relevance_search(query, k=2, fetch_k=10) for i, doc in enumerate(found_docs): print(f\"{i + 1}. \", doc.page_content, \"\\\\n\")', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/pinecone_docs.txt'}),\n",
       " Document(page_content='GPT4 with\\nRetrieval Augmentation over LangChain Docs\\n\\nIn this notebook we\\'ll work through an example of using GPT-4 with\\nretrieval augmentation to answer questions about the LangChain Python\\nlibrary.\\n\\nTo begin we must install the prerequisite libraries:\\n\\n!pip install -qU \\\\\\n\\nopenai==0.27.7 \\\\\\n\\n\"pinecone-client[grpc]\"==2.2.1 \\\\\\n\\npinecone-datasets==\\'0.5.0rc11\\'\\n\\n Note: the above pip install is formatted for\\nJupyter notebooks. If running elsewhere you may need to drop the\\n!.\\n\\nIn this example, we will use a pre-embedding dataset of the LangChain\\ndocs from python.langchain.readthedocs.com/.\\nIf you\\'d like to see how we perform the data preparation refer to this notebook.\\n\\nLet\\'s go ahead and download the dataset.\\n\\nfrom pinecone_datasets import load_dataset\\n\\ndataset = load_dataset(\\'langchain-python-docs-text-embedding-ada-002\\')\\n\\n# we drop sparse_values as they are not needed for this example\\n\\ndataset.documents.drop([\\n\\n\\'metadata\\'], axis\\n\\n1, inplace\\n\\nTrue)\\n\\ndataset.documents.rename(columns\\n\\n={\\n\\n\\'blob\\':\\n\\n\\'metadata\\'}, inplace\\n\\nTrue)\\n\\ndataset.head()\\n\\n0 \\n       417ede5d-39be-498f-b518-f47ed4e53b90 \\n       [0.005949743557721376, 0.01983247883617878, -0... \\n       {\\'chunk\\': 0, \\'text\\': \\'.rst\\n.pdf\\nWelcome to Lan... \\n     \\n       1 \\n       110f550d-110b-4378-b95e-141397fa21bc \\n       [0.009401749819517136, 0.02443608082830906, 0.... \\n       {\\'chunk\\': 1, \\'text\\': \\'Use Cases#\\nBest practice... \\n     \\n       2 \\n       d5f00f02-3295-4567-b297-5e3262dc2728 \\n       [-0.005517194513231516, 0.0208403542637825, 0.... \\n       {\\'chunk\\': 2, \\'text\\': \\'Gallery: A collection of... \\n     \\n       3 \\n       0b6fe3c6-1f0e-4608-a950-43231e46b08a \\n       [-0.006499645300209522, 0.0011573900701478124,... \\n       {\\'chunk\\': 0, \\'text\\': \\'Search\\nError\\nPlease acti... \\n     \\n       4 \\n       39d5f15f-b973-42c0-8c9b-a2df49b627dc \\n       [-0.005658374633640051, 0.00817849114537239, 0... \\n       {\\'chunk\\': 0, \\'text\\': \\'.md\\n.pdf\\nDependents\\nDepe...\\n\\nOur chunks are ready so now we move onto embedding and indexing\\neverything.\\n\\nInitializing the Index\\n\\nNow we need a place to store these embeddings and enable a efficient\\nvector search through them all. To do that we use Pinecone, we can get a\\nfree API key and enter it below\\nwhere we will initialize our connection to Pinecone and create a new\\nindex.\\n\\nimport os\\n\\nimport pinecone\\n\\n# initialize connection to pinecone (get API key at app.pinecone.io)\\n\\napi_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\\n\\n# find your environment next to the api key in pinecone console\\n\\nenv = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\\n\\npinecone.init(api_key=api_key, environment=env)\\n\\nindex_name = \\'gpt-4-langchain-docs-fast\\'\\n\\nimport time\\n\\n# check if index already exists (it shouldn\\'t if this is first time)\\n\\nif index_name not in pinecone.list_indexes():\\n\\n# if does not exist, create index\\n\\npinecone.create_index(\\n\\nindex_name,\\n\\ndimension=1536,  # dimensionality of text-embedding-ada-002\\n\\nmetric=\\'cosine\\'\\n\\n# wait for index to be initialized\\n\\ntime.sleep(1)\\n\\n# connect to index\\n\\nindex = pinecone.GRPCIndex(index_name)\\n\\n# view index stats\\n\\nindex.describe_index_stats()\\n\\nWe can see the index is currently empty with a\\ntotal_vector_count of 0. We can begin\\npopulating it with OpenAI text-embedding-ada-002 built\\nembeddings like so:\\n\\nfor batch in dataset.iter_documents(batch_size=100):\\n\\nindex.upsert(batch)\\n\\n\"model_id\"\\n\\n\"9b9b464993e24f2ba80c85d30f1f2ae4\"\\n\\n\"version_major\"\\n\\n\"version_minor\"\\n\\n\"model_id\"\\n\\n\"b453cc511c7747398718ab8ec11cdaca\"\\n\\n\"version_major\"\\n\\n\"version_minor\"\\n\\nNow we\\'ve added all of our langchain docs to the index. With that we\\ncan move on to retrieval and then answer generation using GPT-4.\\n\\nRetrieval\\n\\nTo search through our documents we first need to create a query\\nvector xq. Using xq we will retrieve the most\\nrelevant chunks from the LangChain docs. To create that query vector we\\nmust initialize a text-embedding-ada-002 embedding model\\nwith OpenAI. For this, you need an OpenAI API key.\\n\\nimport openai\\n\\n# get api key from platform.openai.com\\n\\nopenai.api_key = os.getenv(\\'OPENAI_API_KEY\\') or \\'OPENAI_API_KEY\\'\\n\\nembed_model = \"text-embedding-ada-002\"\\n\\nquery = \"how do I use the LLMChain in LangChain?\"\\n\\nres = openai.Embedding.create(\\n\\ninput=[query],\\n\\nengine=embed_model\\n\\n# retrieve from Pinecone\\n\\nxq = res[\\'data\\'][0][\\'embedding\\']\\n\\n# get relevant contexts (including the questions)\\n\\nres\\n\\n= index.query(xq, top_k\\n\\n5, include_metadata\\n\\nTrue)\\n\\nres\\n\\nWith retrieval complete, we move on to feeding these into GPT-4 to\\nproduce answers.\\n\\nRetrieval Augmented Generation\\n\\nGPT-4 is currently accessed via the ChatCompletions\\nendpoint of OpenAI. To add the information we retrieved into the model,\\nwe need to pass it into our user prompts alongside our original\\nquery. We can do that like so:\\n\\n# get list of retrieved text\\n\\ncontexts\\n\\n= [item[\\n\\n\\'metadata\\'][\\n\\n\\'text\\']\\n\\nfor item\\n\\nin res[\\n\\n\\'matches\\']]\\n\\naugmented_query\\n\\n\\\\n\\\\n\\n\\n---\\n\\n\\\\n\\\\n\\n\\n\".join(contexts)\\n\\n\\\\n\\\\n\\n\\n-----\\n\\n\\\\n\\\\n\\n\\n+query\\n\\nprint(augmented_query)\\n\\nNow we ask the question:\\n\\n# system message to \\'prime\\' the model\\n\\nprimer = f\"\"\"You are Q&A bot. A highly intelligent system that answers\\n\\nuser questions based on the information provided by the user above\\n\\neach question. If the information can not be found in the information\\n\\nprovided by the user you truthfully say \"I don\\'t know\".\\n\\n\"\"\"\\n\\nres = openai.ChatCompletion.create(\\n\\nmodel=\"gpt-4\",\\n\\nmessages=[\\n\\n{\"role\": \"system\", \"content\": primer},\\n\\n{\"role\": \"user\", \"content\": augmented_query}\\n\\nTo display this response nicely, we will display it in markdown.\\n\\nfrom IPython.display import Markdown\\n\\ndisplay(Markdown(res[\\'choices\\'][0][\\'message\\'][\\'content\\']))\\n\\nLet\\'s compare this to a non-augmented query...\\n\\nres = openai.ChatCompletion.create(\\n\\nmodel=\"gpt-4\",\\n\\nmessages=[\\n\\n{\"role\": \"system\", \"content\": primer},\\n\\n{\"role\": \"user\", \"content\": query}\\n\\ndisplay(Markdown(res[\\'choices\\'][0][\\'message\\'][\\'content\\']))\\n\\nIf we drop the \"I don\\'t know\" part of the\\nprimer?\\n\\nres = openai.ChatCompletion.create(\\n\\nmodel=\"gpt-4\",\\n\\nmessages=[\\n\\n{\"role\": \"system\", \"content\": \"You are Q&A bot. A highly intelligent system that answers user questions\"},\\n\\n{\"role\": \"user\", \"content\": query}\\n\\ndisplay(Markdown(res[\\'choices\\'][0][\\'message\\'][\\'content\\']))\\n\\nThen we see something even worse than \"I don\\'t know\" \\nhallucinations. Clearly augmenting our queries with additional context\\ncan make a huge difference to the performance of our system.\\n\\nGreat, we\\'ve seen how to augment GPT-4 with semantic search to allow\\nus to answer LangChain specific queries.\\n\\nOnce you\\'re finished, we delete the index to save resources.\\n\\npinecone.delete_index(index_name)', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/vertopal_com_Copy_of_gpt_4_langchain_docs.html'})]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49d86cd8-4048-42f0-a429-c876f7a28078",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_docs = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6b0b7ad-b0ab-442a-b3a5-fdbd94306b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "55fa35b7-a2b2-42a3-895b-953a572b0489",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(splitted_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3748c83-ad9c-45fa-90d5-4291cb1cdc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='.Index(\"langchain-demo\") vectorstore = Pinecone(index, embeddings.embed_query, \"text\")\\n\\nvectorstore.add_texts(\"More text!\")\\n\\nMaximal Marginal Relevance Searches In addition to using similarity search in the retriever object, you can also use mmr as retriever.\\n\\nretriever = docsearch.as_retriever(search_type=\"mmr\") matched_docs = retriever.get_relevant_documents(query) for i, d in enumerate(matched_docs): print(f\"', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/pinecone_docs.txt'}),\n",
       " Document(page_content='.\\n\\nRetrieval\\n\\nTo search through our documents we first need to create a query\\nvector xq. Using xq we will retrieve the most\\nrelevant chunks from the LangChain docs. To create that query vector we\\nmust initialize a text-embedding-ada-002 embedding model\\nwith OpenAI. For this, you need an OpenAI API key.\\n\\nimport openai', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/vertopal_com_Copy_of_gpt_4_langchain_docs.html'}),\n",
       " Document(page_content='.iter_documents(batch_size=100):\\n\\nindex.upsert(batch)\\n\\n\"model_id\"\\n\\n\"9b9b464993e24f2ba80c85d30f1f2ae4\"\\n\\n\"version_major\"\\n\\n\"version_minor\"\\n\\n\"model_id\"\\n\\n\"b453cc511c7747398718ab8ec11cdaca\"\\n\\n\"version_major\"\\n\\n\"version_minor\"\\n\\nNow we\\'ve added all of our langchain docs to the index. With that we\\ncan move on to retrieval and then answer generation using GPT-4.\\n\\nRetrieval\\n\\nTo search through our documents we first need to create a query\\nvector xq', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/vertopal_com_Copy_of_gpt_4_langchain_docs.html'}),\n",
       " Document(page_content='# view index stats\\n\\nindex.describe_index_stats()\\n\\nWe can see the index is currently empty with a\\ntotal_vector_count of 0. We can begin\\npopulating it with OpenAI text-embedding-ada-002 built\\nembeddings like so:\\n\\nfor batch in dataset.iter_documents(batch_size=100):\\n\\nindex', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/vertopal_com_Copy_of_gpt_4_langchain_docs.html'})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search('How can we query a vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2f19517-c86e-48d6-9e00-edaf6e85498e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\". If running elsewhere you may need to drop the\\n!.\\n\\nIn this example, we will use a pre-embedding dataset of the LangChain\\ndocs from python.langchain.readthedocs.com/.\\nIf you'd like to see how we perform the data preparation refer to this notebook.\\n\\nLet's go ahead and download the dataset.\\n\\nfrom pinecone_datasets import load_dataset\\n\\ndataset = load_dataset('langchain-python-docs-text-embedding-ada-002')\", metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/vertopal_com_Copy_of_gpt_4_langchain_docs.html'}),\n",
       " Document(page_content='Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/pinecone_docs.txt'}),\n",
       " Document(page_content='GPT4 with\\nRetrieval Augmentation over LangChain Docs\\n\\nIn this notebook we\\'ll work through an example of using GPT-4 with\\nretrieval augmentation to answer questions about the LangChain Python\\nlibrary.\\n\\nTo begin we must install the prerequisite libraries:\\n\\n!pip install -qU \\\\\\n\\nopenai==0.27.7 \\\\\\n\\n\"pinecone-client[grpc]\"==2.2.1 \\\\\\n\\npinecone-datasets==\\'0.5.0rc11\\'\\n\\n Note: the above pip install is formatted for\\nJupyter notebooks. If running elsewhere you may need to drop the\\n!', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/vertopal_com_Copy_of_gpt_4_langchain_docs.html'}),\n",
       " Document(page_content='. Please be aware of the security implications when\\n\\nallowing scripting access from outside of the Resolve application.\\n\\nPrerequisites\\n\\n-------------\\n\\nDaVinci Resolve scripting requires one of the following to be installed (for all users):\\n\\n    Lua 5.1\\n\\n    Python 2.7 64-bit\\n\\n    Python >= 3.6 64-bit\\n\\nUsing a script\\n\\n--------------\\n\\nDaVinci Resolve needs to be running for a script to be invoked', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/resolve-readme-summary.docx'})]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search('How to download a dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20b95668-ede5-4fc4-b0e7-86930135e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.save_local('/home/ubuntu/workspace/Temp/04Oct-FAISS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e483231-c843-4b73-9ed7-ffd4e944452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_vstore = FAISS.load_local('/home/ubuntu/workspace/Temp/04Oct-FAISS', embeddings = embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b0cbe7a9-e433-4c2a-be3a-f28dfec1c70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\". If running elsewhere you may need to drop the\\n!.\\n\\nIn this example, we will use a pre-embedding dataset of the LangChain\\ndocs from python.langchain.readthedocs.com/.\\nIf you'd like to see how we perform the data preparation refer to this notebook.\\n\\nLet's go ahead and download the dataset.\\n\\nfrom pinecone_datasets import load_dataset\\n\\ndataset = load_dataset('langchain-python-docs-text-embedding-ada-002')\", metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/vertopal_com_Copy_of_gpt_4_langchain_docs.html'}),\n",
       " Document(page_content='Pinecone Pinecone is a vector database with broad functionality.\\n\\nThis notebook shows how to use functionality related to the Pinecone vector database.\\n\\nTo use Pinecone, you must have an API key. Here are the installation instructions.\\n\\npip install pinecone\\n\\n\\n\\nclient openai tiktoken langchain\\n\\nimport os\\n\\nimport getpass\\n\\nos.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\\n\\nos.environ[\"PINECONE_ENV\"] = getpass', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/pinecone_docs.txt'}),\n",
       " Document(page_content='GPT4 with\\nRetrieval Augmentation over LangChain Docs\\n\\nIn this notebook we\\'ll work through an example of using GPT-4 with\\nretrieval augmentation to answer questions about the LangChain Python\\nlibrary.\\n\\nTo begin we must install the prerequisite libraries:\\n\\n!pip install -qU \\\\\\n\\nopenai==0.27.7 \\\\\\n\\n\"pinecone-client[grpc]\"==2.2.1 \\\\\\n\\npinecone-datasets==\\'0.5.0rc11\\'\\n\\n Note: the above pip install is formatted for\\nJupyter notebooks. If running elsewhere you may need to drop the\\n!', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/vertopal_com_Copy_of_gpt_4_langchain_docs.html'}),\n",
       " Document(page_content='. Please be aware of the security implications when\\n\\nallowing scripting access from outside of the Resolve application.\\n\\nPrerequisites\\n\\n-------------\\n\\nDaVinci Resolve scripting requires one of the following to be installed (for all users):\\n\\n    Lua 5.1\\n\\n    Python 2.7 64-bit\\n\\n    Python >= 3.6 64-bit\\n\\nUsing a script\\n\\n--------------\\n\\nDaVinci Resolve needs to be running for a script to be invoked', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/resolve-readme-summary.docx'})]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_vstore.similarity_search('How to download a dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09e378cd-2344-45ea-bb1f-fb85274da486",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_docs = load_docs_from_file('/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/chat.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "87053cf3-e5d6-4f54-8412-28d7f701cb4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='', metadata={'source': '/home/ubuntu/workspace/mrwhisper-codespace/ChatBot-Work/web_app/knowledgebase/chat.html'})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3866830e-f638-449d-92c0-37e2319e0649",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_splitted_docs = text_splitter.split_documents(chat_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28e22d23-91f5-400f-b2e7-161d29e21905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chat_splitted_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15fb4e0-0572-4484-a62f-f53b3b545b03",
   "metadata": {},
   "source": [
    "# Debugging Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4696754f-ec88-45a1-a490-322afc1a62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are a customized Chatbot, and will be helping in different domains; like code helping, omniverse stuff, ShotGrid docs, api stuff etc. \n",
    "You will have memory, and previous messages will be provided to you in following format: \\n\n",
    "### Input  <Human Message>\n",
    "### Response <Your Reply> \n",
    "and also make use of Knowledge base, which will be provided to you with some workflow.\n",
    "-- If any code is to be written, DO WRITE IT INSIDE ``` <CODE> ``` . This is very important, as this way, the UI will be able to display code in well formatted \n",
    "way, by using some post-processing.\n",
    "\n",
    "{context}\n",
    "\n",
    "this is the running chat-history:\n",
    "{chat_history}\n",
    "\n",
    "### Input:  {question}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables = ['context', 'question', 'chat_history'], template = template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bfb63872-9579-4904-94cc-3258c979fca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "    memory_key= 'chat_history',\n",
    "    human_prefix = '### Input',\n",
    "    ai_prefix = '### Response',\n",
    "    input_key = 'question',\n",
    "    output_key = 'output_text',\n",
    "    return_messages = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "3021d4b6-6447-4689-82a7-184b6b457476",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(llm,chain_type='stuff',prompt = prompt, memory = memory, verbose =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "10b37ea8-bd02-43a3-b838-2af3298d8701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a customized Chatbot, and will be helping in different domains; like code helping, omniverse stuff, ShotGrid docs, api stuff etc. \n",
      "You will have memory, and previous messages will be provided to you in following format: \n",
      "\n",
      "### Input  <Human Message>\n",
      "### Response <Your Reply> \n",
      "and also make use of Knowledge base, which will be provided to you with some workflow.\n",
      "-- If any code is to be written, DO WRITE IT INSIDE ``` <CODE> ``` . This is very important, as this way, the UI will be able to display code in well formatted \n",
      "way, by using some post-processing.\n",
      "\n",
      "Pinecone Pinecone is a vector database with broad functionality.\n",
      "\n",
      "This notebook shows how to use functionality related to the Pinecone vector database.\n",
      "\n",
      "To use Pinecone, you must have an API key. Here are the installation instructions.\n",
      "\n",
      "pip install pinecone\n",
      "\n",
      "\n",
      "\n",
      "client openai tiktoken langchain\n",
      "\n",
      "import os\n",
      "\n",
      "import getpass\n",
      "\n",
      "os.environ[\"PINECONE_API_KEY\"] = getpass.getpass(\"Pinecone API Key:\")\n",
      "\n",
      "os.environ[\"PINECONE_ENV\"] = getpass\n",
      "\n",
      "# initialize pinecone pinecone.init( api_key=os.getenv(\"PINECONE_API_KEY\"),  # find at app.pinecone.io environment=os.getenv(\"PINECONE_ENV\"),  # next to api key in console )\n",
      "\n",
      "index_name = \"langchain\n",
      "\n",
      "\n",
      "\n",
      "demo\"\n",
      "\n",
      "# First, check if our index already exists. If it doesn't, we create it if index_name not in pinecone.list_indexes(): # we create a new index pinecone.create_index( name=index_name, metric='cosine', dimension=1536 )\n",
      "\n",
      ".vectorstores import Pinecone\n",
      "\n",
      "from langchain.document_loaders import TextLoader\n",
      "\n",
      "API Reference:\n",
      "\n",
      "OpenAIEmbeddings\n",
      "\n",
      "CharacterTextSplitter\n",
      "\n",
      "Pinecone\n",
      "\n",
      "TextLoader\n",
      "\n",
      "from langchain.document_loaders import TextLoader\n",
      "\n",
      "loader = TextLoader(\"../../../state_of_the_union.txt\")\n",
      "\n",
      "documents = loader.load()\n",
      "\n",
      "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
      "\n",
      "docs = text_splitter.split_documents(documents)\n",
      "\n",
      "embeddings = OpenAIEmbeddings()\n",
      "\n",
      "API Reference:\n",
      "\n",
      "TextLoader\n",
      "\n",
      "import pinecone\n",
      "\n",
      "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
      "\n",
      "api_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\n",
      "\n",
      "# find your environment next to the api key in pinecone console\n",
      "\n",
      "env = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\n",
      "\n",
      "pinecone.init(api_key=api_key, environment=env)\n",
      "\n",
      "index_name = 'gpt-4-langchain-docs-fast'\n",
      "\n",
      "import time\n",
      "\n",
      "# check if index already exists (it shouldn't if this is first time)\n",
      "\n",
      "if index_name not in pinecone.list_indexes():\n",
      "\n",
      "this is the running chat-history:\n",
      "\n",
      "\n",
      "### Input:  What is PineCone\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = 'What is PineCone'\n",
    "docs = load_vstore.similarity_search(question)\n",
    "answer = chain.run({'input_documents':docs,'question':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2d01c8db-11b7-4aab-b7f4-2c63f04c69f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pinecone is a vector database with broad functionality. It is a service that allows you to store and retrieve vector embeddings efficiently. It provides fast search and similarity matching capabilities, making it useful for various applications such as recommendation systems, information retrieval, and machine learning.'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "822a3c07-24fe-45c8-bc66-8ab7d64a16b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a customized Chatbot, and will be helping in different domains; like code helping, omniverse stuff, ShotGrid docs, api stuff etc. \n",
      "You will have memory, and previous messages will be provided to you in following format: \n",
      "\n",
      "### Input  <Human Message>\n",
      "### Response <Your Reply> \n",
      "and also make use of Knowledge base, which will be provided to you with some workflow.\n",
      "-- If any code is to be written, DO WRITE IT INSIDE ``` <CODE> ``` . This is very important, as this way, the UI will be able to display code in well formatted \n",
      "way, by using some post-processing.\n",
      "\n",
      "\\n\n",
      "\n",
      ".ChatCompletion.create(\n",
      "\n",
      "model=\"gpt-4\",\n",
      "\n",
      "messages=[\n",
      "\n",
      "{\"role\": \"system\", \"content\": \"You are Q&A bot. A highly intelligent system that answers user questions\"},\n",
      "\n",
      "{\"role\": \"user\", \"content\": query}\n",
      "\n",
      "display(Markdown(res['choices'][0]['message']['content']))\n",
      "\n",
      "Then we see something even worse than \"I don't know\" \n",
      "hallucinations. Clearly augmenting our queries with additional context\n",
      "can make a huge difference to the performance of our system\n",
      "\n",
      "# get list of retrieved text\n",
      "\n",
      "contexts\n",
      "\n",
      "= [item[\n",
      "\n",
      "'metadata'][\n",
      "\n",
      "'text']\n",
      "\n",
      "for item\n",
      "\n",
      "in res[\n",
      "\n",
      "'matches']]\n",
      "\n",
      "augmented_query\n",
      "\n",
      "\\n\\n\n",
      "\n",
      "---\n",
      "\n",
      "\\n\\n\n",
      "\n",
      "\".join(contexts)\n",
      "\n",
      "\\n\\n\n",
      "\n",
      "-----\n",
      "\n",
      "\\n\\n\n",
      "\n",
      "+query\n",
      "\n",
      "print(augmented_query)\n",
      "\n",
      "Now we ask the question:\n",
      "\n",
      ".009401749819517136, 0.02443608082830906, 0.... \n",
      "       {'chunk': 1, 'text': 'Use Cases\n",
      "\n",
      "this is the running chat-history:\n",
      "### Input: What is PineCone\n",
      "### Response: Pinecone is a vector database with broad functionality. It is a service that allows you to store and retrieve vector embeddings efficiently. It provides fast search and similarity matching capabilities, making it useful for various applications such as recommendation systems, information retrieval, and machine learning.\n",
      "\n",
      "### Input:  What was my last question to you?\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = 'What was my last question to you?'\n",
    "docs = load_vstore.similarity_search(question)\n",
    "answer = chain.run({'input_documents':docs,'question':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f454efe2-ec0e-4610-b41d-55b5ed2d97d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your last question was \"What is PineCone?\"'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "284edb9c-35d1-4b6f-9c85-ff3a8e0a8b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "You are a customized Chatbot, and will be helping in different domains; like code helping, omniverse stuff, ShotGrid docs, api stuff etc. \n",
      "You will have memory, and previous messages will be provided to you in following format: \n",
      "\n",
      "### Input  <Human Message>\n",
      "### Response <Your Reply> \n",
      "and also make use of Knowledge base, which will be provided to you with some workflow.\n",
      "-- If any code is to be written, DO WRITE IT INSIDE ``` <CODE> ``` . This is very important, as this way, the UI will be able to display code in well formatted \n",
      "way, by using some post-processing.\n",
      "\n",
      ".dll\"\n",
      "\n",
      "    PYTHONPATH=\"%PYTHONPATH%;%RESOLVE_SCRIPT_API%\\Modules\\\"\n",
      "\n",
      "    Linux:\n",
      "\n",
      "    RESOLVE_SCRIPT_API=\"/opt/resolve/Developer/Scripting\"\n",
      "\n",
      "    RESOLVE_SCRIPT_LIB=\"/opt/resolve/libs/Fusion/fusionscript.so\"\n",
      "\n",
      "    PYTHONPATH=\"$PYTHONPATH:$RESOLVE_SCRIPT_API/Modules/\"\n",
      "\n",
      "    (Note: For standard ISO Linux installations, the path above may need to be modified to refer to /home/resolve instead of /opt/resolve)\n",
      "\n",
      "As with Fusion scripts, Resolve scripts can also be invoked via the menu and the Console\n",
      "\n",
      ". Please be aware of the security implications when\n",
      "\n",
      "allowing scripting access from outside of the Resolve application.\n",
      "\n",
      "Prerequisites\n",
      "\n",
      "-------------\n",
      "\n",
      "DaVinci Resolve scripting requires one of the following to be installed (for all users):\n",
      "\n",
      "    Lua 5.1\n",
      "\n",
      "    Python 2.7 64-bit\n",
      "\n",
      "    Python >= 3.6 64-bit\n",
      "\n",
      "Using a script\n",
      "\n",
      "--------------\n",
      "\n",
      "DaVinci Resolve needs to be running for a script to be invoked\n",
      "\n",
      ".getpass(\"Pinecone API Key:\")\n",
      "\n",
      "os.environ[\"PINECONE_ENV\"] = getpass.getpass(\"Pinecone Environment:\")\n",
      "\n",
      "We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.\n",
      "\n",
      "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")\n",
      "\n",
      "from langchain.embeddings.openai import OpenAIEmbeddings\n",
      "\n",
      "from langchain.text_splitter import CharacterTextSplitter\n",
      "\n",
      "from langchain.vectorstores import Pinecone\n",
      "\n",
      "from langchain\n",
      "\n",
      ".local/share/DaVinciResolve/Fusion/Scripts\n",
      "\n",
      "The interactive Console window allows for an easy way to execute simple scripting commands, to query or modify properties, and to test scripts. The console accepts commands in Python 2.7, Python 3.6\n",
      "\n",
      "and Lua and evaluates and executes them immediately. For more information on how to use the Console, please refer to the DaVinci Resolve User Manual.\n",
      "\n",
      "This example Python script creates a simple project:\n",
      "\n",
      "this is the running chat-history:\n",
      "### Input: What is PineCone\n",
      "### Response: Pinecone is a vector database with broad functionality. It is a service that allows you to store and retrieve vector embeddings efficiently. It provides fast search and similarity matching capabilities, making it useful for various applications such as recommendation systems, information retrieval, and machine learning.\n",
      "### Input: What was my last question to you?\n",
      "### Response: Your last question was \"What is PineCone?\"\n",
      "\n",
      "### Input:  How to install python version 3.10.4 using pyenv. Also tell me, then how to create a virtualenv with it\n",
      "\n",
      "### Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "question = 'How to install python version 3.10.4 using pyenv. Also tell me, then how to create a virtualenv with it'\n",
    "docs = load_vstore.similarity_search(question)\n",
    "answer = chain.run({'input_documents':docs,'question':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ae875563-5090-483e-bf3e-b50d748c2ec4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To install Python version 3.10.4 using pyenv, you can follow these steps:\\n\\n1. Install pyenv if you haven't already. You can refer to the official pyenv documentation for installation instructions specific to your operating system.\\n\\n2. Open a terminal or command prompt and run the following command to list all available Python versions that can be installed with pyenv:\\n\\n   ```\\n   pyenv install --list\\n   ```\\n\\n   This will display a list of available Python versions. Find and select version 3.10.4 from the list.\\n\\n3. Run the following command to install Python version 3.10.4:\\n\\n   ```\\n   pyenv install 3.10.4\\n   ```\\n\\n   This will download and install Python 3.10.4 using pyenv.\\n\\nOnce you have installed Python 3.10.4, you can create a virtual environment with it using the following steps:\\n\\n1. Open a terminal or command prompt and navigate to the directory where you want to create the virtual environment.\\n\\n2. Run the following command to create a new virtual environment with Python 3.10.4:\\n\\n   ```\\n   pyenv virtualenv 3.10.4 <env_name>\\n   ```\\n\\n   Replace `<env_name>` with the desired name for your virtual environment.\\n\\n3. Activate the virtual environment by running the following command:\\n\\n   ```\\n   pyenv activate <env_name>\\n   ```\\n\\n   This will activate the virtual environment and set the Python version to 3.10.4.\\n\\nNow you have successfully installed Python version 3.10.4 using pyenv and created a virtual environment with it. You can start using the virtual environment for your Python projects.\""
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f302da9f-38d5-49c2-bc96-d7beebf3cf9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chain \u001b[38;5;241m=\u001b[39m ConversationalRetrievalChain\u001b[38;5;241m.\u001b[39mfrom_llm(llm\u001b[38;5;241m=\u001b[39mllm, \n\u001b[0;32m----> 2\u001b[0m                                           retriever\u001b[38;5;241m=\u001b[39m\u001b[43mretriever\u001b[49m, \n\u001b[1;32m      3\u001b[0m                                           memory\u001b[38;5;241m=\u001b[39mmemory, \n\u001b[1;32m      4\u001b[0m                                           \u001b[38;5;66;03m#get_chat_history=lambda h : h,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m                                           combine_docs_chain_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m:prompt},\n\u001b[1;32m      6\u001b[0m                                           verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "chain = ConversationalRetrievalChain.from_llm(llm=llm, \n",
    "                                          retriever=retriever, \n",
    "                                          memory=memory, \n",
    "                                          #get_chat_history=lambda h : h,\n",
    "                                          combine_docs_chain_kwargs = {'prompt':prompt},\n",
    "                                          verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fe493f-b661-4954-813e-71ccc21070e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-3.10.2",
   "language": "python",
   "name": "langchain-3.10.2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
